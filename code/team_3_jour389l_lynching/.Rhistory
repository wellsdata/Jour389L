athlete_gender <- read.csv("athlete_gender.csv")
athlete_gender <- distinct(athlete_gender)
library(tidyverse)
library(rvest)
library(janitor)
athlete_gender <- distinct(athlete_gender)
View(athlete_gender)
write_csv(athlete_gender, "gender_fixed.csv")
noc_athletes %>%
filter(athlete_id != "NA")
noc_athletes <- noc_athletes %>%
filter(athlete_id != "NA")
write_csv(noc_athletes, "final_noc_athletes.csv")
noc_athletes %>%
filter(athlete_id != "NA") %>%
arrange(desc(athlete_id))
noc_athletes <- distinct(noc_athletes)
distinct(noc_athletes)
write_csv(noc_athletes, "final_final_noc_athletes.csv")
athletes_medals <- merge(x = medal_athletes, y = final_athlete, by.x = "Name", all = TRUE)
athletes_medals_final <- merge(x = athletes_medals, y = medals, by.x = "Medal", by.y = "medal_name", all = TRUE) %>%
select(athlete_id, medal_id)
write_csv(athletes_medals, "final_athletes_medals.csv")
athletes_medals <- merge(x = medal_athletes, y = final_athlete, by.x = "Name", all = TRUE)
athletes_medals_final <- merge(x = athletes_medals, y = medals, by.x = "Medal", by.y = "medal_name", all = TRUE) %>%
select(athlete_id, medal_id)
write_csv(athletes_medals_final, "final_final_athletes_medals.csv")
distinct(olympics_events)
distinct(olympics_events) %>% arrange(event_id)
olympics_events <- distinct(olympics_events) %>% arrange(event_id)
olympics_events <- distinct(olympics_events) %>% arrange(event_id)
write_csv(olympics_events, "distinct_olympic_events.csv")
noc_athletes
medal_athletes %>%
select(Name, Medal, Event)
athletes_with_keys <- read_csv("final hump/olympic_athletes.csv")
View(athletes_with_keys)
View(athletes_events)
medals_with_keys <- read_csv("a_tier_csv/medals.csv")
athletes_events
olympics_events
athletes_with_keys
with_comp_id <- read_csv("final hump/olympic_athletes.csv")
athletes_with_keys <- read_csv("a_tier_csv/athletes.csv")
medals_with_keys <- read_csv("a_tier_csv/medals.csv")
events_with_keys <- read_csv("a_tier_csv/events.csv")
with_comp_id
athletes_with_keys
medals_with_keys
events_with_keys
events_with_keys
events_with_keys <- read_csv("a_tier_csv/events.csv")
events_with_keys
athlete_events_no_keys <- medal_athletes %>%
select(Name, Medal, Event)
athlete_events_no_keys
athletes_with_keys
athletes_with_keys <- read_csv("a_tier_csv/athletes.csv")
final_athlete %>%
arrange(athlete_id)
with_comp_id
final_athlete
medals_with_keys
events_with_keys
athlete_events_no_keys
merge(x = athlete_events_no_keys, y = final_athlete, by.x = "Name", by.y = "athlete_name", all = TRUE)
with_comp_id
final_athlete
medals_with_keys
events_with_keys
athlete_events_no_keys
sports_events_table <- merge(x = athlete_events_no_keys, y = final_athlete, by = "Name", all = TRUE)
merge(x = athlete_events_no_keys, y = final_athlete, by = "Name", all = TRUE)
table1 <- merge(x = athlete_events_no_keys, y = final_athlete, by = "Name", all = TRUE)
merge(x = table1, y = events_with_keys, by.x = "Event", by.y = "event_name", all = TRUE)
merge(x = table2, y = medals_with_keys, by.x = "Medal", by.y = "medal_name", all = TRUE)
medals_with_keys
table2
table2 <- merge(x = table1, y = events_with_keys, by.x = "Event", by.y = "event_name", all = TRUE)
table3 <- merge(x = table2, y = medals_with_keys, by.x = "Medal", by.y = "medal_name", all = TRUE)
merge(x = table2, y = medals_with_keys, by.x = "Medal", by.y = "medal_name", all = TRUE)
merge(x = table3, y = with_comp_id, by = "medal_id", all = TRUE)
table3
with_comp_id
with_comp_id <- read_csv("final hump/olympic_athletes.csv")
with_comp_id
merge(x = table3, y = with_comp_id, by = "athlete_id", all = TRUE)
table4 <- merge(x = table3, y = with_comp_id, by = "athlete_id", all = TRUE)
table4 %>%
select(event_id, competitor_id, medal_id)
table4 %>%
select(event_id, competitor_id, medal_id) %>%
arrange(competitor_id)
athlete_events <- table4 %>%
select(event_id, competitor_id, medal_id) %>%
arrange(competitor_id)
distinct(athlete_events)
table4 %>%
select(event_id, competitor_id, medal_id, olympic_id)
with_olympic_id <- table4 %>%
select(event_id, competitor_id, medal_id, olympic_id)
distinct(with_olympic_id)
with_olympic_id <- table4 %>%
select(event_id, competitor_id, medal_id, olympic_id)
with_olympic_id %>%
arrange(olympic_id)
with_comp_id
table1 <- merge(x = athlete_events_no_keys, y = final_athlete, by = "Name", all = TRUE)
table1
table1_copy <- merge(x = table1, y = medals_with_keys, by.x = "Medal", by.y = "medal_name", all = TRUE)
table1_copy
table1_copy %>%
arrange(athlete_id)
table_1_medals <- table1_copy %>%
arrange(athlete_id)
write_csv(table_1_medals, "medal_id_with_athlete.csv")
with_olympic_id
table2
table2 %>% arrange(athlete_id)
table4 %>% arrange(athlete_id)
# YOU WON"T HAVE ATHLETE_ID IN THE FINAL TABLE
mayhaps_fix <- distinct(table4)
mayhaps_fix %>%
arrange(athlete_id)
distinct(table3)
distinct(table3)
table3
table4
install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
install.packages("ggmap")
library(ggmap)
install.packages("geosphere")
library(geosphere)
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
install.packages("here")
install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
install.packages("ggmap")
library(ggmap)
install.packages("geosphere")
library(geosphere)
starter_code <- lynch_geocoded_9.16 %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
install.packages("here")
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
View(starter_code)
lynching_data %>%
head(10)
library(janitor)
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state) %>%
clean_names()
lynching_data %>%
head(10)
geography_starter_df <- select(newspaper_name1, newspaper_state_code, state_lynch, Newspaper_Region, in_state, year, lynching.lon, lynching.lat, news_location.lon, news_location.lat)
geography_starter_df <- starter_code %>%
select(newspaper_name1, newspaper_state_code, state_lynch, Newspaper_Region, in_state, year, lynching.lon, lynching.lat, news_location.lon, news_location.lat)
starter_code
geography_starter_df <- lynching_data %>%
select(newspaper_name1, newspaper_state_code, state_lynch, Newspaper_Region, in_state, year, lynching.lon, lynching.lat, news_location.lon, news_location.lat)
geography_starter_df
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state) %>%
clean_names()
starter_code
geography_starter_df <- lynching_data %>%
clean_names()
geography_starter_df
geography_starter_df <- lynching_data %>%
clean_names()
select(newspaper_name1, newspaper_state_code, state_lynch, newspaper_region, in_state, year, lynching_lon, lynching_lat, news_location_lon, news_location_lat)
geography_starter_df
geography_starter_df <- lynching_data %>%
clean_names() %>%
select(newspaper_name1, newspaper_state_code, state_lynch, newspaper_region, in_state, year, lynching_lon, lynching_lat, news_location_lon, news_location_lat)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(dplyr)
install.packages("writexl")
library(writexl)
library(ggplot2)
library(janitor)
library(readxl)
#install.packages("plyr")
library(plyr)
data23 <- read_csv('newdatarightformat.csv')
View(data23)
data23 <- data23[-c(1,105,293),]
View(data23)
data23 <- data23 %>%
clean_names() %>%
pivot_longer(cols = c(contains('faculty'), contains('staff'), contains('student')))
View(data23)
colnames(data23) <- c('date','drop','role','count')
View(data23)
data23$date <- as.Date(parse_date_time(data23$date, "mdy"))
data23 <- data23 %>%
select(role, date, count)
data23[is.na(data23)] <- 0
data <- read_csv('data930.csv')
colnames(data) <- c('drop','type','role','date','count')
data <- data%>%
drop_na(drop)
data <- subset(data, select = -c(drop))
View(data)
View(data23)
data <- subset(data, select = -c(type))
data$date <- as.Date(parse_date_time(data$date, "dmy"))
#victoria 9/21/2023 making all nas equal to zero in old data
data[is.na(data)] <- 0
yearly_data <- left_join(data, data23)
View(yearly_data)
historic_data <- rbind(data, data23) %>%
# removing the role column because we really just want the totals
select(-c(role))
merge(data, data23)
yearly_data <- rbind(data, data23)
# joining the two dataframes
historic_data <- rbind(data, data23) %>%
# removing the role column because we really just want the totals
select(-c(role)) %>%
# now I'm summing the case count for each date
historic_data <- aggregate(.~date,data=historic_data,FUN=sum)
# joining the two dataframes
historic_data <- rbind(data, data23) %>%
# removing the role column because we really just want the totals
select(-c(role))
# now I'm summing the case count for each date
historic_data <- aggregate(.~date,data=historic_data,FUN=sum)
historic_data <- historic_data %>%
mutate(sum = rowSums(across(where(is.numeric)), na.rm=TRUE))
historic_data <- aggregate(.~date,data=historic_data,FUN=sum)
View(historic_data)
historic_data %>%
mutate(sum = rowSums(across(where(is.numeric)), na.rm=TRUE))
# read in covid positive report data 9-20-2022 - 9-05-2023
data23 <- read_csv('newdatarightformat.csv')
#get rid of totals rows for each year
data23 <- data23[-c(1,105,293),]
# clean names and create 1 column for role type instead of having 1 column for each different role
data23 <- data23 %>%
clean_names() %>%
pivot_longer(cols = c(contains('faculty'), contains('staff'), contains('student')))
# rename the columns to match what the old dataframe below has
colnames(data23) <- c('date','drop','role','count')
# make the dates consistent with the old datas format
data23$date <- as.Date(parse_date_time(data23$date, "mdy"))
# select only the columns I need, we don't need the column that has the totals for each day
data23 <- data23 %>%
select(role, date, count)
# make all the NAs
data23[is.na(data23)] <- 0
# this is all devon adding in the dataframe we had last year, adjusting column names, getting rid of nas
data <- read_csv('data930.csv')
colnames(data) <- c('drop','type','role','date','count')
data <- data%>%
drop_na(drop)
data <- subset(data, select = -c(drop))
#victoria 9/21/2023 also dropping type column from the old data to match our current data
data <- subset(data, select = -c(type))
# this was devon, just adjustng the date
data$date <- as.Date(parse_date_time(data$date, "dmy"))
#victoria 9/21/2023 making all nas equal to zero in old data
data[is.na(data)] <- 0
# joining the two dataframes
historic_data <- rbind(data, data23) %>%
# removing the role column because we really just want the totals
select(-c(role))
# now I'm summing the case count for each date
historic_data <- aggregate(.~date,data=historic_data,FUN=sum)
historic_data <- historic_data %>%
mutate(sum = rowSums(across(where(is.numeric)), na.rm=TRUE))
historic_data <- historic_data %>%
mutate(sum = rowSums(across(where(is.numeric)), na.rm=TRUE))
# joining the two dataframes
historic_data <- rbind(data, data23) %>%
# removing the role column because we really just want the totals
select(-c(role))
# now I'm summing the case count for each date
historic_data <- aggregate(.~date,data=historic_data,FUN=sum)
#historic_data <- historic_data %>%
#  mutate(sum = rowSums(across(where(is.numeric)), na.rm=TRUE))
# and then I'm writing it out to this file
write_xlsx(historic_data, "newcasedata.xlsx")
View(historic_data)
View(data)
v_fall2022  <- historic_data[historic_data$date >= "2022-08-29" & historic_data$date <= "2022-09-06",]
v_fall2023  <- historic_data[historic_data$date >= "2023-08-28" & historic_data$date <= "2023-09-05",]
sum(v_fall2022$count)
sum(v_fall2023$count)
# This is the end of victoria stuff
sum(v_fall2023$count)
sum(v_fall2022$count)
knitr::opts_chunk$set(echo = TRUE)
#install.packages(("writexl"))
library(writexl)
library(tidyverse)
library(lubridate)
library(dplyr)
library(writexl)
library(ggplot2)
library(janitor)
library(readxl)
data <- read_csv('cases_by_county.csv')
data <- data %>%
mutate(
difference_Allegany = Allegany - lag(Allegany),
difference_Anne_Arundel = Anne_Arundel - lag(Anne_Arundel),
difference_Baltimore = Baltimore - lag(Baltimore),
difference_Baltimore_City = Baltimore_City - lag(Baltimore_City),
difference_Calvert = Calvert - lag(Calvert),
difference_Caroline = Caroline - lag(Caroline),
difference_Carroll = Carroll - lag(Carroll),
difference_Cecil = Cecil - lag(Cecil),
difference_Charles = Charles - lag(Charles),
difference_Dorchester = Dorchester - lag(Dorchester),
difference_Frederick = Frederick - lag(Frederick),
difference_Garrett = Garrett - lag(Garrett),
difference_Harford = Harford - lag(Harford),
difference_Howard = Howard - lag(Howard),
difference_Kent = Kent - lag(Kent),
difference_Montgomery = Montgomery - lag(Montgomery),
difference_Prince_Georges = Prince_Georges - lag(Prince_Georges),
difference_Queen_Annes = Queen_Annes - lag(Queen_Annes),
difference_Somerset = Somerset - lag(Somerset),
difference_St_Marys = St_Marys - lag(St_Marys),
difference_Talbot = Talbot - lag(Talbot),
difference_Washington = Washington - lag(Washington),
difference_Wicomico = Wicomico - lag(Wicomico),
difference_Worcester = Worcester - lag(Worcester)
)
# victoria - we only used PG so just doing this for ease
correct_data <- data %>%
select(DATE, difference_Prince_Georges, Prince_Georges)
# victoria - the first row in this is incorrect, but we can just put 9 in on the graphic it's fine.
View(correct_data)
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
# vibe check
lynching_data %>%
head(10)
## Regional differences
#starter code
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state) %>%
clean_names()
lynching_data
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state) %>%
clean_names()
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
# vibe check
lynching_data %>%
head(10)
lynching_data %>%
select(decade, Newspaper_Region, in_state)
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state) %>%
clean_names()
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
# vibe check
lynching_data %>%
head(10)
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
# vibe check
lynching_data %>%
head(10)
#starter code
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
lynching_data %>%
head(10)
lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
# vibe check
lynching_data %>%
head(10)
lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
setwd("~/Documents/GitHub/jour389l_lynching")
lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
# vibe check
lynching_data %>%
head(10)
lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
# vibe check
lynching_data %>%
head(10)
View(lynching_data)
lynching_data %>%
clean_names() %>%
select(newspaper_name1, newspaper_state_code, state_lynch, newspaper_region, in_state, year, lynching_lon, lynching_lat, news_location_lon, news_location_lat)
geography_starter_df <- lynching_data %>%
clean_names() %>%
select(newspaper_name1, newspaper_state_code, state_lynch, newspaper_region, in_state, year, lynching_lon, lynching_lat, news_location_lon, news_location_lat)
geography_starter_df
geography_starter_df %>%
group_by(year) %>%
summarise(count = n(in_state))
geography_starter_df %>%
group_by(year) %>%
summarise(count = n())
geography_starter_df %>%
group_by(year) %>%
summarize(
count = n())
install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
install.packages("ggmap")
library(ggmap)
install.packages("geosphere")
library(geosphere)
library(janitor)
install.packages("ggmap")
install.packages("geosphere")
install.packages("here")
install.packages("geosphere")
geography_starter_df %>%
group_by(year) %>%
summarize(
count = n())
geography_starter_df %>%
group_by(year) %>%
count(in_state)
geography_starter_df <- lynching_data %>%
clean_names() %>%
select(newspaper_name1, newspaper_state_code, state_lynch, newspaper_region, in_state, year, lynching_lon, lynching_lat, news_location_lon, news_location_lat)
geography_starter_df
geography_starter_df %>%
group_by(year) %>%
count(in_state)
geography_starter_df %>%
group_by(year)
geography_starter_df %>%
filter(newspaper_region == "South") %>%
group_by(year) %>%
summarise(count = n()) %>%
arrange(desc(count))
knitr::opts_chunk$set(echo = TRUE)
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
lynching_data %>%
head(10)
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#install.packages("geosphere")
library(geosphere)
library(janitor)
library(dplyr)
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
lynching_data %>%
head(10)
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
lynching_data
lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)

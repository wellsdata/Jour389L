---
title: "Geographic Analysis"
author: "Khushboo Rathore"
date: "2023-09-20"
output: html_document
---

```{r}
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("tigris")
library(tigris)
library(stringr)
library(janitor)
```
# Analysis
```{r}
lynch_geocoded_9.16 <- read.csv("../data/lynch_geocoded_9.16.csv")

```

## Regional classification for newspaper
```{r}
#Adapt this code to the tolnay_beck data to create a regional identifier

`lynch_geocoded_9.27 <- lynch_geocoded_9.27 %>% 
  mutate(Newspaper_Region2=Newspaper_State) %>% 
  mutate(Newspaper_Region2 = case_when(
    Newspaper_State=="South Carolina" ~ "South",
    Newspaper_State=="Texas" ~ "South",
    Newspaper_State=="Louisiana" ~ "South",
    Newspaper_State=="Tennessee" ~ "South",
    Newspaper_State=="Mississippi" ~ "South",
    Newspaper_State=="Arkansas" ~ "South",
    Newspaper_State=="Alabama" ~ "South",
    Newspaper_State=="Georgia" ~ "South",
    Newspaper_State=="Virginia" ~ "South",
    Newspaper_State=="Florida" ~ "South",
    Newspaper_State=="North Carolina" ~ "South",
    Newspaper_State=="Maryland" ~ "Border",
    Newspaper_State=="Delaware" ~ "Border",
    Newspaper_State=="West Virginia" ~ "Border",
    Newspaper_State=="Kentucky" ~ "Border",
    Newspaper_State=="Missouri" ~ "Border",
    Newspaper_State=="Maine" ~ "North",
    Newspaper_State=="New York" ~ "North",
    Newspaper_State=="New Hampshire" ~ "North",
    Newspaper_State=="Vermont" ~ "North",
    Newspaper_State=="Massassachusetts" ~ "North",
    Newspaper_State=="Connecticut" ~ "North",
    Newspaper_State=="Rhode Island" ~ "North",
    Newspaper_State=="Pennsylvania" ~ "North",
    Newspaper_State=="New Jersey" ~ "North",
    Newspaper_State=="Ohio" ~ "North",
    Newspaper_State=="Indiana" ~ "North",
    Newspaper_State=="Kansas" ~ "North",
    Newspaper_State=="Michigan" ~ "North",
    Newspaper_State=="Wisconsin" ~ "North",
    Newspaper_State=="Minnesota" ~ "North",
    Newspaper_State=="Iowa" ~ "North",
    Newspaper_State=="California" ~ "North",
    Newspaper_State=="Nevada" ~ "North",
    Newspaper_State=="Oregon" ~ "North",
    Newspaper_State=="Illinois" ~ "North",
    Newspaper_State=="Nebraska" ~ "Misc",
    Newspaper_State=="Colorado" ~ "Misc",
    Newspaper_State=="North Dakota" ~ "Misc",
    Newspaper_State=="South Dakota" ~ "Misc",
    Newspaper_State=="Montana" ~ "Misc",
    Newspaper_State=="Washington" ~ "Misc",
    Newspaper_State=="Idaho" ~ "Misc",
    Newspaper_State=="Wyoming" ~ "Misc",
    Newspaper_State=="Utah" ~ "Misc",
    Newspaper_State=="Oklahoma" ~ "Misc",
    Newspaper_State=="New Mexico" ~ "Misc",
    Newspaper_State=="Arizona" ~ "Misc",
    Newspaper_State=="Alaska" ~ "Misc",
    Newspaper_State=="Hawaii" ~ "Misc",
    Newspaper_State=="District of Columbia" ~ "Misc",
    Newspaper_State=="Virgin Islands" ~ "Misc",
    TRUE~Newspaper_State
    )) 


#write.csv(lynch_geocoded_9.27, "../data/lynch_geocoded_9.27.csv")
```

# WEEK ONE
```{r}
## Pct of Newspapers in state vs out of state
 lynch_geocoded_9.16 %>% 
  count(in_state) %>% 
  mutate(pct = round(n/3292,2))


# in_state
# N	3068	0.93		
# Y	223	0.07	

summary(lynch_geocoded_9.16$miles)
#Newspapers, on average, were 878 miles away from a lynching event during the whole time period

duplicated_entries <- lynch_geocoded_9.16 %>% 
  group_by(file_id) %>% 
  count(lynch_address) %>% 
  filter(n > 1)

# Repetitions: 
write_sheet(duplicated_entries, "https://docs.google.com/spreadsheets/d/1mt-K372GEtjE_v0qeiX-57MlnAD-9tT5f4AZr4NxZnY/edit#gid=0")


```

## In State v Out of State by Decade
```{r}

x <- lynch_geocoded_9.16 %>% 
  select(decade, miles, in_state) %>% 
  group_by(decade) %>% 
  count(in_state)

  ggplot(x, aes(x=decade, y=n, color=in_state, fill=in_state)) +
    geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "DRAFT FINDINGS: Little Local Coverage of Lynching",
       subtitle = "Local (teal) vs Out-of-State (red) Lynching Coverage",
       caption = "Teal bar=In-state. Red Bar=Out-of-State. Source: Library of Congress. n=3188 Graphic by Rob Wells. Sept 20 2024",
       y="Count of articles",
       x="")
  
cleaned_articles <- lynch_geocoded_9.16 %>% 
  distinct(file_id, lynch_address, .keep_all = TRUE) %>% 
  mutate(location = case_when(
    newspaper_state_code == state_lynch ~ "in_state",
    TRUE~"out_of_state"
  ))
  
decade_data <- cleaned_articles %>% 
  group_by(decade, location) %>% 
  count(.) %>% 
  pivot_wider(names_from = location, values_from = n) %>% 
  mutate_if(is.numeric, ~replace(., is.na(.), 0)) %>% 
  mutate(sum_articles = out_of_state + in_state) %>% 
  mutate(percent_oos = round(out_of_state/sum_articles*100, 2)) %>% 
  mutate(percent_is = round(in_state/sum_articles*100, 2))

write_sheet(decade_data, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0")

state_groupings <- cleaned_articles %>%
  group_by(newspaper_state_code, location) %>% 
  count(.) %>% 
  pivot_wider(names_from = location, values_from = n) %>% 
  mutate_if(is.numeric, ~replace(., is.na(.), 0)) %>% 
  mutate(sum_articles = out_of_state + in_state) %>% 
  mutate(percent_oos = round(out_of_state/sum_articles*100, 2)) %>% 
  mutate(percent_is = round(in_state/sum_articles*100, 2))

write_sheet(state_groupings, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0")

```

# WEEK TWO
```{r}
# Analyze broader dataset of all lynchings by decade and state and region. Want to see if our sample of lynchings by state is skewed. Total and percentage by state. Group in Regional categories. Next, calculate baseline totals of the newspapers in the states. 
tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>% 
  as.data.frame()

tolnay_beck <- janitor::clean_names(tolnay_beck)

lynch_article_data <- read_csv("~/Desktop/GitHub/Jour389L/data/lynch_geocoded_9.27.csv")

# Basic filter to distinct lynchings
lynch_data <- lynch_article_data %>% 
  mutate(date = as.Date(date, "%m/%d/%Y")) %>% 
  distinct(year, lynch_address, .keep_all = TRUE)

beck_lynchings <- tolnay_beck %>% 
  filter(!str_detect(status, "death"))

```

## Analysis by State
```{r}
beck_by_state <- beck_lynchings %>%
  group_by(lynch_state) %>% 
  count(.) %>% 
  rename(state_lynch = lynch_state) %>% 
  rename(beck_count = n) %>% 
  mutate(percent_lynchings_beck = round(beck_count/nrow(beck_lynchings)*100,2))
    

# Percent of all lynchings that are in each state
lynch_by_state <- lynch_data %>% 
  group_by(state_lynch) %>% 
  count(.) %>% 
  rename(input_count = n) %>% 
  mutate(percent_lynchings_input = round(input_count/nrow(lynch_data)*100,2))

state_lynchings_compare <- lynch_by_state %>% 
  left_join(beck_by_state, by = "state_lynch") %>% 
  mutate(beck_input_dif = percent_lynchings_beck-percent_lynchings_input)


write_sheet(state_lynchings_compare, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0", sheet = "state_lynchings_compare")
```

## Analysis by Region
```{r}
beck_region_coded <- beck_lynchings %>%
  select(status, lynch_state) %>% 
  mutate(lynch_region = case_when(
    str_detect(lynch_state, "SC|TX|LA|TN|MS|AR|AL|GA|VA|FL|NC") ~ "south",
    str_detect(lynch_state, "MD|DE|WV|KY|MO") ~ "border",
    str_detect(lynch_state, "ME|NY|NH|VT|MA|CT|RI|PA|NJ|OH|IN|KS|MI|WI|MN|IA|CA|NV|OR|IL") ~ "north",
    str_detect(lynch_state, "NE|CO|ND|SD|MT|WA|ID|WY|UT|OK|NM|AZ|AK|HI") ~ "misc",
    TRUE~lynch_state
    )) 

lynch_cleaned <- lynch_data %>% 
  clean_names() %>% 
  mutate(newspaper_region2 = tolower(newspaper_region2))

beck_by_region <- beck_region_coded %>% 
  
```


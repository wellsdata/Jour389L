---
title: "Geographic Analysis"
author: "Khushboo Rathore"
date: "2023-09-20"
output: html_document
---


```{r}
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("tigris")
#install.packages("zoo")
library(tigris)
library(stringr)
library(janitor)
library(zoo)

```
# Analysis
```{r}
lynch_geocoded_9.16 <- read.csv("../data/lynch_geocoded_9.16.csv")

```

## Regional classification for newspaper

```{r}
#Classification based on https://www.census.gov/programs-surveys/economic-census/guidance-geographies/levels.html#par_textimage_34
lynch_geocoded_9.27 <- lynch_geocoded_9.27 %>% 
  mutate(Newspaper_Region=Newspaper_State) %>% 
  mutate(Newspaper_Region = case_when(Newspaper_State=="South Carolina" ~ "South",
                           Newspaper_State=="Texas" ~ "South",
                            Newspaper_State=="Louisiana" ~ "South",
                            Newspaper_State=="Tennessee" ~ "South",
                            Newspaper_State=="Mississippi" ~ "South",
                            Newspaper_State=="Arkansas" ~ "South",
                            Newspaper_State=="Alabama" ~ "South",
                            Newspaper_State=="Georgia" ~ "South",
                            Newspaper_State=="Virginia" ~ "South",
                            Newspaper_State=="Florida" ~ "South",
                            Newspaper_State=="North Carolina" ~ "South",
                            Newspaper_State=="Maryland" ~ "South",
                            Newspaper_State=="Delaware" ~ "South",
                            Newspaper_State=="West Virginia" ~ "South",
                            Newspaper_State=="Kentucky" ~ "South",
                            Newspaper_State=="Missouri" ~ "Midwest",
                            Newspaper_State=="Maine" ~ "Northeast",
                            Newspaper_State=="New York" ~ "Northeast",
                            Newspaper_State=="New Hampshire" ~ "Northeast",
                            Newspaper_State=="Vermont" ~ "Northeast",
                            Newspaper_State=="Massachusetts" ~ "Northeast",
                            Newspaper_State=="Connecticut" ~ "Northeast",
                            Newspaper_State=="Rhode Island" ~ "Northeast",
                            Newspaper_State=="Pennsylvania" ~ "Northeast",
                            Newspaper_State=="New Jersey" ~ "Northeast",
                            Newspaper_State=="Ohio" ~ "Midwest",
                            Newspaper_State=="Indiana" ~ "Midwest",
                            Newspaper_State=="Kansas" ~ "Midwest",
                            Newspaper_State=="Michigan" ~ "Midwest",
                             Newspaper_State=="Wisconsin" ~ "Midwest",
                             Newspaper_State=="Minnesota" ~ "Midwest",
                             Newspaper_State=="Iowa" ~ "Midwest",
                             Newspaper_State=="California" ~ "West",
                             Newspaper_State=="Nevada" ~ "West",
                             Newspaper_State=="Oregon" ~ "West",
                            Newspaper_State=="Illinois" ~ "Midwest",
                            Newspaper_State=="Nebraska" ~ "Midwest",
                            Newspaper_State=="Colorado" ~ "West",
                            Newspaper_State=="North Dakota" ~ "Midwest",
                            Newspaper_State=="South Dakota" ~ "Midwest",
                            Newspaper_State=="Montana" ~ "West",
                            Newspaper_State=="Washington" ~ "West",
                            Newspaper_State=="Idaho" ~ "West",
                            Newspaper_State=="Wyoming" ~ "West",
                            Newspaper_State=="Utah" ~ "West",
                            Newspaper_State=="Oklahoma" ~ "South",
                            Newspaper_State=="New Mexico" ~ "West",
                            Newspaper_State=="Arizona" ~ "West",
                            Newspaper_State=="Alaska" ~ "West",
                            Newspaper_State=="Hawaii" ~ "West",
                            Newspaper_State=="District of Columbia" ~ "South",
                            Newspaper_State=="Virgin Islands" ~ "Misc",
                                                     TRUE~Newspaper_State))

#delete old version
# lynch_geocoded_9.27 <- subset(lynch_geocoded_9.27, select =-Newspaper_Region2)

#write.csv(lynch_geocoded_9.27, "../data/lynch_geocoded_9.27.csv")
```

### Border Designation
```{r}
lynch_geocoded_9.27 <- lynch_geocoded_9.27 %>% 
  mutate(Border = case_when(Newspaper_State=="Maryland" ~ "Border",
                            Newspaper_State=="Delaware" ~ "Border",
                            Newspaper_State=="West Virginia" ~ "Border",
                            Newspaper_State=="Kentucky" ~ "Border",
                            Newspaper_State=="Missouri" ~ "Border",
                               .default = "Not_Border"))

# write.csv(lynch_geocoded_9.27, "../data/lynch_geocoded_9.27.csv")
```

# WEEK ONE
```{r}
## Pct of Newspapers in state vs out of state
 lynch_geocoded_9.16 %>% 
  count(in_state) %>% 
  mutate(pct = round(n/3292,2))


# in_state
# N	3068	0.93		
# Y	223	0.07	

summary(lynch_geocoded_9.16$miles)
#Newspapers, on average, were 878 miles away from a lynching event during the whole time period

duplicated_entries <- lynch_geocoded_9.16 %>% 
  group_by(file_id) %>% 
  count(lynch_address) %>% 
  filter(n > 1)

# Repetitions: 
write_sheet(duplicated_entries, "https://docs.google.com/spreadsheets/d/1mt-K372GEtjE_v0qeiX-57MlnAD-9tT5f4AZr4NxZnY/edit#gid=0")


```

## In State v Out of State by Decade
```{r}

x <- lynch_geocoded_9.16 %>% 
  select(decade, miles, in_state) %>% 
  group_by(decade) %>% 
  count(in_state)

  ggplot(x, aes(x=decade, y=n, color=in_state, fill=in_state)) +
    geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "DRAFT FINDINGS: Little Local Coverage of Lynching",
       subtitle = "Local (teal) vs Out-of-State (red) Lynching Coverage",
       caption = "Teal bar=In-state. Red Bar=Out-of-State. Source: Library of Congress. n=3188 Graphic by Rob Wells. Sept 20 2024",
       y="Count of articles",
       x="")
  
cleaned_articles <- lynch_geocoded_9.16 %>% 
  distinct(file_id, lynch_address, .keep_all = TRUE) %>% 
  mutate(location = case_when(
    newspaper_state_code == state_lynch ~ "in_state",
    TRUE~"out_of_state"
  ))
  
decade_data <- cleaned_articles %>% 
  group_by(decade, location) %>% 
  count(.) %>% 
  pivot_wider(names_from = location, values_from = n) %>% 
  mutate_if(is.numeric, ~replace(., is.na(.), 0)) %>% 
  mutate(sum_articles = out_of_state + in_state) %>% 
  mutate(percent_oos = round(out_of_state/sum_articles*100, 2)) %>% 
  mutate(percent_is = round(in_state/sum_articles*100, 2))

write_sheet(decade_data, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0")

state_groupings <- cleaned_articles %>%
  group_by(newspaper_state_code, location) %>% 
  count(.) %>% 
  pivot_wider(names_from = location, values_from = n) %>% 
  mutate_if(is.numeric, ~replace(., is.na(.), 0)) %>% 
  mutate(sum_articles = out_of_state + in_state) %>% 
  mutate(percent_oos = round(out_of_state/sum_articles*100, 2)) %>% 
  mutate(percent_is = round(in_state/sum_articles*100, 2))

write_sheet(state_groupings, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0")

```


# WEEK TWO
```{r}
# Analyze broader dataset of all lynchings by decade and state and region. Want to see if our sample of lynchings by state is skewed. Total and percentage by state. Group in Regional categories. Next, calculate baseline totals of the newspapers in the states. 

tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>% 
  as.data.frame()

tolnay_beck <- janitor::clean_names(tolnay_beck)

#Import failed due to the link. Please assign links to Github
# lynch_article_data <- read_csv("~/Desktop/GitHub/Jour389L/data/lynch_geocoded_9.27.csv")

lynch_article_data <- read_csv("../data/lynch_geocoded_9.27.csv")

# Basic filter to distinct lynchings
# RSW comment - distinct line drops 700+ cases. I think you are trying to get a df with single instances of a lynching incident to compare apples-apples to tolnay-beck, right? 
#Need to improve the distinct line since year is too broad. try year-month


lynch_data <- lynch_article_data %>% 
  mutate(date = as.Date(date, "%m/%d/%Y")) %>% 
  distinct(year, lynch_address, .keep_all = TRUE)

#RSW - creating yearmo, we lose just 437 cases this way
lynch_data1 <- lynch_article_data %>% 
  mutate(date = as.Date(date, "%m/%d/%Y")) %>% 
  mutate(yearmo = zoo::as.yearmon(date)) %>% 
  distinct(yearmo, lynch_address, .keep_all=TRUE)         

x <- lynch_data1 %>% 
  count(file_id) %>% 
  arrange(desc(n))


#RSW: Interesting call on the filter, it removes bystanders
beck_lynchings <- tolnay_beck %>% 
  filter(!str_detect(status, "death"))

```

## Analysis by State
```{r}
beck_by_state <- beck_lynchings %>%
  group_by(lynch_state) %>% 
  count(.) %>% 
  rename(state_lynch = lynch_state) %>% 
  rename(beck_count = n) %>% 
  mutate(percent_lynchings_beck = round(beck_count/nrow(beck_lynchings)*100,2))
    

# Percent of all lynchings that are in each state
lynch_by_state <- lynch_data %>% 
  group_by(state_lynch) %>% 
  count(.) %>% 
  rename(input_count = n) %>% 
  mutate(percent_lynchings_input = round(input_count/nrow(lynch_data)*100,2))

#RSW
lynch_by_state1 <- lynch_data1 %>% 
  group_by(state_lynch) %>% 
  count(.) %>% 
  rename(input_count = n) %>% 
  mutate(percent_lynchings_input = round(input_count/nrow(lynch_data)*100,2))

state_lynchings_compare <- lynch_by_state %>% 
  left_join(beck_by_state, by = "state_lynch") %>% 
  mutate(beck_input_dif = percent_lynchings_beck-percent_lynchings_input)

#RSW
state1_lynchings_compare <- lynch_by_state1 %>% 
  left_join(beck_by_state, by = "state_lynch") %>% 
  mutate(beck_input_dif = percent_lynchings_beck-percent_lynchings_input)

write_sheet(state_lynchings_compare, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0", sheet = "state_lynchings_compare")
```

## Analysis by Region
```{r}
beck_region_coded <- beck_lynchings %>%
  select(status, lynch_state) %>% 
  mutate(lynch_region = case_when(
    str_detect(lynch_state, "SC|TX|LA|TN|MS|AR|AL|GA|VA|FL|NC") ~ "south",
    str_detect(lynch_state, "MD|DE|WV|KY|MO") ~ "border",
    str_detect(lynch_state, "ME|NY|NH|VT|MA|CT|RI|PA|NJ|OH|IN|KS|MI|WI|MN|IA|CA|NV|OR|IL") ~ "north",
    str_detect(lynch_state, "NE|CO|ND|SD|MT|WA|ID|WY|UT|OK|NM|AZ|AK|HI") ~ "misc",
    TRUE~lynch_state
    )) 

lynch_cleaned <- lynch_data %>% 
  clean_names() %>% 
  mutate(newspaper_region = tolower(newspaper_region))

lynch_by_region <- lynch_cleaned %>% 
  group_by(newspaper_region) %>% 
  count(.) %>% 
  rename(input_count = n) %>% 
  mutate(percent_lynchings_input = round(input_count/nrow(lynch_data)*100,2))

beck_by_region <- beck_region_coded %>% 
  group_by(lynch_region) %>% 
  count(.) %>% 
  rename(beck_count = n) %>% 
  mutate(percent_lynchings_beck = round(beck_count/nrow(beck_region_coded)*100,2))

regional_comparison <- lynch_by_region %>% 
  left_join(beck_by_region, by = c("newspaper_region" = "lynch_region")) %>% 
    mutate(beck_input_dif = percent_lynchings_beck-percent_lynchings_input)

write_sheet(regional_comparison, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0", sheet = "regional_comparison")
```
## Newspapers in States & Regions
```{r}
newspaper_list <- lynch_article_data %>% 
  clean_names() %>% 
  distinct(newspaper_name, news_address, .keep_all = TRUE)

paper_count_by_state <- newspaper_list %>% 
  group_by(newspaper_state) %>% 
  count(.) %>% 
  rename(count_papers = n) %>% 
  mutate(percent_papers = round(count_papers/nrow(newspaper_list)*100,2))

write_sheet(paper_count_by_state, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0", sheet = "count_newspapers_in_states")

paper_count_by_region <- newspaper_list %>% 
  group_by(newspaper_region) %>% 
  count(.) %>% 
  rename(count_papers = n) %>% 
  mutate(percent_papers = round(count_papers/nrow(newspaper_list)*100,2))

write_sheet(paper_count_by_region, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0", sheet = "count_newspapers_per_region")
```

```{r}
beck_lynchings %>% 
  group_by(status) %>% 
  count(.) 
```

# WEEK THREE
```{r}
tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>% 
  as.data.frame()

tolnay_beck <- clean_names(tolnay_beck)

#Import failed due to the link. Please assign links to Github
lynch_article_data <- read_csv("../data/lynch_geocoded_10.8.csv")

# Basic filter to distinct lynchings
# RSW comment - distinct line drops 700+ cases. I think you are trying to get a df with single instances of a lynching incident to compare apples-apples to tolnay-beck, right? 
#Need to improve the distinct line since year is too broad. try year-month

lynch_data <- lynch_article_data %>% 
  mutate(date = as.Date(date, "%m/%d/%Y")) %>% 
  mutate(yearmo = as.yearmon(date)) %>% 
  #distinct(year, lynch_address, .keep_all = TRUE) %>% 
    distinct(yearmo, lynch_address, .keep_all = TRUE) %>% 
  clean_names() %>% 
  mutate(lynch_region = case_when(
    str_detect(state_lynch, "SC|TX|LA|TN|MS|AR|AL|GA|VA|FL|NC|MD|DE|WV|KY|OK|DC")~"South",
    str_detect(state_lynch, "MO|OH|IN|KS|MI|WI|MN|IA|IL|NE|ND|SD")~"Midwest",
    str_detect(state_lynch, "NY|NH|VT|MA|CT|RI|PA|NJ|ME")~"Northeast",
    str_detect(state_lynch, "CA|NV|OR|CO|MT|WA|ID|WY|UT|NM|AZ|AK|HI")~"West",
    TRUE~state_lynch
  )) %>% 
  mutate(lynch_border = case_when(
    str_detect(state_lynch, "MD|DE|WV|KY|MO")~"Border",
    TRUE~"Not_Border"
  ))


#RSW: Interesting call on the filter, it removes bystanders
beck_lynchings <- tolnay_beck %>% 
  filter(!str_detect(status, "death")) %>% 
  mutate(lynch_region = case_when(
    str_detect(lynch_state, "SC|TX|LA|TN|MS|AR|AL|GA|VA|FL|NC|MD|DE|WV|KY|OK|DC")~"South",
    str_detect(lynch_state, "MO|OH|IN|KS|MI|WI|MN|IA|IL|NE|ND|SD")~"Midwest",
    str_detect(lynch_state, "NY|NH|VT|MA|CT|RI|PA|NJ")~"Northeast",
    str_detect(lynch_state, "CA|NV|OR|CO|MT|WA|ID|WY|UT|NM|AZ|AK|HI")~"West",
    TRUE~lynch_state
  )) %>% 
  mutate(lynch_border = case_when(
    str_detect(lynch_state, "MD|DE|WV|KY|MO")~"Border",
    TRUE~"Not_Border"
  ))
  
```

## Region & Border Reclassification and Analysis
```{r}
lynch_by_region_updated <- lynch_data %>% 
  group_by(lynch_region) %>% 
  count(.) %>% 
  rename(lynch_count = n) %>% 
  mutate(lynch_percent = round(lynch_count/nrow(lynch_data)*100,2))
  

beck_region_coded <- beck_lynchings %>%
  select(status, lynch_state, lynch_region, lynch_border)
  

beck_regional <- beck_region_coded %>% 
  group_by(lynch_region) %>% 
  count(.) %>% 
  rename(beck_count = n) %>% 
  mutate(beck_percent = round(beck_count/nrow(beck_region_coded)*100,2))

regional_comparison_updated <- lynch_by_region_updated %>% 
  left_join(beck_regional) %>% 
  mutate(percent_diff = lynch_percent - beck_percent)

write_sheet(regional_comparison_updated, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0", sheet = "oct_nine_regional")

lynch_by_border <- lynch_data %>% 
  group_by(lynch_border) %>% 
  count(.) %>% 
  rename(lynch_count = n) %>% 
  mutate(lynch_percent = round(lynch_count/nrow(lynch_data)*100,2))

beck_border <- beck_region_coded %>% 
  group_by(lynch_border) %>% 
  count(.) %>% 
  rename(beck_count = n) %>% 
  mutate(beck_percent = round(beck_count/nrow(beck_region_coded)*100,2))

border_state_comparison <- lynch_by_border %>% 
  left_join(beck_border) %>% 
  mutate(percent_diff = lynch_percent - beck_percent)

write_sheet(border_state_comparison, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0", sheet = "oct_nine_border")
```

## Updated state comparisons
```{r}
lynch_by_state_updated <- lynch_data %>% 
  clean_names() %>% 
  group_by(state_lynch) %>% 
  count(.) %>% 
  rename(lynch_count = n) %>% 
  rename(lynch_state = state_lynch) %>% 
  mutate(lynch_percent = round(lynch_count/nrow(lynch_data)*100,2))

#code fails on beck_count/nrow(beck_region_coded
# beck_by_state <- beck_lynchings %>% 
#   group_by(lynch_state) %>% 
#   count(.) %>% 
#   rename(beck_count = n) %>% 
#   mutate(beck_percent = round(beck_count/nrow(beck_region_coded)*100,2))

#fixed-rsw
beck_by_state <- beck_lynchings %>% 
  group_by(lynch_state) %>% 
  count(.) %>% 
  rename(beck_count = n) %>% 
  mutate(beck_percent = round(beck_count/nrow(beck_lynchings)*100,2))

state_comparison_updated <- lynch_by_state_updated %>% 
  left_join(beck_by_state) %>% 
 # mutate(percent_diff = lynch_percent - beck_percent)
    mutate(pct_point_diff = lynch_percent - beck_percent)

# write.csv(state_comparison_updated, "/Users/robwells/Code/hcij_lynching_phase_two/narratives/output/state_comparison_updated.csv")

write_sheet(state_comparison_updated, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0", sheet = "oct_nine_state")

```

## Decade Comparisons
```{r}
lynch_by_decade <- lynch_data %>% 
  clean_names() %>% 
  mutate(decade = floor(year/10) * 10) %>% 
  group_by(decade) %>% 
  count(.) %>% 
  rename(lynch_count = n) %>%
  mutate(lynch_percent = round(lynch_count/nrow(lynch_data)*100,2))

#code fails on beck_count/nrow(beck_region_coded
# beck_by_decade <- beck_lynchings %>%
#   mutate(decade = floor(year/10) * 10) %>% 
#   group_by(decade) %>% 
#   count(.) %>% 
#   rename(beck_count = n) %>% 
#   mutate(beck_percent = round(beck_count/nrow(beck_region_coded)*100,2))

#fixed - rsw
beck_by_decade <- beck_lynchings %>%
  mutate(decade = floor(year/10) * 10) %>% 
  group_by(decade) %>% 
  count(.) %>% 
  rename(beck_count = n) %>% 
  mutate(beck_percent = round(beck_count/nrow(beck_lynchings)*100,2))


decade_comparison <- lynch_by_decade %>% 
  full_join(beck_by_decade) %>% 
  #it's percentage point, not percentage difference - rsw
  #  mutate(percent_diff = lynch_percent - beck_percent)
    mutate(pct_point_diff = lynch_percent - beck_percent)

# write.csv(decade_comparison, "/Users/robwells/Code/hcij_lynching_phase_two/narratives/output/decade_comparison.csv")

write_sheet(decade_comparison, "https://docs.google.com/spreadsheets/d/1udiQCGe-Qou-WlAiBYJgW23AccOx-_4pNHKNjzIreXA/edit#gid=0", sheet = "oct_nine_decades")

```

# CLEAN MAIN INDEX

## Cleaning lists
```{r}
alabama <- c("Ala")
alaska <- c("Alas", "Alask")
arizona <- c("Ari", "Ariz", "ArizT", "ATA", "Cochise", "Marico", "Pima", "PimaCo", "Pin")
arkansas <- c("A", "Ark", "Arkcurrent", "Faulkne", "Ar")
california <- c("Butte", "Ca", "Cal", "Calif", "El", "ElDora", "Placer", "Cali")
colorado <- c("Ba", "Col", "Colo", "Conejo", "DeltaCo", "Elbert", "Logan", "Montez", "RubyCaGunnisonCountyColo", "Wel", "WeldCou")
connecticut <- c("Con", "Conn", "Litc")
delaware <- c("De", "Del")
florida <- c("F", "Fl", "Fla", "MarionC")
georgia <- c("Ga")
hawaii <- c("Hawai", "HawaiianIsland", "HI", "Maui", "TH")
idaho <- c("Id", "Ida", "Idah", "Idaho", "North")
illinois <- c("Il", "Ill")
indiana <- c("Ia", "IAi", "Ind", "MarshallCountyInd", "Ran", "Wayne")
iowa <- c("Audubo", "CedarC", "How", "Howard", "Io", "Iow", "Maha", "TamaCo", "Winnes", "I")
kansas <- c("AllenCou", "K", "Ka", "Kan", "Kansa", "Sum")
kentucky <- c("Bourbon", "Ky", "Madis", "Wo")
louisiana <- c("Attak", "Bossi", "Calcasie", "GrantP", "L", "La", "Lna", "P", "Par", "Pari")
maine <- c("Me")
maryland <- c("Md")
massachusetts <- c()
michigan <- c("Mic", "Mich", "StClair", "LSM")
minnesota <- c("Be", "Beltra", "BlueE", "Brown", "Goodh", "Min", "Minn", "MT", "PineCou", "Stevens", "StL", "St")
mississippi <- c("Cop", "DeSot", "Lafayet", "Lefl", "MT", "Marshal", "Mis", "Miss", "Stone")
missouri <- c("Ada", "Audrain", "Ch", "IronC", "Lafa", "Missour", "Mo", "RayC", "Salin", "ScottC")
montana <- c("MTM", "Mont", "Montcurrent", "Mon")
nebraska <- c("BoxB", "Cher", "H","Neb", "Nebr", "Nebra", "Nebras", "Nebrask", "Nem", "Webs")
nevada <- c("Nev", "NT")
new_hampshire <- c()
new_jersey <- c("NJ")
new_mexico <- c("Gu", "MoraCo", "NM", "NMT", "Rooseve", "Socorr", "Torra")
new_york <- c("NY")
north_carolina <- c("E", "Edge", "Meck", "NC", "Watauga")
north_dakota <- c("Billings", "Bott", "D", "DickeyC", "DT", "Gr", "McLea", "ND", "Richl", "Stark", "Star", "Stut", "Ward", "WardCou", "Will")
ohio <- c("Ashlan", "Bro", "Hancoc", "High", "Ho", "HolmesCoOOhio", "Mahon", "Meigs", "O", "Oh", "Ohi", "OO", "OOh", "OOhi", "OOhio", "Sandus", "StarkC", "Vinto", "Woo")
oklahoma <- c("Choctaw", "CraigC", "Indi", "India", "Indian", "IndianT", "IndTe", "Okla", "Oklah", "OTO")
oregon <- c("Lin", "Lincoln", "LinnCo", "Morrow", "Or", "Orego", "Wal")
pennsylvania <- c("Pa")
rhode_island <- c()
south_carolina <- c("Claren", "SC")
south_dakota <- c("Bl", "Broo", "Brule", "Dako", "S", "DayCo", "DTS", "Haakon", "HandCo", "Pen", "Rober", "SD", "South", "Stanley", "Unio")
tennessee <- c("GibsonC", "Hardem", "McNairy", "MorganC", "T", "Te", "Ten", "Tenn")
texas <- c("Tex")
utah <- c("CityUt", "U", "Uta")
vermont <- c("St. Johnsb", "Orlean", "Vt")
virginia <- c("Augus", "Highl", "V", "Va")
washington <- c("Cheha", "OT", "Wa", "Wash", "Washin", "WT", "Was")
w_dc <- c("DC")
west_virginia <- c("VaW", "WV", "WVa", "W")
wisconsin <- c("Ashland", "Jeff", "Pi", "Rusk", "Vi", "Wi", "Wis")
wyoming <- c("Carbo", "Wyo")

```

## Clean given states
```{r}
main_index <- read_csv("../data/mainindex.csv")

x <- main_index %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))

clean_main_index <- main_index %>% 
  # Clean out any numbers
  mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>% 
  # Clean out any non-letter characters
  mutate(newspaper_state_clean = str_squish(gsub("\\W", "", newspaper_state_clean))) %>% 
  mutate(newspaper_state_clean = case_when(
    newspaper_state_clean %in% alabama ~ "AL",
    newspaper_state_clean %in% alaska ~ "AK",
    newspaper_state_clean %in% arizona ~ "AZ",
    newspaper_state_clean %in% arkansas ~ "AR",
    newspaper_state_clean %in% california | str_detect(newspaper_city, "Grass Valley") ~ "CA",
    newspaper_state_clean %in% colorado | str_detect(newspaper_city, "Cañon City") ~ "CO",
    newspaper_state_clean %in% connecticut | str_detect(newspaper_city, "New Britain") ~ "CT",
    newspaper_state_clean %in% delaware ~ "DE",
    newspaper_state_clean %in% w_dc ~ "DC",
    newspaper_state_clean %in% florida ~ "FL",
    newspaper_state_clean %in% georgia ~ "GA",
    newspaper_state_clean %in% hawaii ~ "HI",
    newspaper_state_clean %in% idaho | (newspaper_state_clean == "I" & str_detect(newspaper_city, "Silver City")) ~ "ID",
    newspaper_state_clean %in% illinois ~ "IL",
    newspaper_state_clean %in% indiana | str_detect(newspaper_city, "Bloomington|Indianapolis") ~ "IN",
    newspaper_state_clean %in% iowa | str_detect(newspaper_city, "Mashalltown|Independence")~ "IA",
    newspaper_state_clean %in% kansas ~ "KS",
    newspaper_state_clean %in% kentucky ~ "KY",
    newspaper_state_clean %in% louisiana ~ "LA",
    newspaper_state_clean %in% maine ~ "ME",
    newspaper_state_clean %in% maryland | str_detect(newspaper_city, "Port Tobacco|Leonard Town") ~ "MD",
    newspaper_state_clean %in% michigan | (newspaper_state_clean == "M" & str_detect(newspaper_city, "Grand Rapids")) | str_detect(newspaper_city, "East Saginaw|Constantine|Grand Haven") ~ "MI",
    newspaper_state_clean %in% montana | str_detect(newspaper_city, "Stevensville|Diamond City|Philipsburg|Great Falls|Fort Benton") ~ "MT",
    newspaper_state_clean %in% missouri | str_detect(newspaper_city, "Farmington") ~ "MO",
    newspaper_state_clean %in% minnesota | str_detect(newspaper_city, "Grand Marais|Little Falls|Fergus Falls|Sauk Rapids|Minneapolis|White Earth|Worthington") ~ "MN",
    newspaper_state_clean %in% mississippi | (newspaper_state_clean == "M" & str_detect(newspaper_city, "Philadelphia|Water Valley")) | (newspaper_state_clean == "Mi" & str_detect(newspaper_city, "Hattiesburg|Leakesville|Poplarville|Port Gibson")) ~ "MS",
    newspaper_state_clean %in% nebraska | str_detect(newspaper_city, "North Platte|Grand Island|Nemaha City|Dakota City") ~ "NE",
    newspaper_state_clean %in% nevada | (newspaper_state_clean == "Ne" & str_detect(newspaper_city, "Silver City")) | str_detect(newspaper_city, "Carson City|Gardnerville") ~ "NV",
    newspaper_state_clean %in% new_jersey | str_detect(newspaper_city, "Mount Holly|Perth Amboy|Penn's Grove") ~ "NJ",
    newspaper_state_clean %in% new_mexico | str_detect(newspaper_city, "Silver City|Albuquerque") ~ "NM",
    newspaper_state_clean %in% new_york ~ "NY",
    newspaper_state_clean %in% north_carolina | (newspaper_state_clean == "N" & str_detect(newspaper_city, "Hillsborough")) | str_detect(newspaper_city, "Jacksonville|Chapel Hill") ~ "NC",
    newspaper_state_clean %in% north_dakota | str_detect(newspaper_city, "Bismarck|Pembina|Valley City|Grand Forks") ~ "ND",
    newspaper_state_clean %in% oregon | str_detect(newspaper_city, "Pendleton") ~ "OR",
    newspaper_state_clean %in% ohio ~ "OH",
    newspaper_state_clean %in% oklahoma ~ "OK",
    newspaper_state_clean %in% pennsylvania ~ "PA",
    newspaper_state_clean %in% south_carolina ~ "SC",
    newspaper_state_clean %in% wisconsin ~ "WI",
    newspaper_state_clean %in% south_dakota | str_detect(newspaper_city, "Gann Valley|Mitchell|Hurley|Miller") ~ "SD",
    newspaper_state_clean %in% texas | str_detect(newspaper_city, "Brownsville|San Antonio") ~ "TX",
    newspaper_state_clean %in% tennessee ~ "TN",
    newspaper_state_clean %in% utah ~ "UT",
    newspaper_state_clean %in% vermont ~ "VT",
    newspaper_state_clean %in% washington | str_detect(newspaper_city, "North Yakima|White Bluffs") ~ "WA",
    newspaper_state_clean %in% west_virginia | str_detect(newspaper_city, "Charles|Clarksburg|Lewisburg|Wheeling|Morgantown") ~ "WV",
    newspaper_state_clean %in% virginia ~ "VA",
    newspaper_state_clean %in% wisconsin ~ "WI",
    newspaper_state_clean %in% wyoming ~ "WY",
    TRUE ~ newspaper_state_clean
  ))

y <- clean_main_index %>% 
  count(newspaper_state_clean) %>% 
  arrange(desc(n))

#347 categories in the newspaper_states, needs to be cleaned to 50 or so categories in standard two-digit state format, ie. GA for Georgia, CA for California

write.csv(clean_main_index,"../data/mainindex_10_25.csv")

```

## Filters for ga, ms, na titles
```{r}
ga_papers <- clean_main_index %>% 
  filter(newspaper_state_clean =="GA") %>% 
  group_by(newspaper_state_clean, newspaper_name) %>% 
  count()

#write.csv(ga_papers,"../output/ga_papers.csv")

ga_papers_years <- clean_main_index %>% 
  filter(newspaper_state_clean =="GA") %>% 
  group_by(newspaper_state_clean, newspaper_name, year) %>% 
  count()

#write.csv(ga_papers_years,"../output/ga_papers_years.csv")


ms_papers <- clean_main_index %>% 
  filter(newspaper_state_clean =="MS") %>% 
  group_by(newspaper_state_clean, newspaper_name) %>% 
  count()

#write.csv(ms_papers,"../output/ms_papers.csv")

ms_papers_years <- clean_main_index %>% 
  filter(newspaper_state_clean =="MS") %>% 
  group_by(newspaper_state_clean, newspaper_name, year) %>% 
  count()

#write.csv(ms_papers_years,"../output/ms_papers_years.csv")


#isolate the NAs
df_na <- clean_main_index %>% filter(if_all(c(newspaper_state_clean), ~ is.na(.)))


df_na %>% 
    count(newspaper_city) %>% 
  arrange(desc(n))


```
Alabama: "Ala
Alaska: "Alas", "Alask
Arizona: "Ari", "Ariz", "ArizT", "ATA", "Cochise", "Marico", "Pima", "PimaCo", "Pin
Arkansas: "A", "Ark", "Arkcurrent", "Faulkne
California: "Butte", "Ca", "Cal", "C & Grass Valley", "Calif", "El", "ElDora", "Placer", "Cali"
Colorado: "Ba", "Co & Cañon City", "Col", "Colo", "Conejo", "DeltaCo", "Elbert", "Logan", "Montez", "RubyCaGunnisonCountyColo", "Wel", "WeldCou
Connecticut: "Co & New Britain", "Con", "Conn", "Litc
Delaware: "De", "Del", "
Florida: "F", "Fl", "Fla", "MarionC
Georgia: "Ga
Hawaii: "Hawai", "HawaiianIsland", "HI", "Maui", "TH
Idaho: "I & Silver City", "Id", "Ida", "Idah", "Idaho", "North
Illinois: "Il", "Ill
Indiana: "I & Bloomington", "I & Indianapolis", "Ia", "Iai", "Ind", "MarshallCountyInd", "Ran", "Wayne
Iowa: "Audubo", "CedarC", "How", "Howard", "I & Marshalltown", "I & Independence", "Io", "Iow", "Maha", "TamaCo", "Winnes
Kansas: "Allen Cou", "K", "Ka", "Kan", "Kansa", "Sum"
Kentucky: "Bourbon", "Ky," "Madis", "Wo
Louisiana: "Attak", "Bossi", "Calcasie", "GrantP", "L", "La", "Lna", "P", "Par", "Pari
Maine: "Me
Maryland: "M & Port Tobacco", "M & Leonard Town", "Md
Massachusetts
Michigan: "LSM", "M & Grand Rapids", "M & East Saginaw", "Mic", "Mich", "StClair", "Constantine & Mi", "Grand Haven & Mi"
Minnesota: "Be", "Beltra", "BlueE", "Brown", "C & Grand Marais", "Goodh", "M & Little Falls", "M & Fergus Falls", "M & Grand Marais", "M & Sauk Rapids", "Min", "Minn", "MT", "PineCou", "St (Excepting Missouri)", "Stevens", "StL", "Minneapolis & Mi", "White Earth & Mi", "Worthington & Mi
Mississippi: "Cop", "DeSot", "Lafayet", "Lefl", "M & Water Valley", "M & Philadelphia", "MT", "Marshal", "Mis", "Miss", "Stone", "Hattiesburg & Mi", "Leakesville & Mi", "Poplarville & Mi", "Port Gibson & Mi
Missouri: "Ada", "Audrain", "Ch", "IronC", "Lafa", "Missour", "Mo (excepting Montana examples)", "RayC", "Salin", "ScottC", "Farmington & St"
Montana: "M & Stevensville", "M & Diamond City", "MTM", "Philipsburg & Mo", "Mo & Great Falls", "Mo & Fort Benton", "Mont", "Montcurrent", "MTM", "Mon"
Nebraska: "BoxB", "Cher", "H", "N & North Platte", "N & Grand Island", "Ne & Nemaha City", "Ne & Dakota City", "Neb", "Nebr", "Nebra", Nebras", "Nebrask", "Nem", "Webs
Nevada: "Ne & Carson City", "Ne & Silver City", "Nev", "NT
New Hampshire
New Jersey: "Bu & Mount Holly", "N & Perth Amboy", "NJ", "Penn's Grove & S
New Mexico: "Gu", "MoraCo", "N & Silver City", "N & Albuquerque", "Ne & Albuquerque", "NM", "NMT", "Rooseve", "Socorr", "Torra
New York: "NY
North Carolina: "E", "Edge", "Meck", "N & Jacksonville", "N & Hillsborough", "N & Chapel Hill", "NC", "Watauga
North Dakota: "Billings", "Bott", "D", "Dakot & Bismarck", "Dakota & Pembina", 'Star", "DickeyC", "DT", "Gr", "McLea", "N & Valley City", "N & Grand Forks", "ND", "Richl", "Stark", "Stut", "Ward", "WardCou", "Will
Ohio: "Ashlan", "Bro", "Hancoc", "High", "Ho", "HolmesCoOOhio", "Mahon", "Meigs", "O (Excepting Oregon)", "Oh", "Ohi", "OO", "OOh", "OOhi", "OOhio", "Sandus", "StarkC", "Vinto", "Woo
Oklahoma: "Choctaw", "CraigC", "Indi", "India", "Indian", "IndianT", "IndTe", "Okla", "Oklah", "OTO
Oregon: "Lin", "Lincoln", "LinnCo", "Morrow", "O & Pendleton", "Or", "Orego", "Wal
Pennsylvania: "Pa
Rhode Island
South Carolina: "Claren", "SC
South Dakota: "Bl", "Broo", "Brule", "Gann Valley & Dakota", "Dako", "Dakot & Mitchell", "S (Excepting NJ)", "Dakota & Hurley", "Dakota & Miller", "DayCo", "DTS", "Haakon", "HandCo", "Pen", "Rober", "SD", "South", "Stanley", "Unio
Tennessee: "GibsonC", "Hardem", "McNairy", "MorganC", "T", "Te (excepting Texas results)", "Ten", "Tenn
Texas: "Brownsville & Te", "San Antonio & Te", "Tex
Utah: "CityUt", "U", "Uta
Vermont: "St. Johnsb", "Orlean", "Vt
Virginia: "Augus", "Highl", "V (Excepting WV)
Washington: "Cheha", "OT", "North Yakima & W", "White Bluffs & W", "Wa", "Wash", "Washin", "WT", "Port Townsend", "Was"
Washington D.C.: "DC
West Virginia: "Charles Town & V", "Charlestown & Va", "Clarksburg & Va", "Lewisburg & Va", "Morgantown & Va", "Wheeling & Va", W, "VaW", "WV", "WVa
Wisconsin: "Ashland", "Jeff", "Pi", "Rusk", "Vi", "Wi", "Wis
Wyoming: "Carbo", "Wyo

## Clean NAs
```{r}
new_main_index <- read_csv("../data/mainindex_10_25.csv")

clean_main_index <- new_main_index %>% 
  mutate(newspaper_state_clean = case_when(
    str_detect(newspaper_city, "San Francisco|Los Angeles|Sacramento") ~ "CA",
    str_detect(newspaper_city, "Salt Lake City|Great Salt Lake") ~ "UT",
    str_detect(newspaper_city, "Washington") ~ "DC",
    str_detect(newspaper_city, "York") ~ "NY",
    str_detect(newspaper_city, "Richmond|Lynchburg") ~ "VA",
    str_detect(newspaper_city, "Grand Rapids") ~ "MI",
    str_detect(newspaper_city, "Mineral Point") ~ "WI",
    str_detect(newspaper_city, "Honolulu") ~ "HI",
    str_detect(newspaper_city, "Omaha") ~ "NE",
    str_detect(newspaper_city, "Shepherdstown") ~ "WV",
    str_detect(newspaper_city, "Ste. Genevieve") ~ "MO",
    str_detect(newspaper_city, "Maysville") ~ "KY",
    str_detect(newspaper_city, "Savannah") ~ "GA",
    str_detect(newspaper_city, "Winston|Hendersonville") ~ "NC",
    str_detect(newspaper_city, "West Randolph|St. Johnsbury") ~ "VT",
    str_detect(newspaper_city, "Redwood Falls") ~ "MN",
    str_detect(newspaper_city, "St. Clairsville|Hillsborough") ~ "OH",
    str_detect(newspaper_city, "Waterbury|New Haven") ~ "CT",
    str_detect(newspaper_city, "Medicine Lodge|Baxter Springs") ~ "KS",
    str_detect(newspaper_city, "Donaldsonville|New Orleans") ~ "LA",
    str_detect(newspaper_name, "Le Meschacébé|The weekly messenger") ~ "LA",
    TRUE ~ newspaper_state_clean
  ))

filtered_main_index <- clean_main_index %>% 
  filter(is.na(newspaper_state_clean)) %>% 
  filter(!is.na(newspaper_city))
  

cities_list <- filtered_main_index %>% 
  group_by(newspaper_city) %>% 
  count()

no_city <- clean_main_index %>% 
  filter(is.na(newspaper_state_clean)) %>% 
  filter(is.na(newspaper_city))

no_city %>% 
  count(newspaper_state)

write.csv(new_main_index,"../data/mainindex_10_30.csv")

```


```{r}
ga_papers <- new_main_index %>% 
  filter(newspaper_state_clean =="GA") %>% 
  group_by(newspaper_state_clean, newspaper_name) %>% 
  count()
sum(ga_papers$n)
#997

ga_papers1 <- clean_main_index %>% 
  filter(newspaper_state_clean =="GA") %>% 
  group_by(newspaper_state_clean, newspaper_name) %>% 
  count()
sum(ga_papers1$n)
#1085

#write.csv(ga_papers,"../output/ga_papers.csv")

ga_papers_years <- new_main_index %>% 
  filter(newspaper_state_clean =="GA") %>% 
  group_by(newspaper_state_clean, newspaper_name, year) %>% 
  count()

#write.csv(ga_papers_years,"../output/ga_papers_years.csv")


ms_papers <- new_main_index %>% 
  filter(newspaper_state_clean =="MS") %>% 
  group_by(newspaper_state_clean, newspaper_name) %>% 
  count()
#sum(ms_papers$n)
#1084

ms_papers1 <- clean_main_index %>% 
  filter(newspaper_state_clean =="MS") %>% 
  group_by(newspaper_state_clean, newspaper_name) %>% 
  count()
#sum(ms_papers1$n)
#1084

#write.csv(ms_papers,"../output/ms_papers.csv")

ms_papers_years <- new_main_index %>% 
  filter(newspaper_state_clean =="MS") %>% 
  group_by(newspaper_state_clean, newspaper_name, year) %>% 
  count()



```

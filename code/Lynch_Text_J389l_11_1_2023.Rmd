---
title: "Lynching Text Analysis"
author: "Rob Wells"
date: '2023-11-1'
output: html_document
---

```{r}
library(tidyverse)
#install.packages("tidytext")
library(tidytext)
library(rio)
#install.packages("quanteda") 
library(quanteda)
```
#Jour 389L version
#This notebook imports the latest dataframe of 6,448 articles, tokenizes the text and creates separate .csv files with top phrases by decade


```{r}
#import df 
#lynch <- read_csv("articles_df.csv")

lynch <- read_csv("../data/articles_oct_19.csv")

```


# plot of years covered
```{r}

#Range of years covered
years_ct <- lynch %>%
  distinct(filename, .keep_all = TRUE) %>% 
  count(year)

y <- lynch %>%
  distinct(filename, .keep_all = TRUE)

#Chart of years
ggplot(years_ct,aes(x = year, y = n,
             fill = n)) +
  geom_col(position = "dodge") + 
  theme(legend.position = "none") +
  labs(title = "Years of Lynching Coverage",
       subtitle = "Based in 7,162 extracted articles",
       caption = "Graphic by Rob Wells, 10-30-2023",
       y="Articles",
       x="Year")

# ggsave("../output_images_tables/Figure2_years_lynching_coverage_10.30.23.png",device = "png",width=9,height=6, dpi=800)
```
# By decade

## pre1850s
```{r}
pre1850 <- lynch %>% 
  filter(year < 1850)

pre1850 %>% 
  select(filename) %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  count(filename) %>% 
  summarize(total =sum(n)) 
#132 articles prior to 1850

statespre1850s <- pre1850 %>% 
  select(newspaper_state, filename) %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))

statespre1850s %>% 
  select(newspaper_state, n) %>% 
slice_max(n, n=10)

# Most of the coverage in northern states, but Virginia and West Virginia were notable
# Vermont	22			
# New York	15			
# Wisconsin	14			
# West Virginia	12			
# Virginia	11			
# District of Columbia	10			
# Ohio	10	

#Fact Check
#sum(statespre1850s$n)
x <- pre1850 %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  arrange(date)

#write_csv(x, "../output/pre1850s_index.csv")
```
##1850s

```{r}
the1850s <-  lynch %>% 
  filter(year >= 1850 & year <=1859)

the1850s %>% 
  select(filename) %>% 
 distinct(filename, .keep_all = TRUE) %>% 
  count(filename) %>% 
  summarize(total =sum(n)) 
#469 articles prior to 1880s

statesthe1850s <- the1850s %>% 
  select(newspaper_state, filename) %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))

statesthe1850s %>% 
  select(newspaper_state, n) %>% 
slice_max(n, n=15)
# Southern state coverage is prominent
# Virginia	106			
# Ohio	77			
# North Carolina	29			
# District of Columbia	28			
# Louisiana	28			
# Vermont	22			
# Iowa	20			
# Indiana	19			
# Wisconsin	19			
# New York	16	

#Fact Check
#sum(statesthe1850s$n)

x <- the1850s %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  arrange(date)

write_csv(x, "../output/the1850s_index.csv")
```

##1860s

```{r}
the1860s <-  lynch %>% 
  filter(year >= 1860 & year <=1869)

the1860s %>% 
  select(filename) %>% 
 distinct(filename, .keep_all = TRUE) %>% 
  count(filename) %>% 
  summarize(total =sum(n)) 
#108 articles 

statesthe1860s <- the1860s %>% 
  select(newspaper_state, filename) %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))

statesthe1860s %>% 
  select(newspaper_state, n) %>% 
slice_max(n, n=10)
newspaper_state

# Ohio	14			
# Virginia	13			
# New York	10			
# Illinois	9			
# District of Columbia	7			
# Iowa	7			
# North Carolina	7			
# West Virginia	7			
# Delaware	5			
# Wisconsin	5	

#Fact Check
#sum(statesthe1850s$n)

x <- the1860s %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  arrange(date)

write_csv(x, "../output/the1860s_index.csv")
```

##1870s

```{r}
the1870s <-  lynch %>% 
  filter(year >= 1870 & year <=1879)

the1870s %>% 
  select(filename) %>% 
 distinct(filename, .keep_all = TRUE) %>% 
  count(filename) %>% 
  summarize(total =sum(n)) 
#188 articles 

statesthe1870s <- the1870s %>% 
  select(newspaper_state, filename) %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))

statesthe1870s %>% 
  select(newspaper_state, n) %>% 
slice_max(n, n=10)
#newspaper_state

# Ohio	26			
# Louisiana	16			
# Tennessee	16			
# Illinois	13			
# Vermont	10			
# Maryland	9			
# West Virginia	9			
# Alabama	8			
# Nevada	7			
# Delaware	6	

#Fact Check
#sum(statesthe1850s$n)

x <- the1870s %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  arrange(date)

write_csv(x, "../output/the1870s_index.csv")
```
##1880s

```{r}
the1880s <-  lynch %>% 
  filter(year >= 1880 & year <=1889)

the1880s %>% 
  select(filename) %>% 
 distinct(filename, .keep_all = TRUE) %>% 
  count(filename) %>% 
  summarize(total =sum(n)) 
#826 articles 

statesthe1880s <- the1880s %>% 
  select(newspaper_state, filename) %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))

statesthe1880s %>% 
  select(newspaper_state, n) %>% 
slice_max(n, n=10)
#newspaper_state

# Wisconsin	62			
# Minnesota	61			
# Kansas	51			
# Ohio	48			
# Mississippi	36			
# Montana	36			
# Kentucky	35			
# Alabama	33			
# Georgia	30			
# Tennessee	26	

#Fact Check
#sum(statesthe1850s$n)

x <- the1880s %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  arrange(date)

write_csv(x, "../output/the1880s_index.csv")
```
##1890s

```{r}
the1890s <-  lynch %>% 
  filter(year >= 1890 & year <=1899)

the1890s %>% 
  select(filename) %>% 
 distinct(filename, .keep_all = TRUE) %>% 
  count(filename) %>% 
  summarize(total =sum(n)) 
#1637 articles 

statesthe1890s <- the1890s %>% 
  select(newspaper_state, filename) %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  count(newspaper_state) %>% 
  arrange(desc(n))

statesthe1890s %>% 
  select(newspaper_state, n) %>% 
slice_max(n, n=10)
#newspaper_state

# Kansas	115			
# Wisconsin	102			
# Missouri	86			
# Kentucky	83			
# Minnesota	83			
# North Dakota	75			
# Georgia	73			
# South Dakota	71			
# North Carolina	57			
# Mississippi	54	

#Fact Check
#sum(statesthe1850s$n)

x <- the1890s %>% 
  distinct(filename, .keep_all = TRUE) %>% 
  arrange(date)

write_csv(x, "../output/the1890s_index.csv")
```

# Tokenize

```{r}

stories <- str_replace_all(the1890s$sentence, "- ", "")
stories_df <- tibble(stories,)

# unnest includes lower, punct removal

stories_tokenized <- stories_df %>%
  unnest_tokens(word,stories)

stories_tokenized

#Remove stopwords

data(stop_words)

stories_tokenized <- stories_tokenized %>%
  anti_join(stop_words, by = c("word" = "word")) %>%
  filter(word != "temp_file") %>%
  #NOT SURE IF THIS LINE SHOULD REMAIN
  filter(word != "stories_corpus") %>%
  filter(!grepl('[0-9]', word))

# fix the script so it doesn't pick up these file names, numbers  
# forcibly removing for now


# Word Count

story_word_ct <- stories_tokenized %>%
  count(word, sort=TRUE)

#write_csv(lynch_word_ct, "lynching_corpus_word_count.csv")

```



# Bigrams

```{r}
stories_bigrams <- stories_df %>%
  unnest_tokens(bigram, stories, token="ngrams", n=2)

stories_bigrams

#Filter out stop words.


stories_bigrams_separated <- stories_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

stories_bigrams_filtered <- stories_bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

stories_bigram_cts <- stories_bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

# put back into bigram form if we want to use them
stories_bigrams_united <- stories_bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

#replace Date for the decade analyzed
stories_bigram_cts_1890s <- stories_bigram_cts %>% 
  mutate(decade = "1890")

write_csv(stories_bigram_cts_1890s, "../output/1890s_lynch_bigram_count.csv")

```

# Trigrams

```{r}
stories_trigrams <- stories_df %>%
  unnest_tokens(trigram, stories, token="ngrams", n=3)

stories_trigrams_separated <- stories_trigrams %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ")

stories_trigrams_ct <- stories_trigrams_separated %>%
  count(word1, word2, word3, sort = TRUE)

#filtered
# stories_trigrams_filtered <- stories_trigrams_separated %>%
#   filter(!word1 %in% stop_words$word) %>%
#   filter(!word2 %in% stop_words$word) %>%
#   filter(!word3 %in% stop_words$word)
# 
# stories_trigrams_ct <- stories_trigrams_filtered %>%
#   count(word1, word2, word3, sort = TRUE)

#replace Date for the decade analyzed
stories_trigrams_ct_1890s <- stories_trigrams_ct %>% 
  mutate(decade = "1890")

write_csv(stories_trigrams_ct_1890s, "../output/1890s_lynch_trigram_count.csv")


```

# Quintgrams

```{r}
stories_QUINTgrams <- stories_df %>%
  unnest_tokens(phrase, stories, token="ngrams", n=5)

stories_QUINTgrams_ct <- stories_QUINTgrams %>%
  count(phrase, sort=TRUE)

#write_csv(stories_QUINTgrams_ct, "stories_corpus_quintgram_count.csv")

stories_QUINTgrams_ct

```

north_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "North Central" | newspaper_region == "Northeast") %>%
group_by(decade) %>%
summarise(northern_count = n()) %>%
arrange(desc(northern_count))
west_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "West") %>%
group_by(decade) %>%
summarise(western_count = n()) %>%
arrange(desc(western_count))
overall_count_over_time <- geography_starter_df %>%
group_by(decade) %>%
summarise(total_count = n()) %>%
arrange(desc(total_count))
lynching_geog_over_time <- overall_count_over_time %>% left_join(south_count_over_time, by="decade")
lynching_geog_over_time <- lynching_geog_over_time %>% left_join(north_count_over_time, by='decade')
lynching_geog_over_time <- lynching_geog_over_time %>% left_join(west_count_over_time, by='decade')
View(lynching_geog_over_time)
267+569+148
coverage_occurrences_states <- geography_starter_df %>%
group_by(newspaper_state_code) %>%
summarise(coverage_count = n()) %>%
arrange(desc(coverage_count))
View(coverage_occurrences_states)
lyching_occurrences_states <- geography_starter_df %>%
group_by(state_lynch) %>%
summarise(lynching_count = n()) %>%
arrange(desc(lynching_count))
View(lyching_occurrences_states)
coverage_vs_lynch_occurrence <- left_join(coverage_occurrences_states, lyching_occurrences_states, by=c('newspaper_state_code'='state_lynch'))
View(coverage_vs_lynch_occurrence)
lynching_data <- read.csv("data/lynch_geocoded_9.27.csv")
# vibe check
lynching_data %>%
head(10)
#check for duplicate rows
lynching_data_distinct <- lynching_data %>%
distinct()
#all good
## Regional differences
#starter code
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
write_csv(starter_code, "in_out_of_state_breakdown.csv")
# starter code takeaways
# - northeast only covered one in-state lynching but reported on 222 out of state ones
# - north central region had overwhelmingly the most out of state lynching coverage, which make sense because these states are close to the south in proximity, and therefore it makes sense to cover things, but 1384 out of state vs. 55 in state is crazy
# let's clean up this dataframe
geography_starter_df <- lynching_data %>%
clean_names() %>%
select(newspaper_name1, newspaper_state_code, state_lynch, newspaper_region, in_state, year, decade, lynching_lon, lynching_lat, news_location_lon, news_location_lat)
geography_starter_df <- lynching_data %>%
clean_names() %>%
select(newspaper_name, newspaper_state_code, state_lynch, newspaper_region, in_state, year, decade, lynching_lon, lynching_lat, news_location_lon, news_location_lat)
geography_starter_df %>%
group_by(year) %>%
summarise(total_count = n()) %>%
arrange(desc(total_count))
geography_starter_df %>%
filter(newspaper_region == "South") %>%
group_by(year) %>%
summarise(southern_count = n()) %>%
arrange(desc(southern_count))
# same thing just zooming out for decades -- let's use decades for this preliminary vibe check
south_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "South") %>%
group_by(decade) %>%
summarise(southern_count = n()) %>%
arrange(desc(southern_count))
north_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "North Central" | newspaper_region == "Northeast") %>%
group_by(decade) %>%
summarise(northern_count = n()) %>%
arrange(desc(northern_count))
west_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "West") %>%
group_by(decade) %>%
summarise(western_count = n()) %>%
arrange(desc(western_count))
overall_count_over_time <- geography_starter_df %>%
group_by(decade) %>%
summarise(total_count = n()) %>%
arrange(desc(total_count))
lynching_geog_over_time <- overall_count_over_time %>% left_join(south_count_over_time, by="decade")
lynching_geog_over_time <- lynching_geog_over_time %>% left_join(north_count_over_time, by='decade')
lynching_geog_over_time <- lynching_geog_over_time %>% left_join(west_count_over_time, by='decade')
write_csv(lynching_geog_over_time, "lynching_regional_decades.csv")
# in what state did the most lynching take place?
# in what state was lynching covered the most?
coverage_occurrences_states <- geography_starter_df %>%
group_by(newspaper_state_code) %>%
summarise(coverage_count = n()) %>%
arrange(desc(coverage_count))
lyching_occurrences_states <- geography_starter_df %>%
group_by(state_lynch) %>%
summarise(lynching_count = n()) %>%
arrange(desc(lynching_count))
# the top 5 states where the most lynchings occurred were georgia, tennessee, mississippi, kentucky, alabama and texas
# the top 5 states where the most lynchings were covered were wisconsin, missouri, kansas, ohio, south dakota and north dakota
coverage_vs_lynch_occurrence <- left_join(coverage_occurrences_states, lyching_occurrences_states, by=c('newspaper_state_code'='state_lynch'))
state_in_out_breakdown <- lynch_updated %>%
select(decade, newspaper_state, in_state) %>%
group_by(newspaper_state) %>%
count(in_state)
lynch_updated <- read_csv("data/lynch_geocoded_9.27.csv")
lynch_updated <- lynch_updated %>%
clean_names()
state_in_out_breakdown <- lynch_updated %>%
select(decade, newspaper_state, in_state) %>%
group_by(newspaper_state) %>%
count(in_state)
View(state_in_out_breakdown)
out_of_state_coverage <- state_in_out_breakdown %>%
filter(in_state == "N") %>%
arrange(desc(n))
View(out_of_state_coverage)
states_df <- lynch_updated %>%
select(newspaper_name, newspaper_state_code, state_lynch, in_state)
oos_coverage <- states_df %>%
filter(in_state == "N") %>%
group_by(state_lynch) %>%
count(in_state)
View(oos_coverage)
newspapers_oos <- states_df %>%
filter(in_state == "N") %>%
group_by(newspaper_name) %>%
summarize(count = n()) %>%
arrange(desc(count))
View(newspapers_oos)
states_with_years <- lynch_updated %>%
select(decade, newspaper_state_code, state_lynch, in_state, newspaper_name)
View(states_with_years)
is <- states_with_years %>%
filter(in_state == "Y") %>%
group_by(decade) %>%
summarise(in_state = n())
oos <- states_with_years %>%
filter(in_state == "N") %>%
group_by(decade) %>%
summarise(out_of_state = n())
coverage_by_decade <- is %>% left_join(oos, by='decade')
View(coverage_by_decade)
22+53+18+46+42+31+22+27
261/3
22+53
22+53+18
31+22+27
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
jackindex_geo <- read_csv("../data/extracted_articles_aug25.csv")
View(jackindex_geo)
View(lynch_geocoded_9.27)
lynch <- read_csv("../data/articles_aug_25.csv")
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year)
# append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")
z <- y %>%
select(filename, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
#Count by decade, filter off 1960 as noise
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade==1960)
View(lynch_word_decade)
View(x)
View(lynch)
View(z)
z <- y %>%
select(filename, newspaper_name, URL, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
View(y)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, URL)
# append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")
z <- y %>%
select(filename, newspaper_name, URL, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
View(y)
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
#import df created from sequence below
# lynch <- read_csv("articles_1pct_dec26.csv")
lynch <- read_csv("../data/articles_aug_25.csv")
#index of 1 pct sample which has been checked by a coder and represents all valid entries
# jackindex <- read_csv("../data/jackindex_march8.csv")
# lynch_geocoded_9.27 <- read.csv("../data/lynch_geocoded_9.27.csv")
#black <- read_csv("../data/black_press_4_19_2023.csv")
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, URL)
# append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")
z <- y %>%
select(filename, newspaper_name, URL, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words))
View(z)
lynch_geocoded_9.27 <- read.csv("../data/lynch_geocoded_9.27.csv")
names(lynch_geocoded_9.27)
head(lynch_geocoded_9.27)
View(lynch_geocoded_9.27)
names(lynch_geocoded_9.27)
lynch_geocoded_9.27a <- subset(lynch_geocoded_9.27, select =-X.2, -X.1)
lynch_geocoded_9.27a <- subset(lynch_geocoded_9.27, select -c=(X.2:X.1))
lynch_geocoded_9.27 <- subset(lynch_geocoded_9.27, select =-X.2)
lynch_geocoded_9.27 <- subset(lynch_geocoded_9.27, select =-X.1)
names(z)
z$file_id <- strreplaceall(z$filename, pattern = fixed('.txt'), replacement = fixed())
z$file_id <- str_replace_all(z$filename, pattern = fixed('.txt'), replacement = fixed())
z$file_id <- str_replace_all(z$filename, pattern =fixed('.txt'), replacement = fixed())
z$file_id <- gsub('.txt',"", z$filename)
View(z)
View(lynch_geocoded_9.27)
jackindex_geo <- read_csv("../data/extracted_articles_aug25.csv")
View(jackindex_geo)
googlesheets4::gs4_deauth()
articles <- read_sheet("https://docs.google.com/spreadsheets/d/1zDWMbuoTHVtiJDrrJ9mDfBTp-VxNFTmMHiGcGA-UvjE/edit?usp=sharing") %>%
as.data.frame()
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
googlesheets4::gs4_deauth()
articles <- read_sheet("https://docs.google.com/spreadsheets/d/1zDWMbuoTHVtiJDrrJ9mDfBTp-VxNFTmMHiGcGA-UvjE/edit?usp=sharing") %>%
as.data.frame()
View(articles)
jackindex_geo %>%
count(file_id2) %>%
arrange(desc(n))
jackindex_geo %>%
count(file_id2, file_id)
bf <- jackindex_geo %>%
count(file_id2, file_id)
View(bf)
bf <- jackindex_geo %>%
count(file_id2, file_id) %>%
group_by(file_id) %>%
arrange(desc(file_id))
jackindex_geo %>%
count(file_id2, file_id) %>%
count(file_id)
jackindex_geo %>%
count(file_id2, file_id) %>%
count(file_id) %>%
arrange(desc(n))
lynch_geocoded_9.27 <- read.csv("../data/lynch_geocoded_9.27.csv")
3308-2786
522+2694
View(articles)
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("kableExtra")
library(kableExtra)
#webshot::install_phantomjs()
library(lubridate)
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(month = floor_date(fixed_date, "month")) %>%
select(-date) %>%
rename(paper_state = newspaper_state_code)
View(wire_coverage)
names(lynch_geocoded_10.8)
View(lynch_geocoded_10.8)
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(date)>15)))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))
View(wire_coverage)
wire_grouped <- wire_coverage %>%
group_by(group)
View(wire_grouped)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group)
View(wire_grouped)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=6)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=6) %>%
arrange(desc(n))
#I grouped the stories based on whether they were within 15 days of each other
#This code uses the diff function to calculate the difference between consecutive dates and checks if the difference is greater than 15. It then accumulates the results using cumsum to create a new grouping column called group.
#cumsum: Cumulative Sums, Products, and Extremes
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, URL) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))
View(lynch_geocoded_10.8)
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15))) %>%
group_by(group)
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15))) %>%
group_by(count(group))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))%>%
group_by(group)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=2) %>%
arrange(desc(n))
View(wire_grouped)
wire_coverage <- wire_coverage %>%
filter(lynch_address != "NA, NA" & lynch_address != "None, NONE")
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=2) %>%
arrange(desc(n))
same_towns <- wire_coverage %>%
group_by(lynch_address, group)
View(same_towns)
same_towns <- wire_coverage %>%
group_by(lynch_address)
View(same_towns)
wire_coverage2 <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))%>%
group_by(group)
wire_coverage2 <- wire_coverage2 %>%
filter(lynch_address != "NA, NA" & lynch_address != "None, NONE")
wire_grouped <- wire_coverage2 %>%
group_by(group) %>%
count(group) %>%
filter(n>=2) %>%
arrange(desc(n))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(month = floor_date(fixed_date, "month")) %>%
select(-date) %>%
rename(paper_state = newspaper_state_code) %>%
filter(lynch_address != "NA, NA" & lynch_address != "None, NONE")
duplicates <- wire_coverage[duplicated(wire_coverage$lynch_address) | duplicated(wire_coverage$lynch_address, fromLast = TRUE), ]
View(duplicates)
check_duplicates <- duplicates %>%
group_by(lynch_address) %>%
count()
View(check_duplicates)
city_and_date <- wire_coverage[duplicated(wire_coverage[, c("lynch_address", "month")]) | duplicated(wire_coverage[, c("lynch_address", "month")], fromLast = TRUE), ]
View(city_and_date)
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")]) | duplicated(wire_coverage[, c("lynch_address", "group")], fromLast = TRUE), ]
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")]) | duplicated(wire_coverage2[, c("lynch_address", "group")], fromLast = TRUE), ]
View(same_towns)
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")])
)
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")]]
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")], fromLast = TRUE), ]
<<<<<<< Updated upstream
main_index <- read_csv("../data/mainindex.csv")
=======
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("janitor")
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
maryland <- lynch_geocoded_10.8 %>%
filter(state_lynch=="MD")
#Data needs cleaning on city_lynch - Queen annes county Queen Annes County Queen Annes County (eastern shore)
View(lynch_geocoded_10.8)
>>>>>>> Stashed changes
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
<<<<<<< Updated upstream
#install.packages("tigris")
#install.packages("zoo")
library(tigris)
library(stringr)
library(janitor)
library(zoo)
main_index <- read_csv("../data/mainindex.csv")
x <- main_index %>%
count(newspaper_state) %>%
arrange(desc(n))
View(x)
View(x)
clean_main_index <- main_index %>%
mutate(newspaper_state_clean = case_when(
str_detect(newspaper_state, "[0-9]") ~ str_squish(gsub("[0-9]", "", newspaper_state)),
TRUE ~ nespaper_state
))
clean_main_index <- main_index %>%
mutate(newspaper_state_clean = case_when(
str_detect(newspaper_state, "[0-9]") ~ str_squish(gsub("[0-9]", "", newspaper_state)),
TRUE ~ newspaper_state
))
View(clean_main_index)
View(x)
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state)) %>%
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state)) %>%
# Clean out any non-letter characters
#mutate(newspaper_state_clean = case_when(
))
clean_main_index <- main_index %>%
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
#mutate(newspaper_state_clean = case_when(
#347 categories in the newspaper_states, needs to be cleaned to 50 or so categories in standard two-digit state format, ie. GA for Georgia, CA for California
```
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) #%>%
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
mutate(newspaper_state_clean = str_squish(gsub("\W", "", newspaper_state_clean)))
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
mutate(newspaper_state_clean = str_squish(gsub("\\W", "", newspaper_state_clean)))
y <- clean_main_index %>%
count(newspaper_state_clean) %>%
arrange(desc(n))
View(y)
=======
#install.packages("kableExtra")
library(kableExtra)
#webshot::install_phantomjs()
library(lubridate)
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
filtered_wire_groups <- read_csv("~/Desktop/GitHub/jour389L extra stuff/fixed_filtered_wire_groups.csv")
View(lynch_geocoded_10.8)
View(maryland)
#Data needs cleaning on city_lynch - Queen annes county Queen Annes County Queen Annes County (eastern shore)
maryland <- maryland %>%
str_replace(str_detect("Queen annes|Queen Annes"), "Queen Anne's County")
>>>>>>> Stashed changes

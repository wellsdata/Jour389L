select(filename, sentence, words, year, newspaper_name, URL)
# append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")
z <- y %>%
select(filename, newspaper_name, URL, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
View(y)
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
#import df created from sequence below
# lynch <- read_csv("articles_1pct_dec26.csv")
lynch <- read_csv("../data/articles_aug_25.csv")
#index of 1 pct sample which has been checked by a coder and represents all valid entries
# jackindex <- read_csv("../data/jackindex_march8.csv")
# lynch_geocoded_9.27 <- read.csv("../data/lynch_geocoded_9.27.csv")
#black <- read_csv("../data/black_press_4_19_2023.csv")
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, URL)
# append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")
z <- y %>%
select(filename, newspaper_name, URL, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words))
View(z)
lynch_geocoded_9.27 <- read.csv("../data/lynch_geocoded_9.27.csv")
names(lynch_geocoded_9.27)
head(lynch_geocoded_9.27)
View(lynch_geocoded_9.27)
names(lynch_geocoded_9.27)
lynch_geocoded_9.27a <- subset(lynch_geocoded_9.27, select =-X.2, -X.1)
lynch_geocoded_9.27a <- subset(lynch_geocoded_9.27, select -c=(X.2:X.1))
lynch_geocoded_9.27 <- subset(lynch_geocoded_9.27, select =-X.2)
lynch_geocoded_9.27 <- subset(lynch_geocoded_9.27, select =-X.1)
names(z)
z$file_id <- strreplaceall(z$filename, pattern = fixed('.txt'), replacement = fixed())
z$file_id <- str_replace_all(z$filename, pattern = fixed('.txt'), replacement = fixed())
z$file_id <- str_replace_all(z$filename, pattern =fixed('.txt'), replacement = fixed())
z$file_id <- gsub('.txt',"", z$filename)
View(z)
View(lynch_geocoded_9.27)
jackindex_geo <- read_csv("../data/extracted_articles_aug25.csv")
View(jackindex_geo)
googlesheets4::gs4_deauth()
articles <- read_sheet("https://docs.google.com/spreadsheets/d/1zDWMbuoTHVtiJDrrJ9mDfBTp-VxNFTmMHiGcGA-UvjE/edit?usp=sharing") %>%
as.data.frame()
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
googlesheets4::gs4_deauth()
articles <- read_sheet("https://docs.google.com/spreadsheets/d/1zDWMbuoTHVtiJDrrJ9mDfBTp-VxNFTmMHiGcGA-UvjE/edit?usp=sharing") %>%
as.data.frame()
View(articles)
jackindex_geo %>%
count(file_id2) %>%
arrange(desc(n))
jackindex_geo %>%
count(file_id2, file_id)
bf <- jackindex_geo %>%
count(file_id2, file_id)
View(bf)
bf <- jackindex_geo %>%
count(file_id2, file_id) %>%
group_by(file_id) %>%
arrange(desc(file_id))
jackindex_geo %>%
count(file_id2, file_id) %>%
count(file_id)
jackindex_geo %>%
count(file_id2, file_id) %>%
count(file_id) %>%
arrange(desc(n))
lynch_geocoded_9.27 <- read.csv("../data/lynch_geocoded_9.27.csv")
3308-2786
522+2694
View(articles)
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("kableExtra")
library(kableExtra)
#webshot::install_phantomjs()
library(lubridate)
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(month = floor_date(fixed_date, "month")) %>%
select(-date) %>%
rename(paper_state = newspaper_state_code)
View(wire_coverage)
names(lynch_geocoded_10.8)
View(lynch_geocoded_10.8)
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(date)>15)))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))
View(wire_coverage)
wire_grouped <- wire_coverage %>%
group_by(group)
View(wire_grouped)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group)
View(wire_grouped)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=6)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=6) %>%
arrange(desc(n))
#I grouped the stories based on whether they were within 15 days of each other
#This code uses the diff function to calculate the difference between consecutive dates and checks if the difference is greater than 15. It then accumulates the results using cumsum to create a new grouping column called group.
#cumsum: Cumulative Sums, Products, and Extremes
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, URL) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))
View(lynch_geocoded_10.8)
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15))) %>%
group_by(group)
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15))) %>%
group_by(count(group))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))%>%
group_by(group)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=2) %>%
arrange(desc(n))
View(wire_grouped)
wire_coverage <- wire_coverage %>%
filter(lynch_address != "NA, NA" & lynch_address != "None, NONE")
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=2) %>%
arrange(desc(n))
same_towns <- wire_coverage %>%
group_by(lynch_address, group)
View(same_towns)
same_towns <- wire_coverage %>%
group_by(lynch_address)
View(same_towns)
wire_coverage2 <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))%>%
group_by(group)
wire_coverage2 <- wire_coverage2 %>%
filter(lynch_address != "NA, NA" & lynch_address != "None, NONE")
wire_grouped <- wire_coverage2 %>%
group_by(group) %>%
count(group) %>%
filter(n>=2) %>%
arrange(desc(n))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(month = floor_date(fixed_date, "month")) %>%
select(-date) %>%
rename(paper_state = newspaper_state_code) %>%
filter(lynch_address != "NA, NA" & lynch_address != "None, NONE")
duplicates <- wire_coverage[duplicated(wire_coverage$lynch_address) | duplicated(wire_coverage$lynch_address, fromLast = TRUE), ]
View(duplicates)
check_duplicates <- duplicates %>%
group_by(lynch_address) %>%
count()
View(check_duplicates)
city_and_date <- wire_coverage[duplicated(wire_coverage[, c("lynch_address", "month")]) | duplicated(wire_coverage[, c("lynch_address", "month")], fromLast = TRUE), ]
View(city_and_date)
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")]) | duplicated(wire_coverage[, c("lynch_address", "group")], fromLast = TRUE), ]
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")]) | duplicated(wire_coverage2[, c("lynch_address", "group")], fromLast = TRUE), ]
View(same_towns)
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")])
)
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")]]
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")], fromLast = TRUE), ]
<<<<<<< Updated upstream
main_index <- read_csv("../data/mainindex.csv")
=======
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("janitor")
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
maryland <- lynch_geocoded_10.8 %>%
filter(state_lynch=="MD")
#Data needs cleaning on city_lynch - Queen annes county Queen Annes County Queen Annes County (eastern shore)
View(lynch_geocoded_10.8)
>>>>>>> Stashed changes
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
<<<<<<< Updated upstream
#install.packages("tigris")
#install.packages("zoo")
library(tigris)
library(stringr)
library(janitor)
library(zoo)
main_index <- read_csv("../data/mainindex.csv")
x <- main_index %>%
count(newspaper_state) %>%
arrange(desc(n))
View(x)
View(x)
clean_main_index <- main_index %>%
mutate(newspaper_state_clean = case_when(
str_detect(newspaper_state, "[0-9]") ~ str_squish(gsub("[0-9]", "", newspaper_state)),
TRUE ~ nespaper_state
))
clean_main_index <- main_index %>%
mutate(newspaper_state_clean = case_when(
str_detect(newspaper_state, "[0-9]") ~ str_squish(gsub("[0-9]", "", newspaper_state)),
TRUE ~ newspaper_state
))
View(clean_main_index)
View(x)
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state)) %>%
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state)) %>%
# Clean out any non-letter characters
#mutate(newspaper_state_clean = case_when(
))
clean_main_index <- main_index %>%
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
#mutate(newspaper_state_clean = case_when(
#347 categories in the newspaper_states, needs to be cleaned to 50 or so categories in standard two-digit state format, ie. GA for Georgia, CA for California
```
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) #%>%
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
mutate(newspaper_state_clean = str_squish(gsub("\W", "", newspaper_state_clean)))
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
mutate(newspaper_state_clean = str_squish(gsub("\\W", "", newspaper_state_clean)))
y <- clean_main_index %>%
count(newspaper_state_clean) %>%
arrange(desc(n))
View(y)
=======
#install.packages("kableExtra")
library(kableExtra)
#webshot::install_phantomjs()
library(lubridate)
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
filtered_wire_groups <- read_csv("~/Desktop/GitHub/jour389L extra stuff/fixed_filtered_wire_groups.csv")
View(lynch_geocoded_10.8)
View(maryland)
#Data needs cleaning on city_lynch - Queen annes county Queen Annes County Queen Annes County (eastern shore)
maryland <- maryland %>%
str_replace(str_detect("Queen annes|Queen Annes"), "Queen Anne's County")
>>>>>>> Stashed changes
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("tigris")
#install.packages("zoo")
library(tigris)
library(stringr)
library(janitor)
library(zoo)
main_index <- read_csv("../data/mainindex.csv")
x <- main_index %>%
count(newspaper_state) %>%
arrange(desc(n))
View(x)
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
mutate(newspaper_state_clean = str_squish(gsub("\\W", "", newspaper_state_clean)))
View(clean_main_index)
y <- clean_main_index %>%
count(newspaper_state_clean) %>%
arrange(desc(n))
View(y)
View(main_index)
# Analyze broader dataset of all lynchings by decade and state and region. Want to see if our sample of lynchings by state is skewed. Total and percentage by state. Group in Regional categories. Next, calculate baseline totals of the newspapers in the states.
tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
#Import failed due to the link. Please assign links to Github
# lynch_article_data <- read_csv("~/Desktop/GitHub/Jour389L/data/lynch_geocoded_9.27.csv")
lynch_article_data <- read_csv("../data/lynch_geocoded_9.27.csv")
beck_lynchings <- tolnay_beck %>%
filter(!str_detect(status, "death"))
beck_by_state <- beck_lynchings %>%
group_by(lynch_state) %>%
count(.) %>%
rename(state_lynch = lynch_state) %>%
rename(beck_count = n) %>%
mutate(percent_lynchings_beck = round(beck_count/nrow(beck_lynchings)*100,2))
View(beck_by_state)
View(beck_lynchings)
years <- beck_lynchings %>%
group_by(lynch_state, year) %>%
filter(state_lynch=="GA") %>%
count(.)
years <- beck_lynchings %>%
group_by(lynch_state, year) %>%
filter(lynch_state=="GA") %>%
count(.)
View(years)
write.csv(years, "..\output\ga_years.csv")
write.csv(years, "../output/ga_years.csv")
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("janitor")
library(janitor)
library(lubridate)
library(kableExtra)
library(ggplot2)
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
maryland <- lynch_geocoded_10.8 %>%
filter(state_lynch=="MD")
#Cleaned city_lynch to fix issue with Queen Anne's County and with Bel Air
maryland <- maryland %>%
mutate(city_lynch = ifelse(city_lynch %in% c("Queen annes county", "Queen Annes County", "Queen Annes County (eastern shore)"), "Queen Anne's County", city_lynch)) %>%
mutate(city_lynch = str_replace(city_lynch, "Belair", "Bel Air"))
#Formatting date and adding month column
maryland <- maryland %>%
mutate(date = mdy(date),
month = floor_date(date, "month")) %>%
select(newspaper_name:news_address, newspaper_city:year, city_lynch, state_lynch, lynching.lon:Newspaper_Region, month)
# Unused code
# x <- maryland %>%
#   group_by(city_lynch, year) %>%
#   filter(n()>1)
#
# md_single_cases <- maryland %>%
#     group_by(city_lynch, year) %>%
#     distinct(lynch_address, .keep_all = TRUE)
#
# write.csv(md_single_cases, "../output/md_distinct_cases.csv")
#
# zz <- md_single_cases %>%
#   select(city_lynch, date, newspaper_name)
#Filtering to remove duplicates
distinct_md <- maryland %>%
distinct(city_lynch, month, newspaper_name, .keep_all = TRUE)
md_cases_by_decade <- distinct_md %>%
group_by(decade) %>%
count() %>%
rename(geocoded_count = n)
newspapers <- distinct_md %>%
group_by(newspaper_name, newspaper_state_code) %>%
count()
in_vs_out_of_state <- distinct_md %>%
group_by(in_state) %>%
count()
tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
tolnay_md <- tolnay_beck %>%
filter(lynch_state=="MD") %>%
select(name, year, month, day, status, lynch_county, place, method_of_death, accusation, mob_size, notes, decade) %>%
arrange(year)
#write.csv(tolnay_md, "../output/tolnay_md.csv")
tolnay_by_decade <- tolnay_md %>%
group_by(decade) %>%
count() %>%
rename(tolnay_count = n)
# Comparison of numbers by decade
totals_by_decade <- left_join(md_cases_by_decade, tolnay_by_decade, by = "decade")
#Pulling only cases from Prince George's County
pg_county <- distinct_md %>%
filter(city_lynch == "Prince George's County") %>%
select(city_lynch, year)
tolnay_pg <- tolnay_md %>%
select(year, lynch_county) %>%
filter(lynch_county == "Prince George’s")
total_pg_county <- bind_rows(pg_county, tolnay_pg) %>%
group_by(year) %>%
count()
View(pg_county)
tolnay_pg <- tolnay_md %>%
select(year, lynch_county) %>%
filter(lynch_county == "Prince George’s")
View(tolnay_pg)
View(total_pg_county)
count_tolnay <- tolnay_md %>%
select(decade) %>%
mutate(source = "Tolnay")
count_geocode <- distinct_md %>%
select(decade) %>%
mutate(source = "Our Data")
total_count <- bind_rows(count_tolnay, count_geocode) %>%
group_by(decade, source) %>%
count()
View(total_count)
ggplot(total_count, aes(decade, n, fill = source)) +
geom_bar(stat="identity", position = "dodge", width = 8) +
labs(title="Comparing Lynching Counts in Maryland Per Decade",
x="Decade",
y="Number of Reported Lynchings",
fill = "Source")
edited_distinct <- distinct_md %>%
mutate(in_state = str_replace(in_state, "N", "Out-of-state"),
in_state = str_replace(in_state, "Y", "In-state"))
md_map <- map_data("state", region = "maryland")
map_plot <- ggplot(data = md_map, aes(x = long, y = lat)) +
geom_polygon(fill = "white", color = "black") +
geom_point(data = edited_distinct, aes(x = lynching.lon, y = lynching.lat, color = in_state), size = 3) +
scale_color_manual(values = c("Out-of-state" = "red", "In-state" = "blue")) +
coord_quickmap() +
theme_void() +
labs(title="Map of In-State vs. Out-of-State Coverage for Maryland Lynchings",
color = "Newspaper Coverage") +
theme(axis.title.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank())
print(map_plot)
ggplot(total_count, aes(decade, n, fill = source)) +
geom_bar(stat="identity", position = "dodge", width = 8) +
labs(title="Comparing Lynching Counts in Maryland Per Decade",
x="Decade",
y="Number of Reported Lynchings",
fill = "Source")
print(map_plot)
View(distinct_md)
View(map_plot)
View(tolnay_md)

# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) #%>%
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
mutate(newspaper_state_clean = str_squish(gsub("\W", "", newspaper_state_clean)))
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
mutate(newspaper_state_clean = str_squish(gsub("\\W", "", newspaper_state_clean)))
y <- clean_main_index %>%
count(newspaper_state_clean) %>%
arrange(desc(n))
View(y)
=======
#install.packages("kableExtra")
library(kableExtra)
#webshot::install_phantomjs()
library(lubridate)
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
filtered_wire_groups <- read_csv("~/Desktop/GitHub/jour389L extra stuff/fixed_filtered_wire_groups.csv")
View(lynch_geocoded_10.8)
View(maryland)
#Data needs cleaning on city_lynch - Queen annes county Queen Annes County Queen Annes County (eastern shore)
maryland <- maryland %>%
str_replace(str_detect("Queen annes|Queen Annes"), "Queen Anne's County")
>>>>>>> Stashed changes
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("tigris")
#install.packages("zoo")
library(tigris)
library(stringr)
library(janitor)
library(zoo)
main_index <- read_csv("../data/mainindex.csv")
x <- main_index %>%
count(newspaper_state) %>%
arrange(desc(n))
View(x)
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
mutate(newspaper_state_clean = str_squish(gsub("\\W", "", newspaper_state_clean)))
View(clean_main_index)
y <- clean_main_index %>%
count(newspaper_state_clean) %>%
arrange(desc(n))
View(y)
View(main_index)
# Analyze broader dataset of all lynchings by decade and state and region. Want to see if our sample of lynchings by state is skewed. Total and percentage by state. Group in Regional categories. Next, calculate baseline totals of the newspapers in the states.
tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
#Import failed due to the link. Please assign links to Github
# lynch_article_data <- read_csv("~/Desktop/GitHub/Jour389L/data/lynch_geocoded_9.27.csv")
lynch_article_data <- read_csv("../data/lynch_geocoded_9.27.csv")
beck_lynchings <- tolnay_beck %>%
filter(!str_detect(status, "death"))
beck_by_state <- beck_lynchings %>%
group_by(lynch_state) %>%
count(.) %>%
rename(state_lynch = lynch_state) %>%
rename(beck_count = n) %>%
mutate(percent_lynchings_beck = round(beck_count/nrow(beck_lynchings)*100,2))
View(beck_by_state)
View(beck_lynchings)
years <- beck_lynchings %>%
group_by(lynch_state, year) %>%
filter(state_lynch=="GA") %>%
count(.)
years <- beck_lynchings %>%
group_by(lynch_state, year) %>%
filter(lynch_state=="GA") %>%
count(.)
View(years)
write.csv(years, "..\output\ga_years.csv")
write.csv(years, "../output/ga_years.csv")
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("janitor")
library(janitor)
library(lubridate)
library(kableExtra)
library(ggplot2)
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
maryland <- lynch_geocoded_10.8 %>%
filter(state_lynch=="MD")
#Cleaned city_lynch to fix issue with Queen Anne's County and with Bel Air
maryland <- maryland %>%
mutate(city_lynch = ifelse(city_lynch %in% c("Queen annes county", "Queen Annes County", "Queen Annes County (eastern shore)"), "Queen Anne's County", city_lynch)) %>%
mutate(city_lynch = str_replace(city_lynch, "Belair", "Bel Air"))
#Formatting date and adding month column
maryland <- maryland %>%
mutate(date = mdy(date),
month = floor_date(date, "month")) %>%
select(newspaper_name:news_address, newspaper_city:year, city_lynch, state_lynch, lynching.lon:Newspaper_Region, month)
# Unused code
# x <- maryland %>%
#   group_by(city_lynch, year) %>%
#   filter(n()>1)
#
# md_single_cases <- maryland %>%
#     group_by(city_lynch, year) %>%
#     distinct(lynch_address, .keep_all = TRUE)
#
# write.csv(md_single_cases, "../output/md_distinct_cases.csv")
#
# zz <- md_single_cases %>%
#   select(city_lynch, date, newspaper_name)
#Filtering to remove duplicates
distinct_md <- maryland %>%
distinct(city_lynch, month, newspaper_name, .keep_all = TRUE)
md_cases_by_decade <- distinct_md %>%
group_by(decade) %>%
count() %>%
rename(geocoded_count = n)
newspapers <- distinct_md %>%
group_by(newspaper_name, newspaper_state_code) %>%
count()
in_vs_out_of_state <- distinct_md %>%
group_by(in_state) %>%
count()
tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
tolnay_md <- tolnay_beck %>%
filter(lynch_state=="MD") %>%
select(name, year, month, day, status, lynch_county, place, method_of_death, accusation, mob_size, notes, decade) %>%
arrange(year)
#write.csv(tolnay_md, "../output/tolnay_md.csv")
tolnay_by_decade <- tolnay_md %>%
group_by(decade) %>%
count() %>%
rename(tolnay_count = n)
# Comparison of numbers by decade
totals_by_decade <- left_join(md_cases_by_decade, tolnay_by_decade, by = "decade")
#Pulling only cases from Prince George's County
pg_county <- distinct_md %>%
filter(city_lynch == "Prince George's County") %>%
select(city_lynch, year)
tolnay_pg <- tolnay_md %>%
select(year, lynch_county) %>%
filter(lynch_county == "Prince George’s")
total_pg_county <- bind_rows(pg_county, tolnay_pg) %>%
group_by(year) %>%
count()
View(pg_county)
tolnay_pg <- tolnay_md %>%
select(year, lynch_county) %>%
filter(lynch_county == "Prince George’s")
View(tolnay_pg)
View(total_pg_county)
count_tolnay <- tolnay_md %>%
select(decade) %>%
mutate(source = "Tolnay")
count_geocode <- distinct_md %>%
select(decade) %>%
mutate(source = "Our Data")
total_count <- bind_rows(count_tolnay, count_geocode) %>%
group_by(decade, source) %>%
count()
View(total_count)
ggplot(total_count, aes(decade, n, fill = source)) +
geom_bar(stat="identity", position = "dodge", width = 8) +
labs(title="Comparing Lynching Counts in Maryland Per Decade",
x="Decade",
y="Number of Reported Lynchings",
fill = "Source")
edited_distinct <- distinct_md %>%
mutate(in_state = str_replace(in_state, "N", "Out-of-state"),
in_state = str_replace(in_state, "Y", "In-state"))
md_map <- map_data("state", region = "maryland")
map_plot <- ggplot(data = md_map, aes(x = long, y = lat)) +
geom_polygon(fill = "white", color = "black") +
geom_point(data = edited_distinct, aes(x = lynching.lon, y = lynching.lat, color = in_state), size = 3) +
scale_color_manual(values = c("Out-of-state" = "red", "In-state" = "blue")) +
coord_quickmap() +
theme_void() +
labs(title="Map of In-State vs. Out-of-State Coverage for Maryland Lynchings",
color = "Newspaper Coverage") +
theme(axis.title.x = element_blank(),
axis.text.x = element_blank(),
axis.ticks.x = element_blank(),
axis.title.y = element_blank(),
axis.text.y = element_blank(),
axis.ticks.y = element_blank())
print(map_plot)
ggplot(total_count, aes(decade, n, fill = source)) +
geom_bar(stat="identity", position = "dodge", width = 8) +
labs(title="Comparing Lynching Counts in Maryland Per Decade",
x="Decade",
y="Number of Reported Lynchings",
fill = "Source")
print(map_plot)
View(distinct_md)
View(map_plot)
View(tolnay_md)
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("tigris")
#install.packages("zoo")
library(tigris)
library(stringr)
library(janitor)
library(zoo)
alabama <- c("Ala")
alaska <- c("Alas", "Alask")
arizona <- c("Ari", "Ariz", "ArizT", "ATA", "Cochise", "Marico", "Pima", "PimaCo", "Pin")
arkansas <- c("A", "Ark", "Arkcurrent", "Faulkne", "Ar")
california <- c("Butte", "Ca", "Cal", "Calif", "El", "ElDora", "Placer", "Cali")
colorado <- c("Ba", "Col", "Colo", "Conejo", "DeltaCo", "Elbert", "Logan", "Montez", "RubyCaGunnisonCountyColo", "Wel", "WeldCou")
connecticut <- c("Con", "Conn", "Litc")
delaware <- c("De", "Del")
florida <- c("F", "Fl", "Fla", "MarionC")
georgia <- c("Ga")
hawaii <- c("Hawai", "HawaiianIsland", "HI", "Maui", "TH")
idaho <- c("Id", "Ida", "Idah", "Idaho", "North")
illinois <- c("Il", "Ill")
indiana <- c("Ia", "IAi", "Ind", "MarshallCountyInd", "Ran", "Wayne")
iowa <- c("Audubo", "CedarC", "How", "Howard", "Io", "Iow", "Maha", "TamaCo", "Winnes", "I")
kansas <- c("AllenCou", "K", "Ka", "Kan", "Kansa", "Sum")
kentucky <- c("Bourbon", "Ky", "Madis", "Wo")
louisiana <- c("Attak", "Bossi", "Calcasie", "GrantP", "L", "La", "Lna", "P", "Par", "Pari")
maine <- c("Me")
maryland <- c("Md")
massachusetts <- c()
michigan <- c("Mic", "Mich", "StClair", "LSM")
minnesota <- c("Be", "Beltra", "BlueE", "Brown", "Goodh", "Min", "Minn", "MT", "PineCou", "Stevens", "StL", "St")
mississippi <- c("Cop", "DeSot", "Lafayet", "Lefl", "MT", "Marshal", "Mis", "Miss", "Stone")
missouri <- c("Ada", "Audrain", "Ch", "IronC", "Lafa", "Missour", "Mo", "RayC", "Salin", "ScottC")
montana <- c("MTM", "Mont", "Montcurrent", "Mon")
nebraska <- c("BoxB", "Cher", "H","Neb", "Nebr", "Nebra", "Nebras", "Nebrask", "Nem", "Webs")
nevada <- c("Nev", "NT")
new_hampshire <- c()
new_jersey <- c("NJ")
new_mexico <- c("Gu", "MoraCo", "NM", "NMT", "Rooseve", "Socorr", "Torra")
new_york <- c("NY")
north_carolina <- c("E", "Edge", "Meck", "NC", "Watauga")
north_dakota <- c("Billings", "Bott", "D", "DickeyC", "DT", "Gr", "McLea", "ND", "Richl", "Stark", "Star", "Stut", "Ward", "WardCou", "Will")
ohio <- c("Ashlan", "Bro", "Hancoc", "High", "Ho", "HolmesCoOOhio", "Mahon", "Meigs", "O", "Oh", "Ohi", "OO", "OOh", "OOhi", "OOhio", "Sandus", "StarkC", "Vinto", "Woo")
oklahoma <- c("Choctaw", "CraigC", "Indi", "India", "Indian", "IndianT", "IndTe", "Okla", "Oklah", "OTO")
oregon <- c("Lin", "Lincoln", "LinnCo", "Morrow", "Or", "Orego", "Wal")
pennsylvania <- c("Pa")
rhode_island <- c()
south_carolina <- c("Claren", "SC")
south_dakota <- c("Bl", "Broo", "Brule", "Dako", "S", "DayCo", "DTS", "Haakon", "HandCo", "Pen", "Rober", "SD", "South", "Stanley", "Unio")
tennessee <- c("GibsonC", "Hardem", "McNairy", "MorganC", "T", "Te", "Ten", "Tenn")
texas <- c("Tex")
utah <- c("CityUt", "U", "Uta")
vermont <- c("St. Johnsb", "Orlean", "Vt")
virginia <- c("Augus", "Highl", "V", "Va")
washington <- c("Cheha", "OT", "Wa", "Wash", "Washin", "WT", "Was")
w_dc <- c("DC")
west_virginia <- c("VaW", "WV", "WVa", "W")
wisconsin <- c("Ashland", "Jeff", "Pi", "Rusk", "Vi", "Wi", "Wis")
wyoming <- c("Carbo", "Wyo")
main_index <- read_csv("../data/mainindex.csv")
x <- main_index %>%
count(newspaper_state) %>%
arrange(desc(n))
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
mutate(newspaper_state_clean = str_squish(gsub("\\W", "", newspaper_state_clean))) %>%
mutate(newspaper_state_clean = case_when(
newspaper_state_clean %in% alabama ~ "AL",
newspaper_state_clean %in% alaska ~ "AK",
newspaper_state_clean %in% arizona ~ "AZ",
newspaper_state_clean %in% arkansas ~ "AR",
newspaper_state_clean %in% california | str_detect(newspaper_city, "Grass Valley") ~ "CA",
newspaper_state_clean %in% colorado | str_detect(newspaper_city, "Cañon City") ~ "CO",
newspaper_state_clean %in% connecticut | str_detect(newspaper_city, "New Britain") ~ "CT",
newspaper_state_clean %in% delaware ~ "DE",
newspaper_state_clean %in% w_dc ~ "DC",
newspaper_state_clean %in% florida ~ "FL",
newspaper_state_clean %in% georgia ~ "GA",
newspaper_state_clean %in% hawaii ~ "HI",
newspaper_state_clean %in% idaho | (newspaper_state_clean == "I" & str_detect(newspaper_city, "Silver City")) ~ "ID",
newspaper_state_clean %in% illinois ~ "IL",
newspaper_state_clean %in% indiana | str_detect(newspaper_city, "Bloomington|Indianapolis") ~ "IN",
newspaper_state_clean %in% iowa | str_detect(newspaper_city, "Mashalltown|Independence")~ "IA",
newspaper_state_clean %in% kansas ~ "KS",
newspaper_state_clean %in% kentucky ~ "KY",
newspaper_state_clean %in% louisiana ~ "LA",
newspaper_state_clean %in% maine ~ "ME",
newspaper_state_clean %in% maryland | str_detect(newspaper_city, "Port Tobacco|Leonard Town") ~ "MD",
newspaper_state_clean %in% michigan | (newspaper_state_clean == "M" & str_detect(newspaper_city, "Grand Rapids")) | str_detect(newspaper_city, "East Saginaw|Constantine|Grand Haven") ~ "MI",
newspaper_state_clean %in% montana | str_detect(newspaper_city, "Stevensville|Diamond City|Philipsburg|Great Falls|Fort Benton") ~ "MT",
newspaper_state_clean %in% missouri | str_detect(newspaper_city, "Farmington") ~ "MO",
newspaper_state_clean %in% minnesota | str_detect(newspaper_city, "Grand Marais|Little Falls|Fergus Falls|Sauk Rapids|Minneapolis|White Earth|Worthington") ~ "MN",
newspaper_state_clean %in% mississippi | (newspaper_state_clean == "M" & str_detect(newspaper_city, "Philadelphia|Water Valley")) | (newspaper_state_clean == "Mi" & str_detect(newspaper_city, "Hattiesburg|Leakesville|Poplarville|Port Gibson")) ~ "MS",
newspaper_state_clean %in% nebraska | str_detect(newspaper_city, "North Platte|Grand Island|Nemaha City|Dakota City") ~ "NE",
newspaper_state_clean %in% nevada | (newspaper_state_clean == "Ne" & str_detect(newspaper_city, "Silver City")) | str_detect(newspaper_city, "Carson City|Gardnerville") ~ "NV",
newspaper_state_clean %in% new_jersey | str_detect(newspaper_city, "Mount Holly|Perth Amboy|Penn's Grove") ~ "NJ",
newspaper_state_clean %in% new_mexico | str_detect(newspaper_city, "Silver City|Albuquerque") ~ "NM",
newspaper_state_clean %in% new_york ~ "NY",
newspaper_state_clean %in% north_carolina | (newspaper_state_clean == "N" & str_detect(newspaper_city, "Hillsborough")) | str_detect(newspaper_city, "Jacksonville|Chapel Hill") ~ "NC",
newspaper_state_clean %in% north_dakota | str_detect(newspaper_city, "Bismarck|Pembina|Valley City|Grand Forks") ~ "ND",
newspaper_state_clean %in% oregon | str_detect(newspaper_city, "Pendleton") ~ "OR",
newspaper_state_clean %in% ohio ~ "OH",
newspaper_state_clean %in% oklahoma ~ "OK",
newspaper_state_clean %in% pennsylvania ~ "PA",
newspaper_state_clean %in% south_carolina ~ "SC",
newspaper_state_clean %in% wisconsin ~ "WI",
newspaper_state_clean %in% south_dakota | str_detect(newspaper_city, "Gann Valley|Mitchell|Hurley|Miller") ~ "SD",
newspaper_state_clean %in% texas | str_detect(newspaper_city, "Brownsville|San Antonio") ~ "TX",
newspaper_state_clean %in% tennessee ~ "TN",
newspaper_state_clean %in% utah ~ "UT",
newspaper_state_clean %in% vermont ~ "VT",
newspaper_state_clean %in% washington | str_detect(newspaper_city, "North Yakima|White Bluffs") ~ "WA",
newspaper_state_clean %in% west_virginia | str_detect(newspaper_city, "Charles|Clarksburg|Lewisburg|Wheeling|Morgantown") ~ "WV",
newspaper_state_clean %in% virginia ~ "VA",
newspaper_state_clean %in% wisconsin ~ "WI",
newspaper_state_clean %in% wyoming ~ "WY",
TRUE ~ newspaper_state_clean
))
y <- clean_main_index %>%
count(newspaper_state_clean) %>%
arrange(desc(n))
#347 categories in the newspaper_states, needs to be cleaned to 50 or so categories in standard two-digit state format, ie. GA for Georgia, CA for California
View(y)
write.csv(clean_main_index,"../data/mainindex_10_25.csv")
clean_main_index %>%
group_by(state, newspaper_name) %>%
count()
clean_main_index %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
clean_main_index %>%
filter(newspaper_state_clean =="GA") %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
ga_papers <- clean_main_index %>%
filter(newspaper_state_clean =="GA") %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
write.csv(ga_papers,"../output/ga_papers.csv")
clean_main_index %>%
filter(newspaper_state_clean =="GA") %>%
group_by(newspaper_state_clean, newspaper_name, year) %>%
count()
ga_papers_years <- clean_main_index %>%
filter(newspaper_state_clean =="GA") %>%
group_by(newspaper_state_clean, newspaper_name, year) %>%
count()
write.csv(ga_papers_years,"../output/ga_papers_years.csv")
ms_papers <- clean_main_index %>%
filter(newspaper_state_clean =="MS") %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
View(ms_papers)
write.csv(ms_papers,"../output/ms_papers.csv")
ms_papers_years <- clean_main_index %>%
filter(newspaper_state_clean =="MS") %>%
group_by(newspaper_state_clean, newspaper_name, year) %>%
count()
View(ms_papers_years)
write.csv(ms_papers_years,"../output/ms_papers_years.csv")
View(clean_main_index)
clean_main_index %>%
filter(newspaper_state_clean ==NA) %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
clean_main_index %>%
filter(newspaper_state_clean =="NA") %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
clean_main_index %>%
filter(newspaper_state_clean ==is.na) %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
df_na <- clean_main_index %>% filter(if_all(c(newspaper_state_clean), ~ is.na(.)))
View(df_na)
df_na %>%
count(newspaper_city)
df_na %>%
count(newspaper_city) %>%
arrange(desc(n))
View(ga_papers)
View(ms_papers)
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("janitor")
library(janitor)
library(lubridate)
library(kableExtra)
library(ggplot2)
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
maryland <- lynch_geocoded_10.8 %>%
filter(state_lynch=="MD")
#Cleaned city_lynch to fix issue with Queen Anne's County and with Bel Air
maryland <- maryland %>%
mutate(city_lynch = ifelse(city_lynch %in% c("Queen annes county", "Queen Annes County", "Queen Annes County (eastern shore)"), "Queen Anne's County", city_lynch)) %>%
mutate(city_lynch = str_replace(city_lynch, "Belair", "Bel Air"))
#Formatting date and adding month column
maryland <- maryland %>%
mutate(date = mdy(date),
month = floor_date(date, "month")) %>%
select(newspaper_name:news_address, newspaper_city:year, city_lynch, state_lynch, lynching.lon:Newspaper_Region, month)
tolnay_beck <- read_csv("../data/Bailey_Beck_lynching_list_8_1_2022.csv") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
tolnay_md <- tolnay_beck %>%
filter(lynch_state=="MD") %>%
select(name, year, month, day, status, lynch_county, place, method_of_death, accusation, mob_size, notes, decade) %>%
arrange(year)
#write.csv(tolnay_md, "../output/tolnay_md.csv")
tolnay_by_decade <- tolnay_md %>%
group_by(decade) %>%
count() %>%
rename(tolnay_count = n)
md_memorial_project <- read_sheet("https://docs.google.com/spreadsheets/d/1MUBAvoaaU0tzdgDUgTRcBu7zGWwZL9_RnbtVZ-zO2lc/edit#gid=0") %>%
mutate(name = paste(first_name, last_name),
year = year(date)) %>%
distinct()
googlesheets4::gs4_deauth()
md_memorial_project <- read_sheet("https://docs.google.com/spreadsheets/d/1MUBAvoaaU0tzdgDUgTRcBu7zGWwZL9_RnbtVZ-zO2lc/edit#gid=0") %>%
mutate(name = paste(first_name, last_name),
year = year(date)) %>%
distinct()
subset_tolnay <- tolnay_md %>%
select(name, year)
subset_memorial <- md_memorial_project %>%
select(name, year)
unknown_cases_tolnay <- anti_join(subset_tolnay, subset_memorial)
subset_geocoded <- distinct_md %>%
select(city_lynch, date)
# Unused code
# x <- maryland %>%
#   group_by(city_lynch, year) %>%
#   filter(n()>1)
#
# md_single_cases <- maryland %>%
#     group_by(city_lynch, year) %>%
#     distinct(lynch_address, .keep_all = TRUE)
#
# write.csv(md_single_cases, "../output/md_distinct_cases.csv")
#
# zz <- md_single_cases %>%
#   select(city_lynch, date, newspaper_name)
#Filtering to remove duplicates
distinct_md <- maryland %>%
distinct(city_lynch, month, newspaper_name, .keep_all = TRUE)
md_cases_by_decade <- distinct_md %>%
group_by(decade) %>%
count() %>%
rename(geocoded_count = n)
newspapers <- distinct_md %>%
group_by(newspaper_name, newspaper_state_code) %>%
count()
in_vs_out_of_state <- distinct_md %>%
group_by(in_state) %>%
count()
subset_geocoded <- distinct_md %>%
select(city_lynch, date)
second_subset_memorial <- md_memorial_project %>%
select(city, date)
unknown_cases_geocoded <- anti_join(subset_geocoded, second_subset_memorial)
View(subset_geocoded)
View(second_subset_memorial)
View(md_memorial_project)
View(distinct_md)
md_memorial2 <- md_memorial_project %>%
select(name, city, date, year)
names(distinct_md)
news_md <- distinct_md %>%
select(lynch_address, date, year, newspaper_name, newspaper_state_code, in_state, miles)
combo <- md_memorial2 %>%
full_join(news_md, by=c("year"))
View(combo)
combo <- md_memorial2 %>%
full_join(news_md, by=c("year")) %>%
arrange(desc(year))
combo <- md_memorial2 %>%
full_join(news_md, by=c("year")) %>%
arrange(year)
write.csv(combo, "../output/md_memorial_news_test.csv")

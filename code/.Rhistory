googlesheets4::gs4_deauth()
md_memorial_project <- read_sheet("https://docs.google.com/spreadsheets/d/1MUBAvoaaU0tzdgDUgTRcBu7zGWwZL9_RnbtVZ-zO2lc/edit#gid=0") %>%
mutate(name = paste(first_name, last_name),
year = year(date)) %>%
distinct()
subset_tolnay <- tolnay_md %>%
select(name, year)
subset_memorial <- md_memorial_project %>%
select(name, year)
unknown_cases_tolnay <- anti_join(subset_tolnay, subset_memorial)
subset_geocoded <- distinct_md %>%
select(city_lynch, date)
# Unused code
# x <- maryland %>%
#   group_by(city_lynch, year) %>%
#   filter(n()>1)
#
# md_single_cases <- maryland %>%
#     group_by(city_lynch, year) %>%
#     distinct(lynch_address, .keep_all = TRUE)
#
# write.csv(md_single_cases, "../output/md_distinct_cases.csv")
#
# zz <- md_single_cases %>%
#   select(city_lynch, date, newspaper_name)
#Filtering to remove duplicates
distinct_md <- maryland %>%
distinct(city_lynch, month, newspaper_name, .keep_all = TRUE)
md_cases_by_decade <- distinct_md %>%
group_by(decade) %>%
count() %>%
rename(geocoded_count = n)
newspapers <- distinct_md %>%
group_by(newspaper_name, newspaper_state_code) %>%
count()
in_vs_out_of_state <- distinct_md %>%
group_by(in_state) %>%
count()
subset_geocoded <- distinct_md %>%
select(city_lynch, date)
second_subset_memorial <- md_memorial_project %>%
select(city, date)
unknown_cases_geocoded <- anti_join(subset_geocoded, second_subset_memorial)
View(subset_geocoded)
View(second_subset_memorial)
View(md_memorial_project)
View(distinct_md)
md_memorial2 <- md_memorial_project %>%
select(name, city, date, year)
names(distinct_md)
news_md <- distinct_md %>%
select(lynch_address, date, year, newspaper_name, newspaper_state_code, in_state, miles)
combo <- md_memorial2 %>%
full_join(news_md, by=c("year"))
View(combo)
combo <- md_memorial2 %>%
full_join(news_md, by=c("year")) %>%
arrange(desc(year))
combo <- md_memorial2 %>%
full_join(news_md, by=c("year")) %>%
arrange(year)
write.csv(combo, "../output/md_memorial_news_test.csv")
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("tigris")
#install.packages("zoo")
library(tigris)
library(stringr)
library(janitor)
library(zoo)
alabama <- c("Ala")
alaska <- c("Alas", "Alask")
arizona <- c("Ari", "Ariz", "ArizT", "ATA", "Cochise", "Marico", "Pima", "PimaCo", "Pin")
arkansas <- c("A", "Ark", "Arkcurrent", "Faulkne", "Ar")
california <- c("Butte", "Ca", "Cal", "Calif", "El", "ElDora", "Placer", "Cali")
colorado <- c("Ba", "Col", "Colo", "Conejo", "DeltaCo", "Elbert", "Logan", "Montez", "RubyCaGunnisonCountyColo", "Wel", "WeldCou")
connecticut <- c("Con", "Conn", "Litc")
delaware <- c("De", "Del")
florida <- c("F", "Fl", "Fla", "MarionC")
georgia <- c("Ga")
hawaii <- c("Hawai", "HawaiianIsland", "HI", "Maui", "TH")
idaho <- c("Id", "Ida", "Idah", "Idaho", "North")
illinois <- c("Il", "Ill")
indiana <- c("Ia", "IAi", "Ind", "MarshallCountyInd", "Ran", "Wayne")
iowa <- c("Audubo", "CedarC", "How", "Howard", "Io", "Iow", "Maha", "TamaCo", "Winnes", "I")
kansas <- c("AllenCou", "K", "Ka", "Kan", "Kansa", "Sum")
kentucky <- c("Bourbon", "Ky", "Madis", "Wo")
louisiana <- c("Attak", "Bossi", "Calcasie", "GrantP", "L", "La", "Lna", "P", "Par", "Pari")
maine <- c("Me")
maryland <- c("Md")
massachusetts <- c()
michigan <- c("Mic", "Mich", "StClair", "LSM")
minnesota <- c("Be", "Beltra", "BlueE", "Brown", "Goodh", "Min", "Minn", "MT", "PineCou", "Stevens", "StL", "St")
mississippi <- c("Cop", "DeSot", "Lafayet", "Lefl", "MT", "Marshal", "Mis", "Miss", "Stone")
missouri <- c("Ada", "Audrain", "Ch", "IronC", "Lafa", "Missour", "Mo", "RayC", "Salin", "ScottC")
montana <- c("MTM", "Mont", "Montcurrent", "Mon")
nebraska <- c("BoxB", "Cher", "H","Neb", "Nebr", "Nebra", "Nebras", "Nebrask", "Nem", "Webs")
nevada <- c("Nev", "NT")
new_hampshire <- c()
new_jersey <- c("NJ")
new_mexico <- c("Gu", "MoraCo", "NM", "NMT", "Rooseve", "Socorr", "Torra")
new_york <- c("NY")
north_carolina <- c("E", "Edge", "Meck", "NC", "Watauga")
north_dakota <- c("Billings", "Bott", "D", "DickeyC", "DT", "Gr", "McLea", "ND", "Richl", "Stark", "Star", "Stut", "Ward", "WardCou", "Will")
ohio <- c("Ashlan", "Bro", "Hancoc", "High", "Ho", "HolmesCoOOhio", "Mahon", "Meigs", "O", "Oh", "Ohi", "OO", "OOh", "OOhi", "OOhio", "Sandus", "StarkC", "Vinto", "Woo")
oklahoma <- c("Choctaw", "CraigC", "Indi", "India", "Indian", "IndianT", "IndTe", "Okla", "Oklah", "OTO")
oregon <- c("Lin", "Lincoln", "LinnCo", "Morrow", "Or", "Orego", "Wal")
pennsylvania <- c("Pa")
rhode_island <- c()
south_carolina <- c("Claren", "SC")
south_dakota <- c("Bl", "Broo", "Brule", "Dako", "S", "DayCo", "DTS", "Haakon", "HandCo", "Pen", "Rober", "SD", "South", "Stanley", "Unio")
tennessee <- c("GibsonC", "Hardem", "McNairy", "MorganC", "T", "Te", "Ten", "Tenn")
texas <- c("Tex")
utah <- c("CityUt", "U", "Uta")
vermont <- c("St. Johnsb", "Orlean", "Vt")
virginia <- c("Augus", "Highl", "V", "Va")
washington <- c("Cheha", "OT", "Wa", "Wash", "Washin", "WT", "Was")
w_dc <- c("DC")
west_virginia <- c("VaW", "WV", "WVa", "W")
wisconsin <- c("Ashland", "Jeff", "Pi", "Rusk", "Vi", "Wi", "Wis")
wyoming <- c("Carbo", "Wyo")
main_index <- read_csv("../data/mainindex.csv")
x <- main_index %>%
count(newspaper_state) %>%
arrange(desc(n))
clean_main_index <- main_index %>%
# Clean out any numbers
mutate(newspaper_state_clean = str_squish(gsub("[0-9]", "", newspaper_state))) %>%
# Clean out any non-letter characters
mutate(newspaper_state_clean = str_squish(gsub("\\W", "", newspaper_state_clean))) %>%
mutate(newspaper_state_clean = case_when(
newspaper_state_clean %in% alabama ~ "AL",
newspaper_state_clean %in% alaska ~ "AK",
newspaper_state_clean %in% arizona ~ "AZ",
newspaper_state_clean %in% arkansas ~ "AR",
newspaper_state_clean %in% california | str_detect(newspaper_city, "Grass Valley") ~ "CA",
newspaper_state_clean %in% colorado | str_detect(newspaper_city, "Cañon City") ~ "CO",
newspaper_state_clean %in% connecticut | str_detect(newspaper_city, "New Britain") ~ "CT",
newspaper_state_clean %in% delaware ~ "DE",
newspaper_state_clean %in% w_dc ~ "DC",
newspaper_state_clean %in% florida ~ "FL",
newspaper_state_clean %in% georgia ~ "GA",
newspaper_state_clean %in% hawaii ~ "HI",
newspaper_state_clean %in% idaho | (newspaper_state_clean == "I" & str_detect(newspaper_city, "Silver City")) ~ "ID",
newspaper_state_clean %in% illinois ~ "IL",
newspaper_state_clean %in% indiana | str_detect(newspaper_city, "Bloomington|Indianapolis") ~ "IN",
newspaper_state_clean %in% iowa | str_detect(newspaper_city, "Mashalltown|Independence")~ "IA",
newspaper_state_clean %in% kansas ~ "KS",
newspaper_state_clean %in% kentucky ~ "KY",
newspaper_state_clean %in% louisiana ~ "LA",
newspaper_state_clean %in% maine ~ "ME",
newspaper_state_clean %in% maryland | str_detect(newspaper_city, "Port Tobacco|Leonard Town") ~ "MD",
newspaper_state_clean %in% michigan | (newspaper_state_clean == "M" & str_detect(newspaper_city, "Grand Rapids")) | str_detect(newspaper_city, "East Saginaw|Constantine|Grand Haven") ~ "MI",
newspaper_state_clean %in% montana | str_detect(newspaper_city, "Stevensville|Diamond City|Philipsburg|Great Falls|Fort Benton") ~ "MT",
newspaper_state_clean %in% missouri | str_detect(newspaper_city, "Farmington") ~ "MO",
newspaper_state_clean %in% minnesota | str_detect(newspaper_city, "Grand Marais|Little Falls|Fergus Falls|Sauk Rapids|Minneapolis|White Earth|Worthington") ~ "MN",
newspaper_state_clean %in% mississippi | (newspaper_state_clean == "M" & str_detect(newspaper_city, "Philadelphia|Water Valley")) | (newspaper_state_clean == "Mi" & str_detect(newspaper_city, "Hattiesburg|Leakesville|Poplarville|Port Gibson")) ~ "MS",
newspaper_state_clean %in% nebraska | str_detect(newspaper_city, "North Platte|Grand Island|Nemaha City|Dakota City") ~ "NE",
newspaper_state_clean %in% nevada | (newspaper_state_clean == "Ne" & str_detect(newspaper_city, "Silver City")) | str_detect(newspaper_city, "Carson City|Gardnerville") ~ "NV",
newspaper_state_clean %in% new_jersey | str_detect(newspaper_city, "Mount Holly|Perth Amboy|Penn's Grove") ~ "NJ",
newspaper_state_clean %in% new_mexico | str_detect(newspaper_city, "Silver City|Albuquerque") ~ "NM",
newspaper_state_clean %in% new_york ~ "NY",
newspaper_state_clean %in% north_carolina | (newspaper_state_clean == "N" & str_detect(newspaper_city, "Hillsborough")) | str_detect(newspaper_city, "Jacksonville|Chapel Hill") ~ "NC",
newspaper_state_clean %in% north_dakota | str_detect(newspaper_city, "Bismarck|Pembina|Valley City|Grand Forks") ~ "ND",
newspaper_state_clean %in% oregon | str_detect(newspaper_city, "Pendleton") ~ "OR",
newspaper_state_clean %in% ohio ~ "OH",
newspaper_state_clean %in% oklahoma ~ "OK",
newspaper_state_clean %in% pennsylvania ~ "PA",
newspaper_state_clean %in% south_carolina ~ "SC",
newspaper_state_clean %in% wisconsin ~ "WI",
newspaper_state_clean %in% south_dakota | str_detect(newspaper_city, "Gann Valley|Mitchell|Hurley|Miller") ~ "SD",
newspaper_state_clean %in% texas | str_detect(newspaper_city, "Brownsville|San Antonio") ~ "TX",
newspaper_state_clean %in% tennessee ~ "TN",
newspaper_state_clean %in% utah ~ "UT",
newspaper_state_clean %in% vermont ~ "VT",
newspaper_state_clean %in% washington | str_detect(newspaper_city, "North Yakima|White Bluffs") ~ "WA",
newspaper_state_clean %in% west_virginia | str_detect(newspaper_city, "Charles|Clarksburg|Lewisburg|Wheeling|Morgantown") ~ "WV",
newspaper_state_clean %in% virginia ~ "VA",
newspaper_state_clean %in% wisconsin ~ "WI",
newspaper_state_clean %in% wyoming ~ "WY",
TRUE ~ newspaper_state_clean
))
y <- clean_main_index %>%
count(newspaper_state_clean) %>%
arrange(desc(n))
#347 categories in the newspaper_states, needs to be cleaned to 50 or so categories in standard two-digit state format, ie. GA for Georgia, CA for California
write.csv(clean_main_index,"../data/mainindex_10_25.csv")
new_main_index <- read_csv("../data/mainindex_10_25.csv")
clean_main_index <- new_main_index %>%
mutate(newspaper_state_clean = case_when(
str_detect(newspaper_city, "San Francisco|Los Angeles|Sacramento") ~ "CA",
str_detect(newspaper_city, "Salt Lake City|Great Salt Lake") ~ "UT",
str_detect(newspaper_city, "Washington") ~ "DC",
str_detect(newspaper_city, "York") ~ "NY",
str_detect(newspaper_city, "Richmond|Lynchburg") ~ "VA",
str_detect(newspaper_city, "Grand Rapids") ~ "MI",
str_detect(newspaper_city, "Mineral Point") ~ "WI",
str_detect(newspaper_city, "Honolulu") ~ "HI",
str_detect(newspaper_city, "Omaha") ~ "NE",
str_detect(newspaper_city, "Shepherdstown") ~ "WV",
str_detect(newspaper_city, "Ste. Genevieve") ~ "MO",
str_detect(newspaper_city, "Maysville") ~ "KY",
str_detect(newspaper_city, "Savannah") ~ "GA",
str_detect(newspaper_city, "Winston|Hendersonville") ~ "NC",
str_detect(newspaper_city, "West Randolph|St. Johnsbury") ~ "VT",
str_detect(newspaper_city, "Redwood Falls") ~ "MN",
str_detect(newspaper_city, "St. Clairsville|Hillsborough") ~ "OH",
str_detect(newspaper_city, "Waterbury|New Haven") ~ "CT",
str_detect(newspaper_city, "Medicine Lodge|Baxter Springs") ~ "KS",
str_detect(newspaper_city, "Donaldsonville|New Orleans") ~ "LA",
str_detect(newspaper_name, "Le Meschacébé|The weekly messenger") ~ "LA",
TRUE ~ newspaper_state_clean
))
filtered_main_index <- clean_main_index %>%
filter(is.na(newspaper_state_clean)) %>%
filter(!is.na(newspaper_city))
cities_list <- filtered_main_index %>%
group_by(newspaper_city) %>%
count()
no_city <- clean_main_index %>%
filter(is.na(newspaper_state_clean)) %>%
filter(is.na(newspaper_city))
View(filtered_main_index)
View(cities_list)
View(no_city)
View(new_main_index)
write.csv(new_main_index,"../data/mainindex_10_30.csv")
ga_papers <- new_main_index %>%
filter(newspaper_state_clean =="GA") %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
View(ga_papers)
ga_papers1 <- main_index %>%
filter(newspaper_state_clean =="GA") %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
View(main_index)
View(clean_main_index)
ga_papers1 <- clean_main_index %>%
filter(newspaper_state_clean =="GA") %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
View(ga_papers1)
sum(ga_papers$n)
sum(ga_papers1$n)
ga_papers_years <- new_main_index %>%
filter(newspaper_state_clean =="GA") %>%
group_by(newspaper_state_clean, newspaper_name, year) %>%
count()
ms_papers <- new_main_index %>%
filter(newspaper_state_clean =="MS") %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
#write.csv(ms_papers,"../output/ms_papers.csv")
ms_papers_years <- new_main_index %>%
filter(newspaper_state_clean =="MS") %>%
group_by(newspaper_state_clean, newspaper_name, year) %>%
count()
ms_papers1 <- clean_main_index %>%
filter(newspaper_state_clean =="MS") %>%
group_by(newspaper_state_clean, newspaper_name) %>%
count()
View(ms_papers)
View(ms_papers1)
sum(ms_papers$n)
sum(ms_papers1$n)
View(ga_papers_years)
1085-997
View(no_city)
no_city %>%
count(newspaper_name)
no_city %>%
count(newspaper_state)
View(no_city)
#install.packages("here")
#here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#install.packages("geosphere")
library(geosphere)
library(janitor)
#install.packages('scales')
library(scales)
tolnay_beck <- read.csv("../data/bailey_beck_lynching_list_8_1_2022.csv") %>%
as.data.frame()
tolnay_beck <- janitor::clean_names(tolnay_beck)
al_ga_ms_tolnay_back <- tolnay_beck %>%
filter(lynch_state == "GA" | lynch_state == "MS" | lynch_state == "AL")
tolnay_georgia <- al_ga_ms_tolnay_back %>%
filter(lynch_state == "GA" & year < 1929 & year >= 1880 & status == "Lynching")
lynch_updated <- read_csv("../data/lynch_geocoded_10.8.csv")
# make less ugly
lynch_updated <- lynch_updated %>%
clean_names()
lynch_updated <- lynch_updated %>%
mutate(date = as.Date(date, "%m/%d/%Y"))
new_georgia_coverage <- lynch_updated %>%
filter(newspaper_state_code == "GA") %>%
select(decade, date, newspaper_name, newspaper_state_code, state_lynch, in_state, border, total_words, file_id, url)
new_georgia_coverage <- new_georgia_coverage %>%
dplyr::mutate(year = lubridate::year(date),
month = lubridate::month(date),
day = lubridate::day(date))
new_georgia_coverage %>%
group_by(newspaper_name) %>%
summarise(count = n())
new_georgia_coverage %>%
filter(in_state == "Y") %>%
group_by(year) %>%
summarise(count = n()) %>%
arrange(year)
tolnay_georgia %>%
group_by(year) %>%
summarise(count = n()) %>%
arrange(desc(count))
View(tolnay_beck)
View(lynch_updated)
names(tolnay_beck)
ga_tolnay <- tolnay_beck %>%
filter(lynch_state =="GA")
View(ga_tolnay)
ga_tolnay <- tolnay_beck %>%
filter(lynch_state =="GA") %>%
select(status, year, month, day, name, alt_name_1, lynch_county, lynch_state, method_of_death, accusation)
ga_news <- lynch_updated %>%
filter(newspaper_state_code == "GA") %>%
mutate(date = as.Date(date, "%m/%d/%Y")) %>%
mutate(year = lubridate::year(date),
month = lubridate::month(date),
day = lubridate::day(date)) %>%
select(decade, date, newspaper_name, newspaper_state_code, year, month, day, state_lynch, in_state, border, total_words, file_id, url)
View(ga_news)
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("year", "month"))
View(ga_tolnay_news)
names(lynch_updated)
ga_news <- lynch_updated %>%
filter(newspaper_state_code == "GA") %>%
mutate(date = as.Date(date, "%m/%d/%Y")) %>%
mutate(year = lubridate::year(date),
month = lubridate::month(date),
day = lubridate::day(date)) %>%
select(decade, date, newspaper_name, newspaper_state_code, year, month, day, city_lynch, state_lynch, in_state, border, total_words, file_id, url)
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("year", "month")) %>%
rename(city_newspaper_report = city_lynch, tolnay_day = day.x, news_day = day.y)
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("year", "month")) %>%
rename(city_newspaper_report = city_lynch, tolnay_day = day.x, news_day = day.y) %>%
select(year, month, tolnay_day, name, lynch_county, newspaper_name, date, news_day, city_newspaper_report, state_lynch, total_words, file_id, url, alt_name1, method_of_death, accusation, decade)
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("year", "month")) %>%
rename(city_newspaper_report = city_lynch, tolnay_day = day.x, news_day = day.y) %>%
select(year, month, tolnay_day, name, lynch_county, newspaper_name, date, news_day, city_newspaper_report, state_lynch, total_words, file_id, url, alt_name_1, method_of_death, accusation, decade)
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("year", "month")) %>%
rename(city_newspaper_report = city_lynch, tolnay_day = day.x, news_day = day.y) %>%
select(year, month, tolnay_day, name, lynch_county, newspaper_name, date, news_day, city_newspaper_report, state_lynch, total_words, file_id, url, alt_name_1, method_of_death, accusation, decade) %>%
filter(state_lynch =="GA")
ga_tolnay_news1 <- ga_tolnay_news[ga_tolnay_news$tolnay_day <= ga_tolnay_news$news_day, ]
View(ga_tolnay_news1)
ga_tolnay_news1 <- ga_tolnay_news[ga_tolnay_news$news_day <= ga_tolnay_news$tolnay_day, ]
View(ga_tolnay_news1)
ga_tolnay_news <- ga_tolnay_news[ga_tolnay_news$news_day <= ga_tolnay_news$tolnay_day, ]
ga_tolnay_news
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("year", "month")) %>%
rename(city_newspaper_report = city_lynch, tolnay_day = day.x, news_day = day.y) %>%
select(year, month, tolnay_day, name, lynch_county, newspaper_name, date, news_day, city_newspaper_report, state_lynch, total_words, file_id, url, alt_name_1, method_of_death, accusation, decade) %>%
filter(state_lynch =="GA")
ga_tolnay_news <- ga_tolnay_news[ga_tolnay_news$news_day <= ga_tolnay_news$tolnay_day, ]
ga_tolnay <- tolnay_beck %>%
filter(lynch_state =="GA") %>%
mutate(date = paste(month,day,year, sep = "-"))
View(ga_tolnay)
ga_tolnay <- tolnay_beck %>%
filter(lynch_state =="GA") %>%
mutate(date = paste(month,day,year, sep = "-")) %>%
mutate(date = as.Date(date, "%m/%d/%Y")) %>%
select(status, year, month, day, name, alt_name_1, lynch_county, lynch_state, method_of_death, accusation)
glimpse(ga_tolnay)
ga_tolnay <- tolnay_beck %>%
filter(lynch_state =="GA") %>%
mutate(date = paste(month,day,year, sep = "-")) %>%
mutate(tolnay_date = as.Date(date, "%m/%d/%Y")) %>%
select(status, tolnay_date, year, month, day, name, alt_name_1, lynch_county, lynch_state, method_of_death, accusation)
glimpse(ga_tolnay)
ga_tolnay <- tolnay_beck %>%
filter(lynch_state =="GA") %>%
mutate(date = paste(month,day,year, sep = "-")) %>%
mutate(tolnay_date = as.Date(date)) %>%
select(status, tolnay_date, year, month, day, name, alt_name_1, lynch_county, lynch_state, method_of_death, accusation)
ga_tolnay <- tolnay_beck %>%
filter(lynch_state =="GA") %>%
mutate(date = paste(month,day,year, sep = "/")) %>%
mutate(tolnay_date = as.Date(date, "%m/%d/%Y")) %>%
select(status, tolnay_date, year, month, day, name, alt_name_1, lynch_county, lynch_state, method_of_death, accusation)
glimpse(ga_tolnay)
View(ga_tolnay)
ga_news <- transform(ga_news, date_seq = sapply(date, function(x) seq(x, by = "day", length.out = 16)))
View(ga_news)
ga_news <- transform(ga_news, date_seq = sapply(date, function(x) seq(x, by = "day", length.out = 16)))
# Create a sequence of dates for 15 days after date1
library(tidyr)
ga_news <- transform(ga_news, date_seq = sapply(date, function(x) seq(x, by = "day", length.out = 16)))
ga_news <- lynch_updated %>%
filter(newspaper_state_code == "GA") %>%
mutate(date = as.Date(date, "%m/%d/%Y")) %>%
mutate(year = lubridate::year(date),
month = lubridate::month(date),
day = lubridate::day(date)) %>%
select(date, newspaper_name, newspaper_state_code, year, month, day, city_lynch, state_lynch, in_state, border, total_words, file_id, url)
ga_news <- transform(ga_news, date_seq = sapply(date, function(x) seq(x, by = "day", length.out = 16)))
glimpse(ga_news)
ga_news$date_seq <- lapply(ga_news$date, function(x) seq(as.Date(x), by = "day", length.out = 16))
View(ga_news)
ga_news <- unnest(ga_news, cols = c(date_seq))
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("date", "date_seq"))
names(ga_tolnay)
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("tolnay_date", "date_seq"))
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("tolnay_date"="date_seq"))
View(ga_tolnay_news)
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("tolnay_date"="date_seq")) %>%
rename(city_newspaper_report = city_lynch, tolnay_year = year.x, tolnay_month = month.x, tolnay_day = day.x, news_year = year.y, news_month = month.y, news_day = day.y, news_date = date) %>%
select(year, month, tolnay_date, news_date, name, lynch_county, newspaper_name, date, news_day, city_newspaper_report, state_lynch, total_words, file_id, url, alt_name_1, method_of_death, accusation, decade) %>%
filter(state_lynch =="GA")
ga_news <- lynch_updated %>%
filter(newspaper_state_code == "GA") %>%
mutate(date = as.Date(date, "%m/%d/%Y")) %>%
mutate(year = lubridate::year(date),
month = lubridate::month(date),
day = lubridate::day(date)) %>%
select(date, newspaper_name, newspaper_state_code, year, month, day, decade, city_lynch, state_lynch, in_state, border, total_words, file_id, url)
# Create a sequence of dates for 15 days after date1
ga_news$date_seq <- lapply(ga_news$date, function(x) seq(as.Date(x), by = "day", length.out = 16))
ga_news <- unnest(ga_news, cols = c(date_seq))
ga_tolnay_news <- ga_tolnay %>%
inner_join(ga_news, by=c("tolnay_date"="date_seq")) %>%
rename(city_newspaper_report = city_lynch, tolnay_year = year.x, tolnay_month = month.x, tolnay_day = day.x, news_year = year.y, news_month = month.y, news_day = day.y, news_date = date) %>%
select(tolnay_year, tolnay_date, news_date, name, lynch_county, newspaper_name, news_date, news_day, city_newspaper_report, state_lynch, total_words, file_id, url, alt_name_1, method_of_death, accusation, decade) %>%
filter(state_lynch =="GA")
#install.packages("here")
#here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#install.packages("geosphere")
library(geosphere)
library(janitor)
#install.packages('scales')
library(scales)
black_bigrams <- read_csv("../output/bp_bigrams/BP_all_bigrams_11.15.csv")
black_bigrams_pre_1900 <- read_csv("../output/bp_bigrams/bp_pre1900_lynch_bigram_count.csv")
black_bigrams %>%
filter(n >= 5) %>%
filter(decade == "1930s")
filter(str_detect(word1, 'mob') & str_detect(word2, 'violence'))
View(black_bigrams)
black_bigrams %>%
filter(n >= 5) %>%
filter(decade == "1930s") %>%
filter(str_detect(word1, 'mob') & str_detect(word2, 'violence'))
black_bigrams %>%
filter(n >= 5) %>%
filter(str_detect(word1, 'anti') & str_detect(word2, 'lynching'))
black_bigrams <- black_bigrams %>%
na.omit()
bigrams_for_viz <- black_bigrams %>%
mutate(black_bigrams, x = paste(word1, word2))
View(bigrams_for_viz)
white_bigrams <- read_csv("../output/all_bigrams_11.10.csv")
white_bigrams %>%
filter(str_detect(word1, 'jim') & str_detect(word2, 'crow'))
white_bigrams %>%
filter(str_detect(word1, 'civil') & str_detect(word2, 'rights'))
white_bigrams %>%
filter(decade == "1930") %>%
filter(str_detect(word1, 'anti') & str_detect(word2, 'lynching'))
white_bigrams_for_viz <- white_bigrams %>%
mutate(white_bigrams, x = paste(word1, word2))
white_bigrams %>%
filter(n >= 5) %>%
filter(decade == "1930") %>%
filter(str_detect(word1, 'mob') & str_detect(word2, 'violence'))
black_bigrams %>%
filter(decade == '1910s')
black_bigrams %>%
filter(decade == '1920s')
black_bigrams %>%
filter(decade == '1930s')
black_bigrams %>%
filter(decade == '1940s')
white_bigrams %>%
filter(str_detect(word1, 'civil') & str_detect(word2, 'rights'))
View(white_bigrams)
white_bigrams %>%
filter(str_detect(word1, 'civil') & str_detect(word2, 'rights'))

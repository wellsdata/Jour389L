beck_region_coded <- beck_lynchings %>%
select(status, lynch_state) %>%
mutate(lynch_region = case_when(
str_detect(lynch_state, "SC|TX|LA|TN|MS|AR|AL|GA|VA|FL|NC") ~ "south",
str_detect(lynch_state, "MD|DE|WV|KY|MO") ~ "border",
str_detect(lynch_state, "ME|NY|NH|VT|MA|CT|RI|PA|NJ|OH|IN|KS|MI|WI|MN|IA|CA|NV|OR|IL") ~ "north",
str_detect(lynch_state, "NE|CO|ND|SD|MT|WA|ID|WY|UT|OK|NM|AZ|AK|HI") ~ "misc",
TRUE~lynch_state
))
View(beck_region_coded)
lynch_cleaned <- lynch_data %>%
clean_names() %>%
mutate(newspaper_region2 = tolower(newspaper_region2))
lynch_by_region <- lynch_cleaned %>%
group_by(newspaper_region2) %>%
count(.) %>%
rename(input_count = n) %>%
mutate(percent_lynchings_input = round(input_count/nrow(lynch_data)*100,2))
View(lynch_by_region)
View(lynch_cleaned)
beck_by_region <- beck_region_coded %>%
group_by(lynch_region) %>%
count(.) %>%
rename(beck_count = n) %>%
mutate(percent_lynchings_beck = round(beck_count/nrow(beck_region_coded)*100,2))
View(beck_by_region)
regional_comparison <- lynch_by_region %>%
left_join(beck_by_region, by = c("newspaper_region2" = "lynch_region")) %>%
mutate(beck_input_dif = percent_lynchings_beck-percent_lynchings_input)
View(regional_comparison)
View(beck_lynchings)
View(beck_by_state)
View(beck_lynchings)
newspaper_list <- lynch_article_data %>%
distinct(newspaper_name, news_address, .keep_all = TRUE)
paper_count_by_state <- newspaper_list %>%
group_by(Newspaper_State) %>%
count(.) %>%
rename(count_papers = n) %>%
mutate(percent_papers = round(count_papers/nrow(newspaper_list)*100,2))
View(paper_count_by_state)
paper_count_by_region <- newspaper_list %>%
group_by(Newspaper_Region2) %>%
count(.) %>%
rename(count_papers = n) %>%
mutate(percent_papers = round(count_papers/nrow(newspaper_list)*100,2))
View(paper_count_by_region)
View(state_lynchings_compare)
View(beck_lynchings)
count(beck_lynchings$status)
summarize(beck_lynchings$status)
beck_lynchings %>%
group_by(status) %>%
count(.)
View(lynch_geocoded_9.27)
write.csv(lynch_geocoded_9.27, "../data/lynch_geocoded_9.27.csv")
library(scales)
install.packages('scales')
install.packages("scales")
library(scales)
#install.packages("here")
#here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#install.packages("geosphere")
library(geosphere)
library(janitor)
#install.packages('scales')
library(scales)
lynching_data_distinct <- lynching_data %>%
distinct()
lynching_data <- read.csv("data/lynch_geocoded_9.16.csv")
lynching_data_distinct <- lynching_data %>%
distinct()
View(lynching_data_distinct)
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
View(starter_code)
geography_starter_df <- lynching_data %>%
clean_names() %>%
select(newspaper_name1, newspaper_state_code, state_lynch, newspaper_region, in_state, year, decade, lynching_lon, lynching_lat, news_location_lon, news_location_lat)
geography_starter_df %>%
group_by(year) %>%
summarise(total_count = n()) %>%
arrange(desc(total_count))
geography_starter_df %>%
filter(newspaper_region == "South") %>%
group_by(year) %>%
summarise(southern_count = n()) %>%
arrange(desc(southern_count))
south_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "South") %>%
group_by(decade) %>%
summarise(southern_count = n()) %>%
arrange(desc(southern_count))
View(south_count_over_time)
north_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "North Central" | newspaper_region == "Northeast") %>%
group_by(decade) %>%
summarise(northern_count = n()) %>%
arrange(desc(northern_count))
west_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "West") %>%
group_by(decade) %>%
summarise(western_count = n()) %>%
arrange(desc(western_count))
overall_count_over_time <- geography_starter_df %>%
group_by(decade) %>%
summarise(total_count = n()) %>%
arrange(desc(total_count))
lynching_geog_over_time <- overall_count_over_time %>% left_join(south_count_over_time, by="decade")
lynching_geog_over_time <- lynching_geog_over_time %>% left_join(north_count_over_time, by='decade')
lynching_geog_over_time <- lynching_geog_over_time %>% left_join(west_count_over_time, by='decade')
View(lynching_geog_over_time)
267+569+148
coverage_occurrences_states <- geography_starter_df %>%
group_by(newspaper_state_code) %>%
summarise(coverage_count = n()) %>%
arrange(desc(coverage_count))
View(coverage_occurrences_states)
lyching_occurrences_states <- geography_starter_df %>%
group_by(state_lynch) %>%
summarise(lynching_count = n()) %>%
arrange(desc(lynching_count))
View(lyching_occurrences_states)
coverage_vs_lynch_occurrence <- left_join(coverage_occurrences_states, lyching_occurrences_states, by=c('newspaper_state_code'='state_lynch'))
View(coverage_vs_lynch_occurrence)
lynching_data <- read.csv("data/lynch_geocoded_9.27.csv")
# vibe check
lynching_data %>%
head(10)
#check for duplicate rows
lynching_data_distinct <- lynching_data %>%
distinct()
#all good
## Regional differences
#starter code
starter_code <- lynching_data %>%
select(decade, Newspaper_Region, in_state) %>%
group_by(Newspaper_Region) %>%
count(in_state)
write_csv(starter_code, "in_out_of_state_breakdown.csv")
# starter code takeaways
# - northeast only covered one in-state lynching but reported on 222 out of state ones
# - north central region had overwhelmingly the most out of state lynching coverage, which make sense because these states are close to the south in proximity, and therefore it makes sense to cover things, but 1384 out of state vs. 55 in state is crazy
# let's clean up this dataframe
geography_starter_df <- lynching_data %>%
clean_names() %>%
select(newspaper_name1, newspaper_state_code, state_lynch, newspaper_region, in_state, year, decade, lynching_lon, lynching_lat, news_location_lon, news_location_lat)
geography_starter_df <- lynching_data %>%
clean_names() %>%
select(newspaper_name, newspaper_state_code, state_lynch, newspaper_region, in_state, year, decade, lynching_lon, lynching_lat, news_location_lon, news_location_lat)
geography_starter_df %>%
group_by(year) %>%
summarise(total_count = n()) %>%
arrange(desc(total_count))
geography_starter_df %>%
filter(newspaper_region == "South") %>%
group_by(year) %>%
summarise(southern_count = n()) %>%
arrange(desc(southern_count))
# same thing just zooming out for decades -- let's use decades for this preliminary vibe check
south_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "South") %>%
group_by(decade) %>%
summarise(southern_count = n()) %>%
arrange(desc(southern_count))
north_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "North Central" | newspaper_region == "Northeast") %>%
group_by(decade) %>%
summarise(northern_count = n()) %>%
arrange(desc(northern_count))
west_count_over_time <- geography_starter_df %>%
filter(newspaper_region == "West") %>%
group_by(decade) %>%
summarise(western_count = n()) %>%
arrange(desc(western_count))
overall_count_over_time <- geography_starter_df %>%
group_by(decade) %>%
summarise(total_count = n()) %>%
arrange(desc(total_count))
lynching_geog_over_time <- overall_count_over_time %>% left_join(south_count_over_time, by="decade")
lynching_geog_over_time <- lynching_geog_over_time %>% left_join(north_count_over_time, by='decade')
lynching_geog_over_time <- lynching_geog_over_time %>% left_join(west_count_over_time, by='decade')
write_csv(lynching_geog_over_time, "lynching_regional_decades.csv")
# in what state did the most lynching take place?
# in what state was lynching covered the most?
coverage_occurrences_states <- geography_starter_df %>%
group_by(newspaper_state_code) %>%
summarise(coverage_count = n()) %>%
arrange(desc(coverage_count))
lyching_occurrences_states <- geography_starter_df %>%
group_by(state_lynch) %>%
summarise(lynching_count = n()) %>%
arrange(desc(lynching_count))
# the top 5 states where the most lynchings occurred were georgia, tennessee, mississippi, kentucky, alabama and texas
# the top 5 states where the most lynchings were covered were wisconsin, missouri, kansas, ohio, south dakota and north dakota
coverage_vs_lynch_occurrence <- left_join(coverage_occurrences_states, lyching_occurrences_states, by=c('newspaper_state_code'='state_lynch'))
state_in_out_breakdown <- lynch_updated %>%
select(decade, newspaper_state, in_state) %>%
group_by(newspaper_state) %>%
count(in_state)
lynch_updated <- read_csv("data/lynch_geocoded_9.27.csv")
lynch_updated <- lynch_updated %>%
clean_names()
state_in_out_breakdown <- lynch_updated %>%
select(decade, newspaper_state, in_state) %>%
group_by(newspaper_state) %>%
count(in_state)
View(state_in_out_breakdown)
out_of_state_coverage <- state_in_out_breakdown %>%
filter(in_state == "N") %>%
arrange(desc(n))
View(out_of_state_coverage)
states_df <- lynch_updated %>%
select(newspaper_name, newspaper_state_code, state_lynch, in_state)
oos_coverage <- states_df %>%
filter(in_state == "N") %>%
group_by(state_lynch) %>%
count(in_state)
View(oos_coverage)
newspapers_oos <- states_df %>%
filter(in_state == "N") %>%
group_by(newspaper_name) %>%
summarize(count = n()) %>%
arrange(desc(count))
View(newspapers_oos)
states_with_years <- lynch_updated %>%
select(decade, newspaper_state_code, state_lynch, in_state, newspaper_name)
View(states_with_years)
is <- states_with_years %>%
filter(in_state == "Y") %>%
group_by(decade) %>%
summarise(in_state = n())
oos <- states_with_years %>%
filter(in_state == "N") %>%
group_by(decade) %>%
summarise(out_of_state = n())
coverage_by_decade <- is %>% left_join(oos, by='decade')
View(coverage_by_decade)
22+53+18+46+42+31+22+27
261/3
22+53
22+53+18
31+22+27
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
jackindex_geo <- read_csv("../data/extracted_articles_aug25.csv")
View(jackindex_geo)
View(lynch_geocoded_9.27)
lynch <- read_csv("../data/articles_aug_25.csv")
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year)
# append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")
z <- y %>%
select(filename, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
#Count by decade, filter off 1960 as noise
lynch_word_decade <- z %>%
select(decade, total) %>%
group_by(decade) %>%
summarize(avg_words=mean(total, na.rm=TRUE)) %>%
mutate(avg_words = round(avg_words, 0)) %>%
filter(!decade==1960)
View(lynch_word_decade)
View(x)
View(lynch)
View(z)
z <- y %>%
select(filename, newspaper_name, URL, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
View(y)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, URL)
# append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")
z <- y %>%
select(filename, newspaper_name, URL, words, decade) %>%
group_by(filename, decade) %>%
summarize(total=sum(words))
View(y)
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
library(tidyverse)
library(textdata)
library(tidytext)
library(quanteda)
library(rio)
#import df created from sequence below
# lynch <- read_csv("articles_1pct_dec26.csv")
lynch <- read_csv("../data/articles_aug_25.csv")
#index of 1 pct sample which has been checked by a coder and represents all valid entries
# jackindex <- read_csv("../data/jackindex_march8.csv")
# lynch_geocoded_9.27 <- read.csv("../data/lynch_geocoded_9.27.csv")
#black <- read_csv("../data/black_press_4_19_2023.csv")
x <- stringi::stri_count_words(lynch$sentence, "\\w+") %>%
as.data.frame() %>%
rename(words = ".")
lynch <- cbind(lynch, x)
y <- lynch %>%
select(filename, sentence, words, year, newspaper_name, URL)
# append decade information for aggregation
y$decade <- paste0(substr(y$year, 0, 3), "0")
z <- y %>%
select(filename, newspaper_name, URL, words, decade) %>%
group_by(filename) %>%
summarize(total=sum(words))
View(z)
lynch_geocoded_9.27 <- read.csv("../data/lynch_geocoded_9.27.csv")
names(lynch_geocoded_9.27)
head(lynch_geocoded_9.27)
View(lynch_geocoded_9.27)
names(lynch_geocoded_9.27)
lynch_geocoded_9.27a <- subset(lynch_geocoded_9.27, select =-X.2, -X.1)
lynch_geocoded_9.27a <- subset(lynch_geocoded_9.27, select -c=(X.2:X.1))
lynch_geocoded_9.27 <- subset(lynch_geocoded_9.27, select =-X.2)
lynch_geocoded_9.27 <- subset(lynch_geocoded_9.27, select =-X.1)
names(z)
z$file_id <- strreplaceall(z$filename, pattern = fixed('.txt'), replacement = fixed())
z$file_id <- str_replace_all(z$filename, pattern = fixed('.txt'), replacement = fixed())
z$file_id <- str_replace_all(z$filename, pattern =fixed('.txt'), replacement = fixed())
z$file_id <- gsub('.txt',"", z$filename)
View(z)
View(lynch_geocoded_9.27)
jackindex_geo <- read_csv("../data/extracted_articles_aug25.csv")
View(jackindex_geo)
googlesheets4::gs4_deauth()
articles <- read_sheet("https://docs.google.com/spreadsheets/d/1zDWMbuoTHVtiJDrrJ9mDfBTp-VxNFTmMHiGcGA-UvjE/edit?usp=sharing") %>%
as.data.frame()
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
googlesheets4::gs4_deauth()
articles <- read_sheet("https://docs.google.com/spreadsheets/d/1zDWMbuoTHVtiJDrrJ9mDfBTp-VxNFTmMHiGcGA-UvjE/edit?usp=sharing") %>%
as.data.frame()
View(articles)
jackindex_geo %>%
count(file_id2) %>%
arrange(desc(n))
jackindex_geo %>%
count(file_id2, file_id)
bf <- jackindex_geo %>%
count(file_id2, file_id)
View(bf)
bf <- jackindex_geo %>%
count(file_id2, file_id) %>%
group_by(file_id) %>%
arrange(desc(file_id))
jackindex_geo %>%
count(file_id2, file_id) %>%
count(file_id)
jackindex_geo %>%
count(file_id2, file_id) %>%
count(file_id) %>%
arrange(desc(n))
lynch_geocoded_9.27 <- read.csv("../data/lynch_geocoded_9.27.csv")
3308-2786
522+2694
View(articles)
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("kableExtra")
library(kableExtra)
#webshot::install_phantomjs()
library(lubridate)
lynch_geocoded_10.8 <- read.csv("../data/lynch_geocoded_10.8.csv")
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(month = floor_date(fixed_date, "month")) %>%
select(-date) %>%
rename(paper_state = newspaper_state_code)
View(wire_coverage)
names(lynch_geocoded_10.8)
View(lynch_geocoded_10.8)
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(date)>15)))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))
View(wire_coverage)
wire_grouped <- wire_coverage %>%
group_by(group)
View(wire_grouped)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group)
View(wire_grouped)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=6)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=6) %>%
arrange(desc(n))
#I grouped the stories based on whether they were within 15 days of each other
#This code uses the diff function to calculate the difference between consecutive dates and checks if the difference is greater than 15. It then accumulates the results using cumsum to create a new grouping column called group.
#cumsum: Cumulative Sums, Products, and Extremes
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, URL) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))
View(lynch_geocoded_10.8)
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15))) %>%
group_by(group)
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15))) %>%
group_by(count(group))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))%>%
group_by(group)
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=2) %>%
arrange(desc(n))
View(wire_grouped)
wire_coverage <- wire_coverage %>%
filter(lynch_address != "NA, NA" & lynch_address != "None, NONE")
wire_grouped <- wire_coverage %>%
group_by(group) %>%
count(group) %>%
filter(n>=2) %>%
arrange(desc(n))
same_towns <- wire_coverage %>%
group_by(lynch_address, group)
View(same_towns)
same_towns <- wire_coverage %>%
group_by(lynch_address)
View(same_towns)
wire_coverage2 <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, file_id, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(group = cumsum(c(TRUE, diff(fixed_date)>15)))%>%
group_by(group)
wire_coverage2 <- wire_coverage2 %>%
filter(lynch_address != "NA, NA" & lynch_address != "None, NONE")
wire_grouped <- wire_coverage2 %>%
group_by(group) %>%
count(group) %>%
filter(n>=2) %>%
arrange(desc(n))
wire_coverage <- lynch_geocoded_10.8 %>%
select(newspaper_name, lynch_address, date, miles, newspaper_state_code, url) %>%
mutate(fixed_date=mdy(date)) %>%
mutate(month = floor_date(fixed_date, "month")) %>%
select(-date) %>%
rename(paper_state = newspaper_state_code) %>%
filter(lynch_address != "NA, NA" & lynch_address != "None, NONE")
duplicates <- wire_coverage[duplicated(wire_coverage$lynch_address) | duplicated(wire_coverage$lynch_address, fromLast = TRUE), ]
View(duplicates)
check_duplicates <- duplicates %>%
group_by(lynch_address) %>%
count()
View(check_duplicates)
city_and_date <- wire_coverage[duplicated(wire_coverage[, c("lynch_address", "month")]) | duplicated(wire_coverage[, c("lynch_address", "month")], fromLast = TRUE), ]
View(city_and_date)
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")]) | duplicated(wire_coverage[, c("lynch_address", "group")], fromLast = TRUE), ]
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")]) | duplicated(wire_coverage2[, c("lynch_address", "group")], fromLast = TRUE), ]
View(same_towns)
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")])
)
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")]]
same_towns <- wire_coverage2[duplicated(wire_coverage2[, c("lynch_address", "group")], fromLast = TRUE), ]

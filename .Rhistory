<<<<<<< Updated upstream
group_by(decade) %>%
summarise(count = n()) %>%
arrange(desc(decade))
tolnay_ms %>%
group_by(decade) %>%
summarise(count = n()) %>%
arrange(desc(decade))
tolnay_ms %>%
count()
new_mississippi_coverage %>%
group_by(decade) %>%
summarise(count = n()) %>%
arrange(desc(decade))
tolnay_ms <-  al_ga_ms_tolnay_back %>%
filter(lynch_state == "MS" & year < 1929 & year >= 1880 & status == "Lynching")
tolnay_ms %>%
count()
tolnay_georgia %>%
count()
tolnay_ms %>%
count()
new_mississippi_coverage %>%
count()
tolnay_georgia %>%
filter(in_state == "Y") %>%
count()
new_mississippi_coverage %>%
filter(in_state == "Y") %>%
count()
new_mississippi_coverage %>%
filter(in_state == "N") %>%
group_by(newspaper_name) %>%
summarise(count = n())
new_mississippi_coverage %>%
filter(in_state == "N") %>%
group_by(newspaper_name) %>%
summarise(count = n())
new_mississippi_coverage %>%
filter(in_state == "N") %>%
group_by(newspaper_name) %>%
summarise(count = n())
new_mississippi_coverage %>%
filter(in_state == "Y") %>%
group_by(newspaper_name) %>%
summarise(count = n())
new_mississippi_coverage %>%
filter(newspaper_name == "Oxford eagle.")
tolnay_beck %>%
filter(decade == 1890 | decade == 1910)
tolnay_ms %>%
filter(decade == 1890 | decade == 1910)
new_mississippi_coverage %>%
filter(newspaper_name == "Macon Beacon.")
new_mississippi_coverage %>%
filter(newspaper_name == "Macon beacon.")
new_mississippi_coverage %>%
filter(newspaper_name == "Macon beacon")
tolnay_ms %>%
filter(decade == 1890)
# load libraries
library(dplyr)
library(ggplot2)
=======
load("C:/Program Files/R/R-4.3.1/bin/x64/Rgui.exe")
#| label: load_libraries_settings_functions_data
#| include: false
###
# Load libraries
###
library(here)
source(here("analysis/functions/load_libraries_functions.R"))
install.packages("pacman")
#| label: load_libraries_settings_functions_data
#| include: false
###
# Load libraries
###
library(here)
source(here("analysis/functions/load_libraries_functions.R"))
install_packages("splitstackshape")
install.packages("splitstackshape")
source(here("analysis/functions/load_libraries_functions.R"))
#install.packages("tidyverse")
#install.packages("httr")
>>>>>>> Stashed changes
library(tidyverse)
library(httr)
library(janitor)
options(readr.show_col_types = FALSE)
pdf_urls <- function(registrant) {
registration_number <- all_registrants$registration_number[registrant]
url <- paste0("https://efile.fara.gov/api/v1/RegDocs/csv/", registration_number)
Sys.sleep(5)
documents <- read_csv(url)
print(registrant)
documents
}
pdf_urls <- function(registrant) {
registration_number <- all_registrants$registration_number[registrant]
url <- paste0("https://efile.fara.gov/api/v1/RegDocs/csv/", registration_number)
Sys.sleep(5)
documents <- read_csv(url)
print(registrant)
documents
}
file = 1
get_pdfs <- function(file) {
file_info <- egypt_analysis[file, ]
file_link <- file_info$url
reg_num <- file_info$registration_number
date_stamp <- file_info$date_stamped
doc_type <- gsub(" ", "_", file_info$document_type)
file_name <- paste(date_stamp, reg_num, doc_type, sep = "_")
file_path <- paste0("fara_downloads/", file_name, ".pdf")
download.file(file_link, file_path, mode = "wb")
}
all_pdf_joined <- read_csv("all_pdfs.csv") %>%
mutate(date_stamped = as.Date(date_stamped, "%m/%d/%Y")) %>%
mutate(registration_date = as.Date(registration_date, "%m/%d/%Y"))
pdf_urls <- function(registrant) {
registration_number <- all_registrants$registration_number[registrant]
url <- paste0("https://efile.fara.gov/api/v1/RegDocs/csv/", registration_number)
Sys.sleep(5)
documents <- read_csv(url)
print(registrant)
documents
}
all_pdf_joined <- read_csv("all_pdfs.csv") %>%
mutate(date_stamped = as.Date(date_stamped, "%m/%d/%Y")) %>%
mutate(registration_date = as.Date(registration_date, "%m/%d/%Y"))
#install.packages("tidyverse")
#install.packages("httr")
library(tidyverse)
library(httr)
library(janitor)
options(readr.show_col_types = FALSE)
all_pdf_joined <- read_csv("all_pdfs.csv") %>%
mutate(date_stamped = as.Date(date_stamped, "%m/%d/%Y")) %>%
mutate(registration_date = as.Date(registration_date, "%m/%d/%Y"))
View(all_pdf_joined)
View(all_pdf_joined)
doc_types <- all_pdf_joined %>%
group_by(document_type) %>%
count(.)
View(doc_types)
filtered_docs <- all_pdf_joined %>%
filter(str_detect(document_type, "AB|Supplemental"))
View(filtered_docs)
#install.packages("xml2")
#install.packages("lubridate")
#install.packages("tidyr")
#install.packages("tidyverse")
#install.packages("dplyr")
#install.packages("XML")
#install.packages("purrr")
#install.packages("furrr")
#install.packages("ppcong")
#install.packages("stringi")
#install.packages("pdftools")
#install.packages("tesseract")
library(methods)
library(tidyverse)
library(furrr)
library(ppcong)
#install.packages("xml2")
#install.packages("lubridate")
#install.packages("tidyr")
#install.packages("tidyverse")
#install.packages("dplyr")
#install.packages("XML")
#install.packages("purrr")
#install.packages("furrr")
#install.packages("ppcong")
#install.packages("stringi")
#install.packages("pdftools")
#install.packages("tesseract")
library(methods)
library(tidyverse)
library(furrr)
library(stringi)
library(pdftools)
library(tesseract)
pdf_one <- "~/Senate Docs/0IWgezTOz02j7p3BP3fIrA.pdf"
one_text <- pdf_ocr_text(pdf_one)
view(one_text)
download.file("https://chroniclingamerica.loc.gov/newspapers.txt", "~/data/newspapers_list.txt")
setwd("~/GitHub/Jour389L")
download.file("https://chroniclingamerica.loc.gov/newspapers.txt", "~/data/newspapers_list.txt")
download.file("https://chroniclingamerica.loc.gov/newspapers.txt", "data/newspapers_list.txt")
download.file("https://chroniclingamerica.loc.gov/newspapers.txt", "../data/newspapers_list.txt")
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
#install.packages("tigris")
#install.packages("zoo")
library(tigris)
library(stringr)
library(janitor)
library(zoo)
download_loc <- "../data/newspapers_list.txt"
newspaper_list <- read_delim(download_loc, delim = "|")
View(newspaper_list)
clean_newspaper_list <- newspaper_list %>%
clean_names()
<<<<<<< Updated upstream
library(janitor)
df <- df %>%
clean_names()
df
n_distinct(df$base_amount)
amounts <- as.data.frame(table(df$base_amount))
View(amounts)
amounts %>%
group_by(amounts) %>%
summarise(sum(Freq))
amounts
amounts %>%
group_by(Var1) %>%
summarise(sum(Freq))
(22084/43227)*100
df %>%
group_by(violation) %>%
summarise(count = n())
df %>%
filter(base_amount == 85)
df %>%
filter(base_amount == 85) %>%
group_by(violation) %>%
summarise(count = n())
df
(15138/43227)*100
df %>%
filter(base_amount == 85) %>%
group_by(violation) %>%
summarise(count = n())
(15138/43227)*100
df2 <- read_csv('citationappeals.csv')
View(df2)
View(df2)
n_distinct(df2$cit_ticket_number)
df2 <- read_csv('citationappeals.csv')
n_distinct(df2$cit_ticket_number)
df2 %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
df2 <- read_csv('citationappeals.csv')
View(df2)
View(df2)
df2 <- read_csv('citationappeals.csv')
View(df2)
n_distinct(df)
df
View(df)
n_distinct(df2$cit_ticket_number)
df2
View(df2)
df2
n_distinct(df$base_amount)
n_distinct(df)
n_distinct(df$Location)
9728/43227
9728/43227
n_distinct(df2$cit_ticket_number)
df3 <- n_distinct(df2$cit_ticket_number)
df2 <- read_csv('citationappeals.csv')
n_distinct(df2$cit_ticket_number)
df2 %>%
filter(appl_judgement_display_name == "Cancelled")
df2 %>%
filter(appl_judgement_display_name == "Cancelled") %>%
group_by(appl_judgement_comments) %>%
summarise(count = n())
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning")
df2 <- read_csv('citationappeals.csv')
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning") %>%
group_by(appl_judgement_display_name) %>%
summarise(count = n())
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning") %>%
group_by(cit_ticket_number) %>%
summarise(count = n())
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning") %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
df2 %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning") %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
df2 <- read_csv('citationappeals.csv')
df2 <- read_csv('citationappeals.csv')
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning") %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
6662/43227
df2 <- read_csv('citationappeals.csv')
View(df2)
6662/9728
df2=df2[grepl("Citation Voided|Fine Reduction|Warning", df2$appl_judgement_display_name)]
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning") %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning")
fuck <- df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning")
write_csv(fuck, "fuck.csv")
fuck
9728/43227
df2
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning")
View(fuck)
6664/9728
df2
df2=df2[grepl("2nd", df2$appl_judgement_display_name),]
View(df2)
df2=df2[grepl("Second", df2$appl_judgement_display_name),]
df2=df2[grepl("2nd", df2$appl_judgement_display_name),]
df2=df2[grepl("2nd", df2$appl_judgement_display_name),]
df2=df2[grepl("2nd", df2$appl_judgement_display_name),]
df2
df2=df2[grepl("2nd", df2$appl_judgement_display_name),]
df2 %>%
group_by(appl_judgement_display_name) %>%
summarise(count = n())
# total appeals
df2 <- read_csv('citationappeals.csv')
n_distinct(df2$cit_ticket_number)
# total number of appears is 9728 -- divide by total number of citations to get how many citations got appealed
9728/43227
#NEW FROM BRIDGET what percent of parking tickets are appealed
# 8427/43227
#NEW FROM BRIDGET what percent of parking tickets  resulted in a voided citation, a fine reduction, or the citation was dropped to a warning?
df2 %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
fuck <- df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning")
write_csv(fuck, "fuck.csv")
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning") %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
# 6,215
df2 %>%
filter(appl_judgement_display_name == "Cancelled") %>%
group_by(appl_judgement_comments) %>%
summarise(count = n())
# 449
# 6664
df2=df2[grepl("Citation Voided|Fine Reduction|Warning", df2$appl_judgement_display_name)]
# total appeals
df2 <- read_csv('citationappeals.csv')
n_distinct(df2$cit_ticket_number)
# total number of appears is 9728 -- divide by total number of citations to get how many citations got appealed
9728/43227
#NEW FROM BRIDGET what percent of parking tickets are appealed
# 8427/43227
#NEW FROM BRIDGET what percent of parking tickets  resulted in a voided citation, a fine reduction, or the citation was dropped to a warning?
df2 %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
fuck <- df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning")
write_csv(fuck, "fuck.csv")
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning") %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
# 6,215
df2 %>%
filter(appl_judgement_display_name == "Cancelled") %>%
group_by(appl_judgement_comments) %>%
summarise(count = n())
# 449
# 6664
# total number of successful appeals divided by the total number of appeals generally
6664/9728
# results in 68 percent
# BRIDGET HULLABALLOO
# what percent of appeals are accepted? 73.727%
# 8,427 completed appeals and 6,123 appeals resulted in a voided citation, a fine reduction, or the citation was dropped to a warming
(6213/8427)*100
# what percent of appeals resulted in a fine reduction? 41.189%
(3471/8427)*100
# what percent of appeals resulted in a void? 32.526%
(2741/8427)*100
41.18904 + 32.5264
# what percent of appeals resulted in a warning? 1%<
(7/8427)*100
# what percent of second appeals get accepted? 10.909%
# there are 55 completed second appeals, 3 resulted in a voided citation and three resulted in a fine reduction
6/55
# what percent of appeals get accept by lot? The parking ticket appeal data does not have the lot variable
# how many appeals are there per lot? The parking ticket appeal data does not have the lot variable
df2=df2[grepl("2nd", df2$appl_judgement_display_name),]
df2 %>%
group_by(appl_judgement_display_name) %>%
summarise(count = n())
df2 %>%
group_by(appl_judgement_display_name) %>%
summarise(count = n())
# total appeals
df2 <- read_csv('citationappeals.csv')
n_distinct(df2$cit_ticket_number)
# total number of appears is 9728 -- divide by total number of citations to get how many citations got appealed
9728/43227
#NEW FROM BRIDGET what percent of parking tickets are appealed
# 8427/43227
#NEW FROM BRIDGET what percent of parking tickets  resulted in a voided citation, a fine reduction, or the citation was dropped to a warning?
df2 %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
fuck <- df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning")
write_csv(fuck, "fuck.csv")
df2 %>%
filter(appl_judgement_display_name == "Citation Voided" | appl_judgement_display_name == "Fine Reduction" | appl_judgement_display_name == "Warning") %>%
group_by(cit_ticket_number) %>%
summarise(count = n()) %>%
arrange(desc(count))
# 6,215
df2 %>%
filter(appl_judgement_display_name == "Cancelled") %>%
group_by(appl_judgement_comments) %>%
summarise(count = n())
# 449
# 6664
# total number of successful appeals divided by the total number of appeals generally
6664/9728
# results in 68 percent
# BRIDGET HULLABALLOO
# what percent of appeals are accepted? 73.727%
# 8,427 completed appeals and 6,123 appeals resulted in a voided citation, a fine reduction, or the citation was dropped to a warming
(6213/8427)*100
# what percent of appeals resulted in a fine reduction? 41.189%
(3471/8427)*100
# what percent of appeals resulted in a void? 32.526%
(2741/8427)*100
41.18904 + 32.5264
# what percent of appeals resulted in a warning? 1%<
(7/8427)*100
# what percent of second appeals get accepted? 10.909%
# there are 55 completed second appeals, 3 resulted in a voided citation and three resulted in a fine reduction
6/55
# what percent of appeals get accept by lot? The parking ticket appeal data does not have the lot variable
# how many appeals are there per lot? The parking ticket appeal data does not have the lot variable
df2 %>%
group_by(appl_judgement_display_name) %>%
summarise(count = n())
df3=df2[grepl("2nd", df2$appl_judgement_display_name),]
View(df3)
#install.packages("here")
#here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#install.packages("geosphere")
library(geosphere)
library(janitor)
#install.packages('scales')
library(scales)
states_df
lynch_updated
lynch_updated %>%
filter(state_lynch == "GA" & year == 1903)
new_georgia_coverage
new_georgia_coverage %>%
filter(state_lynch == "GA" & year == 1903)
lynch_updated %>%
filter(state_lynch == "GA" & year == 1903)
new_georgia_coverage %>%
filter(year == 1903)
lynch_updated %>%
filter(state_lynch == "GA" & year == 1903)
new_georgia_coverage %>%
filter(year == 1903)
tolnay_georgia %>%
filter(year == 1889 | year == 1890 | year == 1891)
tolnay_georgia %>%
filter(lynch_county == "Chatham")
tolnay_georgia %>%
filter(year == 1896)
lynch_updated %>%
filter(state_lynch == "GA" & year == 1896)
new_georgia_coverage %>%
filter(newspaper_name == "The morning news")
new_georgia_coverage %>%
filter(newspaper_name == "The morning news" & year == 1896)
new_georgia_coverage
new_georgia_coverage %>%
filter(newspaper_name == "The morning news" | newspaper_name == "Savannah morning news" | newspaper_name == "The Savannah morning news." & year == 1896)
new_georgia_coverage %>%
filter(newspaper_name == "The morning news" | newspaper_name == "Savannah morning news" | newspaper_name == "The Savannah morning news.") %>%
filter(year == 1896)
new_georgia_coverage %>%
filter(year == 1896)
new_georgia_coverage %>%
filter(year == 1903)
tolnay_georgia %>%
filter(year == 1903)
new_mississippi_coverage %>%
filter(newspaper_name == "Warren sheaf")
new_mississippi_coverage <- lynch_updated %>%
filter(newspaper_state_code == "MS") %>%
select(decade, year, newspaper_name, newspaper_state_code, state_lynch, in_state, border, total_words)
new_mississippi_coverage %>%
filter(newspaper_name == "Warren sheaf")
setwd("~/Documents/GitHub/Jour389L")
#install.packages("here")
#here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#install.packages("geosphere")
library(geosphere)
library(janitor)
#install.packages('scales')
library(scales)
bigram_lynch_1890s <- read_csv("output/1890s_lynch_bigram_count.csv")
bigram_lynch_1890s <- read_csv("../output/1890s_lynch_bigram_count.csv")
trigram_lynch_1890s <- read_csv("../output/1890s_lynch_trigram_count.csv")
bigram_lynch_1890s
trigram_lynch_1890s
bigram_lynch_1890s <- bigram_lynch_1890s %>%
filter(n > 5)
bigram_lynch_1890s <- bigram_lynch_1890s %>%
filter(n > 5)
bigram_lynch_1890s
trigram_lynch_1890s <- trigram_lynch_1890s %>%
filter(n > 5)
trigram_lynch_1890s
trigram_lynch_1890s <- trigram_lynch_1890s %>%
drop_na() %>%
trigram_lynch_1890s
trigram_lynch_1890s <- trigram_lynch_1890s %>%
filter(n > 5)
trigram_lynch_1890s
bigram_lynch_1890s %>%
filter(str_detect(word1, 'lynch') | str_detect(word2, 'lynch'))
bigram_lynch_1890s
bigram_lynch_1890s %>%
filter(str_detect(word1, 'lynch') | str_detect(word2, 'lynch'))
bigram_lynch_1890s
bigram_lynch_1890s %>%
filter(str_detect(word1, 'negro') | str_detect(word2, 'negro'))
bigram_lynch_1890s
trigram_lynch_1890s <- trigram_lynch_1890s %>%
filter(n > 5)
trigram_lynch_1890s
trigram_lynch_1890s %>%
filter(str_detect(word1, 'lynch') | str_detect(word2, 'lynch'))
trigram_lynch_1890s
bigram_lynch_1890s
trigram_lynch_1890s
trigram_lynch_1890s %>%
filter(str_detect(word1, 'lynch') | str_detect(word2, 'lynch'))
trigram_lynch_1890s %>%
filter(str_detect(word1, 'lynch') | str_detect(word2, 'lynch') | str_detect(word3, 'lynch'))
trigram_lynch_1890s
bigram_lynch_1890s
trigram_lynch_1890s %>%
filter(str_detect(word1, 'lynch') | str_detect(word2, 'lynch') | str_detect(word3, 'lynch'))
bigram_lynch_1890s
trigram_lynch_1890s
bigram_lynch_1890s
trigram_lynch_1890s %>%
filter(str_detect(word1, 'negro') | str_detect(word2, 'negro') | str_detect(word3, 'negro'))
trigram_lynch_1890s %>%
filter(str_detect(word1, 'mob') | str_detect(word2, 'mob') | str_detect(word3, 'mob'))
=======
View(clean_newspaper_list)
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, no_of_issues, first_issue_date, last_issue_date)
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, no_of_issues, first_issue_date, last_issue_date) %>%
rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date)
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, no_of_issues, first_issue_date, last_issue_date) %>%
rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date) %>%
mutate(state = str_squish(state))
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, no_of_issues, first_issue_date, last_issue_date) %>%
rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date) %>%
mutate(state = str_squish(state)) %>%
mutate(no_issues = str_squish(no_issues))
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, no_of_issues, first_issue_date, last_issue_date) %>%
rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date) %>%
mutate_all(str_squish)
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, no_of_issues, first_issue_date, last_issue_date) %>%
rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date) %>%
mutate_all(str_squish) %>%
mutate(earliest_issue = mdy(earliest_issue))
clean_newspaper_list <- newspaper_list %>%
clean_names() %>%
select(state, title, no_of_issues, first_issue_date, last_issue_date) %>%
rename(newspaper = title, no_issues = no_of_issues, earliest_issue = first_issue_date, latest_issue = last_issue_date) %>%
mutate_all(str_squish) %>%
mutate(earliest_issue = mdy(earliest_issue)) %>%
mutate(latest_issue = mdy(latest_issue))
View(clean_newspaper_list)
write_csv(clean_newspaper_list, "../data/newspaper_list.csv")
>>>>>>> Stashed changes

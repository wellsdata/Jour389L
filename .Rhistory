if (str_detect(cc_details_clean$facility_name[row], "[0-9]+")) {
row_values <- cc_details_clean[row, ]
shifted_values <- c(row_values[1], NA, row_values[-c(1, length(row_values))])
cc_details_clean[row, ] <- shifted_values
}
}
cc_details_clean <- cc_details %>%
mutate(serious_injuries = as.character(serious_injuries)) %>%
mutate(fatalities = as.character(fatalities)) %>%
mutate(level = as.character(level))
row <- 5
for(row in 1:nrow(cc_details_clean)){
if (str_detect(cc_details_clean$facility_name[row], "[0-9]+")) {
row_values <- cc_details_clean[row, ]
shifted_values <- c(row_values[1], NA, row_values[-c(1, length(row_values))])
cc_details_clean[row, ] <- shifted_values
}
}
cc_details_clean <- cc_details %>%
mutate(serious_injuries = as.character(serious_injuries)) %>%
mutate(fatalities = as.character(fatalities)) %>%
mutate(level = as.character(level))
for(row in 1:nrow(cc_details_clean)) {
if (is.na(cc_details_clean$facility_name)) {
print("owo")
}
else if (str_detect(cc_details_clean$facility_name[row], "[0-9]+")) {
row_values <- cc_details_clean[row, ]
shifted_values <- c(row_values[1], NA, row_values[-c(1, length(row_values))])
cc_details_clean[row, ] <- shifted_values
}
}
cc_details_clean <- cc_details %>%
mutate(serious_injuries = as.character(serious_injuries)) %>%
mutate(fatalities = as.character(fatalities)) %>%
mutate(level = as.character(level))
for(row in 1:nrow(cc_details_clean)) {
if (is.na(cc_details_clean$facility_name[row])) {
print("owo")
}
else if (str_detect(cc_details_clean$facility_name[row], "[0-9]+")) {
row_values <- cc_details_clean[row, ]
shifted_values <- c(row_values[1], NA, row_values[-c(1, length(row_values))])
cc_details_clean[row, ] <- shifted_values
}
}
cc_details_clean <- cc_details %>%
mutate(serious_injuries = as.character(serious_injuries)) %>%
mutate(fatalities = as.character(fatalities)) %>%
mutate(level = as.character(level))
for(row in 1:nrow(cc_details_clean)) {
if (is.na(cc_details_clean$facility_name[row])) {
}
else if (str_detect(cc_details_clean$facility_name[row], "[0-9]+")) {
row_values <- cc_details_clean[row, ]
shifted_values <- c(row_values[1], NA, row_values[-c(1, length(row_values))])
cc_details_clean[row, ] <- shifted_values
}
}
View(cc_details_clean)
cc_details_clean <- cc_details %>%
mutate(serious_injuries = as.character(serious_injuries)) %>%
mutate(fatalities = as.character(fatalities)) %>%
mutate(level = as.character(level)) %>%
for(row in 1:nrow(cc_details_clean)) {
if (is.na(cc_details_clean$facility_name[row])) {
}
else if (str_detect(cc_details_clean$facility_name[row], "[0-9]+")) {
row_values <- cc_details_clean[row, ]
shifted_values <- c(row_values[1], NA, row_values[-c(1, length(row_values))])
cc_details_clean[row, ] <- shifted_values
}
}
cc_details_clean <- cc_details %>%
mutate(serious_injuries = as.character(serious_injuries)) %>%
mutate(fatalities = as.character(fatalities)) %>%
mutate(level = as.character(level))
for(row in 1:nrow(cc_details_clean)) {
if (is.na(cc_details_clean$facility_name[row])) {
}
else if (str_detect(cc_details_clean$facility_name[row], "[0-9]+")) {
row_values <- cc_details_clean[row, ]
shifted_values <- c(row_values[1], NA, row_values[-c(1, length(row_values))])
cc_details_clean[row, ] <- shifted_values
}
}
View(cc_details_clean)
load("~/Desktop/GitHub/congressional-private-travel/.RData")
load("~/Desktop/GitHub/congressional-private-travel/.RData")
library(readr)
senate_data_2011_06_05 <- read_csv("Desktop/GitHub/congressional-private-travel/data/senate_data_archives/senate_data_2011-06-05.csv")
View(senate_data_2011_06_05)
# Example URL for the oldest available snapshot
wayback_urls <- c("https://web.archive.org/web/20080131052758/http://soprweb.senate.gov/giftrule/giftruledownload/GiftRuleData.zip", "https://web.archive.org/web/20110605144411/http://soprweb.senate.gov/giftrule/giftruledownload/giftruledata.zip", "https://web.archive.org/web/20150318055851/http://soprweb.senate.gov/giftrule/giftruledownload/giftruledata.zip", "https://web.archive.org/web/20190310121622/http://soprweb.senate.gov/giftrule/giftruledownload/giftruledata.zip", "https://web.archive.org/web/20230302004112/https://giftrule-disclosure.senate.gov/media/giftruledownloads/giftruledata.zip", "https://web.archive.org/web/20130512083149/http://soprweb.senate.gov/giftrule/giftruledownload/GiftRuleData.zip")
wayback_dataframe <- data.frame(urls = unlist(wayback_urls))
wayback_dataframe <- wayback_dataframe |>
mutate(date_time = gsub("[^0-9]", "", urls)) %>%
mutate(parsed_dt = strptime(date_time, format = "%Y%m%d%H%M%S")) %>%
mutate(date = as.Date(parsed_dt))
library(xml2)
library(methods)
library(tidyverse)
library(XML)
library(furrr)
library(rvest)
library(httr)
#JUST for second XML, which has some different factors
xml_two_to_table <- function(file_number) {
xml_listing <- all_listings[[file_number]]
# Get attributes of filer and office name
LastName <- xml_attr(xml_listing, "LastName")
FirstName <- xml_attr(xml_listing, "FirstName")
OfficeName <- xml_attr(xml_find_first(xml_listing, ".//dbo.Office"), "OfficeName")
# Create a list of all filings made my the filer
all_filings <- xml_find_all(xml_listing, ".//dbo.Document")
traveler_filings <- data.frame()
#For each filing, extract information and put it in a dataframe
for(filing_num in 1:length(all_filings)) {
filing <- all_filings[[filing_num]]
ReportingYear <- xml_attr(filing, "ReportingYear")
BeginTravelDate <- xml_attr(filing, "BeginTravelDate")
EndTravelDate <- xml_attr(filing, "EndTravelDate")
DateReceived <- xml_attr(filing, "DateReceived")
TransactionDate <- xml_attr(filing, "TransactionDate")
Pages <- xml_attr(filing, "Pages")
ReportTitle <- xml_attr(xml_find_first(filing, ".//dbo.Reports"), "ReportTitle")
DocURL <- xml_attr(xml_find_first(filing, ".//dbo.Reports"), "DocURL")
report_data <- c(LastName, FirstName, OfficeName, ReportTitle, ReportingYear, BeginTravelDate, EndTravelDate, DateReceived, TransactionDate, Pages, DocURL)
traveler_filings <- rbind(traveler_filings, report_data)
}
traveler_filings <- setNames(traveler_filings, c("filer_lastname", "filer_firstname", "filer_office", "report_title", "reporting_year", "begin_travel_date", "end_travel_date", "date_received", "transaction_date", "num_pages", "doc_url")) %>%
mutate(begin_travel_date = as.Date(begin_travel_date)) %>%
mutate(end_travel_date = as.Date(end_travel_date)) %>%
mutate(date_received = as.Date(date_received)) %>%
mutate(transaction_date = as.Date(transaction_date)) %>%
mutate(num_pages = as.double(num_pages)) %>%
mutate(reporting_year = as.double(reporting_year))
}
# Function to convert all xml listings into a table for each filer
xml_to_table <- function(file_number) {
xml_listing <- all_listings[[file_number]]
# Get attributes of filer and office name
LastName <- xml_attr(xml_listing, "LastName")
FirstName <- xml_attr(xml_listing, "FirstName")
OfficeName <- xml_attr(xml_find_first(xml_listing, ".//dbo.Office"), "OfficeName")
# Create a list of all filings made my the filer
all_filings <- xml_find_all(xml_listing, ".//dbo.Document")
traveler_filings <- data.frame()
#For each filing, extract information and put it in a dataframe
for(filing_num in 1:length(all_filings)) {
filing <- all_filings[[filing_num]]
ReportingYear <- xml_attr(filing, "ReportingYear")
BeginTravelDate <- xml_attr(filing, "BeginTravelDate")
EndTravelDate <- xml_attr(filing, "EndTravelDate")
DateReceived <- xml_attr(filing, "DateReceived")
TransactionDate <- xml_attr(filing, "TransactionDate")
Pages <- xml_attr(filing, "Pages")
ReportTitle <- xml_attr(xml_find_first(filing, ".//dbo.Reports"), "ReportTitle")
DocURL <- xml_attr(xml_find_first(filing, ".//dbo.Reports"), "DocURL")
report_data <- c(LastName, FirstName, OfficeName, ReportTitle, ReportingYear, BeginTravelDate, EndTravelDate, DateReceived, TransactionDate, Pages, DocURL)
traveler_filings <- rbind(traveler_filings, report_data)
}
traveler_filings <- setNames(traveler_filings, c("filer_lastname", "filer_firstname", "filer_office", "report_title", "reporting_year", "begin_travel_date", "end_travel_date", "date_received", "transaction_date", "num_pages", "doc_url")) %>%
mutate(begin_travel_date = as.Date(begin_travel_date,  "%m/%d/%Y")) %>%
mutate(end_travel_date = as.Date(end_travel_date,  "%m/%d/%Y")) %>%
mutate(date_received = as.Date(date_received,  "%m/%d/%Y")) %>%
mutate(transaction_date = as.Date(transaction_date,  "%m/%d/%Y")) %>%
mutate(num_pages = as.double(num_pages)) %>%
mutate(reporting_year = as.double(reporting_year))
}
# Download wayback archive zip files and convert to XML
download_wayback <- function(wayback_number) {
date <- wayback_dataframe$date[wayback_number]
link <- wayback_dataframe$urls[wayback_number]
wayback_response <- GET(link)
if (wayback_response$status_code == 200) {
# Parse the HTML content
zip_loc <- paste0("data/senate_data_archives/senate_data_", date, ".zip")
download.file(link, zip_loc)
# Create a filepath for the XML and unzip the file into the data folder
xml_loc <- gsub(".zip", ".xml", zip_loc)
unzip(zip_loc, exdir = "congressional-private-travel/data/")
#Rename the unzipped file to follow project convention
file.rename("data/giftrule.xml", xml_loc)
file.rename("data/GiftRuleData.xml", xml_loc)
} else {
# Handle errors
cat("Error: Unable to fetch the Wayback Machine URL\n")
}
}
# JUST for the first XML, which has a different format
xml_one_to_table <- function(file_number) {
xml_listing <- all_listings[[file_number]]
LastName = xml_attr(xml_listing, "LastName")
FirstName = xml_attr(xml_listing, "FirstName")
OfficeName = xml_attr(xml_listing, "OfficeName")
all_filings <- xml_find_all(xml_listing, ".//Document")
traveler_filings <- data.frame()
#For each filing, extract information and put it in a dataframe
for(filing_num in 1:length(all_filings)) {
filing <- all_filings[[filing_num]]
ReportingYear <- xml_attr(filing, "ReportingYear")
BeginTravelDate <- xml_attr(filing, "BeginTravelDate")
EndTravelDate <- xml_attr(filing, "EndTravelDate")
DateReceived <- xml_attr(filing, "DateReceived")
TransactionDate <- xml_attr(filing, "TransactionDate")
Pages <- xml_attr(filing, "Pages")
ReportTitle <- xml_attr(filing, "FilingType")
DocURL <- NA
report_data <- c(LastName, FirstName, OfficeName, ReportTitle, ReportingYear, BeginTravelDate, EndTravelDate, DateReceived, TransactionDate, Pages, DocURL)
traveler_filings <- rbind(traveler_filings, report_data)
}
traveler_filings <- setNames(traveler_filings, c("filer_lastname", "filer_firstname", "filer_office", "report_title", "reporting_year", "begin_travel_date", "end_travel_date", "date_received", "transaction_date", "num_pages", "doc_url")) %>%
mutate(begin_travel_date = as.Date(begin_travel_date)) %>%
mutate(end_travel_date = as.Date(end_travel_date)) %>%
mutate(date_received = as.Date(date_received)) %>%
mutate(transaction_date = as.Date(transaction_date)) %>%
mutate(num_pages = as.double(num_pages)) %>%
mutate(reporting_year = as.double(reporting_year))
}
#xml_to_csv <- function(wayback_xml) {
xml_loc <- paste0("data/senate_data_archives/", all_xml$value[wayback_xml])
# Example URL for the oldest available snapshot
wayback_urls <- c("https://web.archive.org/web/20080131052758/http://soprweb.senate.gov/giftrule/giftruledownload/GiftRuleData.zip", "https://web.archive.org/web/20110605144411/http://soprweb.senate.gov/giftrule/giftruledownload/giftruledata.zip", "https://web.archive.org/web/20150318055851/http://soprweb.senate.gov/giftrule/giftruledownload/giftruledata.zip", "https://web.archive.org/web/20190310121622/http://soprweb.senate.gov/giftrule/giftruledownload/giftruledata.zip", "https://web.archive.org/web/20230302004112/https://giftrule-disclosure.senate.gov/media/giftruledownloads/giftruledata.zip", "https://web.archive.org/web/20130512083149/http://soprweb.senate.gov/giftrule/giftruledownload/GiftRuleData.zip")
wayback_dataframe <- data.frame(urls = unlist(wayback_urls))
wayback_dataframe <- wayback_dataframe |>
mutate(date_time = gsub("[^0-9]", "", urls)) %>%
mutate(parsed_dt = strptime(date_time, format = "%Y%m%d%H%M%S")) %>%
mutate(date = as.Date(parsed_dt))
for (number in 1:nrow(wayback_dataframe)) {
download_wayback(number)
}
# Example URL for the oldest available snapshot
wayback_urls <- c("https://web.archive.org/web/20080131052758/http://soprweb.senate.gov/giftrule/giftruledownload/GiftRuleData.zip", "https://web.archive.org/web/20110605144411/http://soprweb.senate.gov/giftrule/giftruledownload/giftruledata.zip", "https://web.archive.org/web/20150318055851/http://soprweb.senate.gov/giftrule/giftruledownload/giftruledata.zip", "https://web.archive.org/web/20190310121622/http://soprweb.senate.gov/giftrule/giftruledownload/giftruledata.zip", "https://web.archive.org/web/20230302004112/https://giftrule-disclosure.senate.gov/media/giftruledownloads/giftruledata.zip", "https://web.archive.org/web/20130512083149/http://soprweb.senate.gov/giftrule/giftruledownload/GiftRuleData.zip")
wayback_dataframe <- data.frame(urls = unlist(wayback_urls))
wayback_dataframe <- wayback_dataframe |>
mutate(date_time = gsub("[^0-9]", "", urls)) %>%
mutate(parsed_dt = strptime(date_time, format = "%Y%m%d%H%M%S")) %>%
mutate(date = as.Date(parsed_dt))
for (number in 1:nrow(wayback_dataframe)) {
download_wayback(number)
}
View(wayback_dataframe)
View(wayback_dataframe)
wayback_number <- 6
date <- wayback_dataframe$date[wayback_number]
link <- wayback_dataframe$urls[wayback_number]
wayback_response <- GET(link)
library(xml2)
library(methods)
library(tidyverse)
library(XML)
library(furrr)
library(rvest)
library(httr)
wayback_response <- GET(link)
# Parse the HTML content
zip_loc <- paste0("data/senate_data_archives/senate_data_", date, ".zip")
download.file(link, zip_loc)
# Create a filepath for the XML and unzip the file into the data folder
xml_loc <- gsub(".zip", ".xml", zip_loc)
unzip(zip_loc, exdir = xml_loc)
unzip(zip_loc, exdir = "data/senate_data_archives")
#Rename the unzipped file to follow project convention
file.rename("data/senate_data_archives/giftrule.xml", xml_loc)
#Rename the unzipped file to follow project convention
file.rename("data/senate_data_archives/giftrule.xml", xml_loc)
all_xml <- as_tibble(list.files("data/senate_data_archives")) %>%
filter(str_detect(value, "xml")) %>%
mutate(date = ymd(str_extract(value, "\\d{4}-\\d{2}-\\d{2}")))
View(all_xml)
for (xml in 3:nrow(all_xml)) {
xml_loc <- paste0("data/senate_data_archives/", all_xml$value[xml])
xml_date <- all_xml$date[xml]
char_date <- as.character(xml_date)
current_data <- read_xml(xml_loc)
#Create a list of all the filers and their documents
all_listings <- xml_find_all(current_data, ".//dbo.filer")
# Speedy for loop to get all the information
filer_travel <- map_dfr(c(1:length(all_listings)), xml_to_table) %>%
mutate(source = xml_date)
csv_loc <- gsub(".xml", ".csv", xml_loc)
write_csv(filer_travel, csv_loc)
}
# List all files in the original data folder
all_files <- as.tibble(list.files("data/senate_data_archives/")) %>%
mutate(date = ymd(str_extract(value, "\\d{4}-\\d{2}-\\d{2}"))) %>%
filter(str_detect(value, "csv"))
today_date <- Sys.Date()
most_recent_csv <- paste0("data/combined_data/senate_data_", today_date, ".csv")
current_combined <- read_csv(most_recent_csv)
archive_added <- current_combined
#file <- 1
for (file in 1:nrow(all_files)) {
csv_name <- paste0("data/senate_data_archives/", as.character(all_files$value[file]))
csv_temp <- read_csv(csv_name)
archive_added <- archive_added %>%
full_join(csv_temp, by = c("filer_lastname", "filer_firstname", "filer_office", "report_title", "reporting_year", "begin_travel_date", "end_travel_date", "date_received", "transaction_date", "num_pages", "doc_url")) %>%
mutate(source = case_when(
is.na(source.x) ~ source.y,
TRUE~ source.x)) %>%
select(-source.x, -source.y)
}
write_csv(archive_added, most_recent_csv)
# Get the file path for the most recent csv
#most_recent_csv <- paste0("/workspaces/congressional-private-travel/data/combined_data/", as.character(all_files[1]))
today_date <- Sys.Date()
new_csv_loc <- paste0("data/combined_data/senate_data_", today_date, ".csv")
current_filings <- read_csv(new_csv_loc)
cleaned_filings <- current_filings %>%
mutate(begin_travel_date = begin_travel_date) %>%
mutate(end_travel_date = end_travel_date) %>%
mutate(date_received = date_received) %>%
mutate(transaction_date = transaction_date) %>%
mutate(office_firstname = str_squish(str_extract(filer_office, "(?<=,).*"))) %>%
mutate(office_last_name = str_squish(str_extract(filer_office, "^[^,]+"))) %>%
mutate(office_middle_name = str_squish(str_extract(office_firstname, "\\s(.*)"))) %>%
mutate(office_first_name = str_squish(str_extract(office_firstname, "^[^\\s]+"))) %>%
select(filer_lastname, filer_firstname, filer_office, office_first_name, office_middle_name, office_last_name, report_title, reporting_year, begin_travel_date, end_travel_date, date_received, transaction_date, num_pages, doc_url)
date_groups <- cleaned_filings %>%
distinct(transaction_date) %>%
arrange(transaction_date)
date_groups <- date_groups %>%
mutate(
previous_date = lag(transaction_date),
elapsed_days = as.numeric(transaction_date - previous_date)
)
View(date_groups)
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
jackindex_geo <- read_csv("../data/extracted_articles_aug25.csv")
jackindex_geo <- separate(data = jackindex_geo, col = newspaper_name, into = c("newspaper_name1","city"), sep = "[(]", extra ="merge", fill = "right")
jackindex_geo <- separate(data = jackindex_geo, col = city, into = c("city1","state"), sep = ",", extra ="merge", fill = "right")
jackindex_geo <- separate(data = jackindex_geo, col = city1, into = c("city1","crap"), sep = "\\[")
#removes ...)
jackindex_geo$city1 <- gsub("\\.\\.\\.)", "", jackindex_geo$city1)
jackindex_geo <- jackindex_geo %>%
mutate(news_address = paste(city1, newspaper_state, sep=", "))
#Import geocoded newspapers
jackindex_geo <- read.csv("../output/geocoded_newspapers_sept4.csv")
#import geocoded articles from class
googlesheets4::gs4_deauth()
articles <- read_sheet("https://docs.google.com/spreadsheets/d/1zDWMbuoTHVtiJDrrJ9mDfBTp-VxNFTmMHiGcGA-UvjE/edit?usp=sharing") %>%
as.data.frame()
#Sort of a bullshit workaround to deal with the university's Google: https://stackoverflow.com/questions/61356212/client-doesnt-have-sufficient-permission
articles$file_id2 <- as.integer(articles$file_id)
joined <- jackindex_geo %>%
inner_join(articles, by=c("file_id"="file_id2"))
#write_csv(joined, "../output/geocoded_TEST_joined_sept15.csv")
View(jackindex_geo)
View(joined)
View(joined)
#prepare new locations
joined <- joined %>%
rename(city_lynch1 = "City, town where lynching event took place", state_lynch1 = "State where lynching event took place", city_lynch2 = "Second Article: City, town of lynching event", state_lynch2 = "Second Article: State of lynching event", city_lynch3 =  "Third Article: City, town of lynching event", state_lynch3 = "Third Article: State of lynching event")
joined <- joined %>%
mutate(lynch_address1 = paste(city_lynch1, state_lynch1, sep=", ")) %>%
mutate(lynch_address2 = paste(city_lynch2, state_lynch2, sep=", ")) %>%
mutate(lynch_address3 = paste(city_lynch3, state_lynch3, sep=", "))
joined <- joined %>%
select(file_id, newspaper_name1, news_address, news_location.lon, news_location.lat, city1, newspaper_state, date, year, month, day, page, URL, lynch_address1, lynch_address2, lynch_address3, city_lynch1, state_lynch1, city_lynch2, state_lynch2, city_lynch3, state_lynch3, `Comments or notes?`,  index, sn)
#write_csv(joined, "../output/geocoded_TEST_joined_sept15.csv")
#rename newspaper_state to Postal code
joined$newspaper_state_code <- state.abb[match(joined$newspaper_state, state.name)]
#write_csv(joined, "../output/geocoded_TEST_joined_sept15.csv")
joined <- joined %>%
mutate(lynching1 = geocode(lynch_address1)) %>%
mutate(lynching2 = geocode(lynch_address2)) %>%
mutate(lynching3 = geocode(lynch_address3))
#restructure df in tidy format
x <- joined %>%
select(newspaper_state_code) %>%
group_by(newspaper_state_code) %>%
count(newspaper_state_code) %>%
rename(news_state_total =n)
y <- joined %>%
select(state_lynch1) %>%
group_by(state_lynch1) %>%
count(state_lynch1) %>%
rename(state_lynch_total =n)
states_compare <- x %>%
inner_join(y, by=c("newspaper_state_code" = "state_lynch1"))
# write.csv(states_compare, "../output/states_compared_TEST_sept15.csv" )
lynch_geocoded_9.16 <- read.csv("../data/lynch_geocoded_9.16.csv")
lynch_geocoded_9.16 <- read.csv("../data/lynch_geocoded_9.16.csv")
View(lynch_geocoded_9.16)
## Pct of Newspapers in state vs out of state
lynch_geocoded_9.16 %>%
count(in_state) %>%
mutate(pct = round(n/3292,2))
# in_state
# N	3068	0.93
# Y	223	0.07
summary(lynch_geocoded_9.16$miles)
#Newspapers, on average, were 878 miles away from a lynching event during the whole time period
duplicated_entries <- lynch_geocoded_9.16 %>%
group_by(file_id) %>%
count(lynch_address)
View(duplicated_entries)
duplicated_entries <- lynch_geocoded_9.16 %>%
group_by(file_id) %>%
count(lynch_address) %>%
filter(n > 1)
View(duplicated_entries)
write_csv(duplicated_entries, "data/duplicated_entries_sep21.csv")
setwd("~/Desktop/GitHub/Jour389L")
write_csv(duplicated_entries, "data/duplicated_entries_sep21.csv")
wd()
write_csv(duplicated_entries, "Jour389L/data/duplicated_entries_sep21.csv")
setwd("~/Desktop/GitHub/Jour389L")
write_csv(duplicated_entries, "/data/duplicated_entries_sep21.csv")
write_csv(duplicated_entries, ".../data/duplicated_entries_sep21.csv")
lynch_geocoded_9.16 <- read.csv("../data/lynch_geocoded_9.16.csv")
write_csv(duplicated_entries, ".../data/duplicated_entries_sep21.csv")
write_csv(duplicated_entries, "~/Desktop/Github/data/duplicated_entries_sep21.csv")
write_csv(duplicated_entries, ".../Desktop/Github/data/duplicated_entries_sep21.csv")
write_csv(duplicated_entries, ".../Desktop/Githuub/rathore-khushboo-personal-repo")
write_csv(duplicated_entries, "~/Desktop/Githuub/rathore-khushboo-personal-repo")
write_csv(duplicated_entries, "~/Desktop/Githuub/rathore-khushboo-personal-repo/lynch_duplicates_0921.csv")
write_csv(duplicated_entries, "~/Desktop/Githuub/rathore-khushboo-personal-repo/lynch_duplicates_0921.csv", lazy = FALSE)
write_csv(duplicated_entries, "~/Desktop/Githuub/rathore-khushboo-personal-repo/lynch_duplicates_0921.csv")
write_sheet(duplicated_entries, "https://docs.google.com/spreadsheets/d/1mt-K372GEtjE_v0qeiX-57MlnAD-9tT5f4AZr4NxZnY/edit#gid=0")
install.packages("googlesheets4")
install.packages("googlesheets4")
library("googlesheets4")
# Repetitions:
```
write_sheet(duplicated_entries, "https://docs.google.com/spreadsheets/d/1mt-K372GEtjE_v0qeiX-57MlnAD-9tT5f4AZr4NxZnY/edit#gid=0")
View(lynch_geocoded_9.16)
View(duplicated_entries)
x <- lynch_geocoded_9.16 %>%
select(decade, miles, in_state) %>%
group_by(decade) %>%
count(in_state)
#install.packages("here")
here::here()
library(tidyverse)
library(tidyr)
#install.packages("ggmap")
library(ggmap)
#register_google(key = "YOUR KEY HERE")
library(googlesheets4)
#install.packages("geosphere")
library(geosphere)
x <- lynch_geocoded_9.16 %>%
select(decade, miles, in_state) %>%
group_by(decade) %>%
count(in_state)
View(x)
ggplot(x, aes(x=decade, y=n, color=in_state, fill=in_state)) +
geom_col(position = "dodge") +
theme(legend.position = "none") +
labs(title = "DRAFT FINDINGS: Little Local Coverage of Lynching",
subtitle = "Local (teal) vs Out-of-State (red) Lynching Coverage",
caption = "Teal bar=In-state. Red Bar=Out-of-State. Source: Library of Congress. n=3188 Graphic by Rob Wells. Sept 20 2024",
y="Count of articles",
x="")
View(lynch_geocoded_9.16)
View(lynch_geocoded_9.16)
state_groupings <- lynch_geocoded_9.16 %>%
group_by(newspaper_state_code, state_lynch)
View(state_groupings)
state_groupings <- lynch_geocoded_9.16 %>%
group_by(newspaper_state_code, state_lynch) %>%
count(.)
View(state_groupings)
state_groupings <- lynch_geocoded_9.16 %>%
group_by(newspaper_state_code, state_lynch) %>%
count(.) %>%
filter(newspaper_state_code != state_lynch)
state_groupings <- lynch_geocoded_9.16 %>%
group_by(newspaper_state_code, state_lynch) %>%
count(.) %>%
filter(newspaper_state_code != state_lynch) %>%
group_by(newspaper_state_code)
state_groupings <- lynch_geocoded_9.16 %>%
filter(newspaper_state_code != state_lynch) %>%
group_by(newspaper_state_code) %>%
count(.)
View(duplicated_entries)
state_groupings <- lynch_geocoded_9.16 %>%
distinct(.)
View(state_groupings)
state_groupings <- lynch_geocoded_9.16 %>%
select(-X, -X.1) %>%
distinct(.)
View(state_groupings)
state_groupings <- lynch_geocoded_9.16
state_groupings <- lynch_geocoded_9.16 %>%
distinct(file_id, lynch_address, index)
View(state_groupings)
state_groupings <- lynch_geocoded_9.16 %>%
distinct(file_id, lynch_address, index, .keep_all = TRUE)
View(state_groupings)
state_groupings <- lynch_geocoded_9.16 %>%
unique(file_id, lynch_address, index, .keep_all = TRUE)
state_groupings <- lynch_geocoded_9.16 %>%
distinct(file_id, lynch_address, .keep_all = TRUE)
View(state_groupings)
state_groupings <- lynch_geocoded_9.16 %>%
distinct(file_id, lynch_address, .keep_all = TRUE) %>%
filter(newspaper_state_code != state_lynch) %>%
group_by(newspaper_state_code) %>%
count(.)
